.. _10-1-ht-errors-and-power:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch10-1">
      <iframe
         id="video-ch10-1"
         title="STAT 350 ‚Äì Chapter 10.1 Hypothesis Testing for the Mean of a Population and Power Video"
          src="https://www.youtube.com/embed/ZQusNqSNSdY?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip
   
   - `Hypothesis Testing for Single Sample (Part 1) (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/Chapter%2010%20Hypothesis%20Testing/Hypothesis%20Testing%20for%20Single%20Sample%20Part1_AC.pptx>`_

The Foundation of Hypothesis Testing
======================================================

Hypothesis testing is a key statistical inference framework 
that **assesses whether claims 
about population parameters are reasonable based on data evidence**. 
In this lesson, we establish the basic language of hypothesis testing to prepare
for the formal steps covered in the upcoming lessons.


.. admonition:: Road Map üß≠
   :class: important

   * Learn the building blocks of hypothesis testing.
   * Formulate the **null and alternative hypotheses in the correct format** for a given research question.
   * Understand the logic of constructing a **decision rule**.
   * Recognize the two **types of errors** that can arise in hypothesis testing and understand how they are controlled or
     influenced by different components of the procedure.

The Building Blocks of Hypothesis Testing
-------------------------------------------

A. The Dual Hypothesis Framework
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A **statistical hypothesis** is a claim about one or more population parameters, expressed as a mathematical statement.
The first step in hypothesis testing is to frame the research question as two competing hypotheses:

* **Null hypothesis**, :math:`H_0`: the status quo or a baseline claim, 
  assumed true until there is sufficient evidence to conclude otherwise

* **Alternative hypothesis**, :math:`H_a`: the competing claim to be tested against the null

When testing a claim about the population mean :math:`\mu`,
the hypothesis formulation follows a set of rules summarized by 
:numref:`HT_template0` and the following list.

.. _HT_template0:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/HT_template.png 
   :figwidth: 70%
   :align: center 
   :alt: hypotheses template

   Template for dual hypothesis

1. In :numref:`HT_template0`, the part in black always stays unchanged.
2. :math:`\mu_0`, called the **null value**, is a point of comparison for :math:`\mu`
   taken from the research context. It is represented with a symbol here, but it takes a **concrete numerical value** in
   applications.
3. :math:`H_0` and :math:`H_a` are complementary‚Äîtheir cases
   must not overlap yet together encompass all possibilities for the parameter.
   :math:`H_0` always includes an equality sign (:math:`=, \leq, \geq`), while
   the inequality in :math:`H_a` is always strict (:math:`\neq, <, >`).

Let us get some practice applying these rules correctly to research questions.

.. admonition:: Example üí°: Writing the Hypotheses Correctly
   :class: note

   For each research scenorio below, write the appropriate set of hypotheses to conduct a hypothesis test.
   Be sure to follow all the rules for hypotheses presentation.

   1. **The census data show that the mean household income in an area is $63K (63 thousand dollars) per year.  
      A market research firm wants to find out whether the mean household income of the shoppers at a mall 
      in this area is HIGHER THAN that of the general population.**

      Let :math:`\mu` denote the true mean household income of the shoppers at this mall. The dual hypothesis is:

      .. math::
         &H_0: \mu \leq 63\\
         &H_a: \mu > 63

      The question raised by the study will always align with the alternative hypothesis.
      Also note that the generalized symbol :math:`\mu_0` in the template (:numref:`HT_template0`) is
      replaced with a specific vlaue, 63, from the context.

   2. **Last year, your company‚Äôs service technicians took an average of 2.6 hours to respond to 
      calls from customers. Do this year‚Äôs data show a DIFFERENT average time?**

      Let :math:`\mu` denote the true mean average response time by service techicians this year.
      The dual hypothesis appropriate for this research question is:

      .. math::
         &H_0: \mu = 2.6\\
         &H_a: \mu \neq 2.6

   3. **The drying time of paint under a specified test condition is known to be normally distributed 
      with mean 75 min and standard deviation 9 min. Chemists have proposed a new additive designed 
      to DECREASE average drying time.  Should the company change to the new additive?**

      Let :math:`\mu` be the true mean drying time of the new paint formula. Then we have:

      .. math::
         &H_0: \mu \geq 75\\
         &H_a: \mu < 75

Three Types of Hypotheses
^^^^^^^^^^^^^^^^^^^^^^^^^^^

From these examples, we see that there are three main ways to formulate
a pair of hypotheses. Focusing on the alternative side, 

* A test with :math:`H_a: \mu > \mu_0` is called an 
  **upper-tailed (right-tailed)** hypothesis test.
* A test with :math:`H_a: \mu < \mu_0` is called a **lower-tailed (left-tailed)** hypothesis test. 
* A test with :math:`H_a: \mu \neq \mu_0` is called a **two-tailed** hypothesis test. 

B. The Significance Level
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before collecting any data, we must decide how strong the evidence must be to reject the null 
hypothesis. The **significance level**, denoted :math:`\alpha`, is the pre-specified probability 
that represents our **tolerance for the error of rejecting a true null hypothesis**.
A small value, typically less than or equal to :math:`0.1`, is chosen based on 
expert recommendations, legal requirements, or field conventions.
The smaller the :math:`\alpha`, the stronger the evidence must be to reject the null hypothesis.

C. The Test Statistic and the Decision
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Identifying the Goal
^^^^^^^^^^^^^^^^^^^^^^^

For a concrete context, suppose we perform the **upper-tailed hypothesis test** for the true mean
income of shoppers at a mall, taken from the first of the three examples above.

.. math::
   &H_0: \mu \leq 63\\
   &H_a: \mu > 63

Let us also assume that 

1. :math:`X_1, X_2, \cdots, X_n` form an *iid* sample from
   the population :math:`X` with mean :math:`\mu` and variance :math:`\sigma^2`.
2. Either the population :math:`X` is normally distributed, or the sample size :math:`n` is
   sufficiently large for the CLT to hold.
3. The population variance :math:`\sigma^2` is known.

We now need to **develop an objective rule** for rejecting the null hypothesis.
This rule must (1) be applicable in any upper-tailed hypothesis testing scenario 
where the assumptions hold, and (2) satisfy the maximum error tolerance condition given by :math:`\alpha`.

Finding the Decision Rule
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It is natural to view the sample mean :math:`\bar{X}` as central to the decision, since it is
one of the best indicators of the true location of :math:`\mu`. 
In the simplest terms, if :math:`\bar{X}` 
yields an observed value :math:`\bar{x}` *much larger* than 63 (thousands of dollars), 
we would want to reject the null hypothesis, 
whereas if it is close to or lower than 63,
there would not be enough evidence against it. The key question is, **how large must** :math:`\bar{x}` 
**be to count as sufficient evidence against the null?**

Under the set of assumptions about the distribution of :math:`X` and its sampling conditions, :math:`\bar{X}` (approximately)
follows a normal distribution. In addition, its full distribution can be given by

.. math:: \bar{X} \sim N\left(63, \frac{\sigma^2}{n}\right)

under the null hypothesis. We call this the *null distribution*.

Let us consider rejecting the null hypothesis **only if** :math:`\bar{x}`
**lands above the cutoff that marks an upper area of** :math:`\alpha` 
under the null distribution: 

.. _upper-HT-decision:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/upper-HT-decision.png 
   :figwidth: 90%
   :align: center 
   :alt: Decision rule for an upper-tailed hypothesis test

   Decision rule for an upper-tailed hypothesis test

(1) **Is this rule objective and universally applicable in other upper-tailed hypothesis tests?** 
    
    Yes. If the same set of assumptions hold, we can make an equivalent
    rule by replacing the values of :math:`\mu_0, \sigma^2`, and :math:`n` appropriately.

(2) **Does this rule limit the false rejection rate to at most** :math:`\alpha` **?**

    Yes. If :math:`H_0` was indeed true, then according to the null
    distribution, :math:`\bar{X}` would generate values above
    the cutoff only :math:`\alpha \cdot 100 \%` of the time. By design, the chance of
    incorrectly rejecting the null hypothesis is limited by how often incorrect answers are generated
    under the null hypothesis.

(3) **What about other potential values under** :math:`H_0` **?**
    
    The null hypothesis :math:`H_0: \mu \leq 63` proposes that :math:`\mu` is 
    anything **less than or equal to** the null value, 63. Is the decision rule also safe
    for candidate values other than 63? The answer is yes.
    When the true mean is strictly less than :math:`\mu_0`, 
    the entire distribution of :math:`\bar{X}` slides to the left and away from the cutoff,
    leaving an upper-tail area smaller than :math:`\alpha`:
    
    .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/other-null-cases.png 
      :figwidth: 90%
      :align: center 
      :alt: Other candicate values from the null hypothesis

      Candicate values for :math:`\mu` other than :math:`\mu_0` in the null hypothesis

    Therefore, error-inducing outcomes are generated even less frequently than when the population mean is exactly 63.
    In general, the boundary case of :math:`\mu=\mu_0` addresses the **worst case scenario** in terms of
    false rejection rates. If the rule is safe under the boundary case, then it is safe under all other scenarios
    belonging to the null hypothesis.

    
How to Locate the Cutoff
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The exact location of the cutoff can be computed by viewing it as a :math:`(1-\alpha)\cdot 100`-th 
percentile of the boundary case null distribution. Using the techniques learned in Chapter 6, confirm that the cutoff is 
:math:`z_{\alpha}\frac{\sigma}{\sqrt{n}} + 63` for this example, where :math:`z_\alpha`
is the :math:`z`-critical value used in Chapter 9. In general,

.. math:: \text{cutoff}_{upper} = z_{\alpha}\frac{\sigma}{\sqrt{n}} + \mu_0.

In summary, we **reject the null hypothesis of an upper-tailed hypothesis test** if 
:math:`\bar{x} > z_{\alpha}\frac{\sigma}{\sqrt{n}} + \mu_0` or, by standardizing both sides, if

.. math:: \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}} > z_{\alpha}.

.. admonition:: What About the Cutoff for a Lower-Tailed Hypothesis Test?
   :class: important

   By making a mirror argument of this section, confirm that
   you would reject the null hypothesis for a **lower-tailed hypothesis test** if
   :math:`\bar{x} < -z_\alpha\frac{\sigma}{\sqrt{n}} + \mu_0 = \text{cutoff}_{lower}`,
   or by standardizing both sides, if 

   .. math:: \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}} < -z_{\alpha}.
   

The Test Statistic
^^^^^^^^^^^^^^^^^^^^^

A statistic that measures the consistency of the observed data 
with the null hypothesis is called the **test statistic**. For hypothesis tests on a population mean,
:math:`\frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}` plays this role. 
Its realized value represents **the standardized distance between the
hypothesized true mean** :math:`\mu_0` **and the generated outcome** :math:`\bar{x}`.
It is also used for comparison against a :math:`z`-critical value to draw the final decision.
Since it follows the standard normal distribution under the null hypothesis,
we denote this quantity :math:`Z_{TS}`:

.. math::
   Z_{TS} = \frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}

and call it the :math:`z`-**test statistic**.

Understanding Type I and Type II Errors
----------------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch10-1-1">
      <iframe
         id="video-ch10-1-1"
         title="STAT 350 ‚Äì Chapter 10.1.1 Type 1, Type 2 Error and Power"
          src="https://www.youtube.com/embed/rc1OOsAohSw?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

Since the results of hypothesis tests always accompany a degree of uncertainty, it is important to
analyze the likelihood and consequences of the possible errors. There are two types of
errors that can arise in hypothesis testing. 
The error of incorrectly rejecting a true null hypothesis is called the **Type I error**, while
the error of failing to reject a false null hypothesis is called the **Type II error**.
The table below summarizes the different combinations of reality and decision.

.. flat-table:: 
   :header-rows: 1

   * - :cspan:`1` Decision 
     - Fail to Reject :math:`H_0`
     - Reject :math:`H_0`
   
   * - :rspan:`1` **Reality**
     - :math:`H_0` is True
     - ‚úÖ Correct 
     - ‚ùå Type I Error
   
   * - :math:`H_0` is False
     - ‚ùå Type II Error
     - ‚úÖ Correct

Type I Error: False Positive
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A **Type I error** occurs when a true null hypothesis is rejected.
This error results in a false positive claim of an effect or difference when none actually 
exists. The probability of making a Type I error is denoted :math:`\alpha`. Formally,

.. math::
   \alpha = P(\text{Type I error}) = P(\text{Reject } H_0|H_0 \text{ is true}).

**Examples of Type I errors**

- Concluding that a new drug is effective when it actually has no effect
- Claiming that a manufacturing process has changed when it is operating the same way as before

Type II Error: False Negative
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A **Type II error** occurs when a false null hypothesis is not rejected.
This results in a false negative case where real effect or difference goes undetected. 
The probability of making a Type II error is 

.. math::
   \beta = P(\text{Type II error}) = P(\text{Fail to reject } H_0| H_0 \text{ is false}).

**Examples of Type II errors**

- Failing to detect that a new drug is more effective than placebo
- Failing to recognize that a manufacturing process has deteriorated

Error Trade-offs and Prioritization of the Type I Error
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Type I and Type II errors are inversely related‚Äîefforts to reduce one type of error typically 
increase the other. The only way to reduce both error types simultaneously is to increase the sample size, 
collect higher quality data, or improve the measurement process.

When constructing a hypothesis test under limited resources, therefore, we must prioritize one
error over the other. **We typically choose to control** :math:`\alpha`. That is, we 
design the decision-making procedure so that its probability of Type I error reamins below
a pre-specified maximum. We make this choice because falsely claiming change from 
the status quo often carries substantial immediate cost. Such costs can include purchasing new factory equipment, 
setting up a production line and marketing strategy for a new drug, or revising a business contract.

By consequence, we cannot directly control :math:`\beta`. 
Instead, we analyze and try to minimize :math:`\beta` by learning its relationship with
the population distribution, sample size, and the significance level :math:`\alpha`.

.. admonition:: A Legal System Analogy üßë‚Äç‚öñÔ∏è
   :class: note

   The analogy between hypothesis testing and the American legal system offers useful insight.
   Just as we would rather let a guilty person go free than convict an innocent person, 
   we are generally more concerned about incorrectly rejecting a true null 
   hypothesis than about failing to detect a false one. Further,

   - The **null hypothesis** is like the defendant, presumed innocent until proven guilty.
   - The **alternative hypothesis** is like the prosecutor, trying to establish the defendant's guilt.
   - The **significance level** :math:`\alpha` represents the standard of evidence required for conviction.
   - The **test statistic** summarizes all the evidence presented at trial.
   - The **p-value** (to be discussed in Section 10.2) measures how convincing this evidence is.

Statistical Power: The Ability to Detect True Change
------------------------------------------------------------


.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch10-1-2">
      <iframe
         id="video-ch10-1-2"
         title="STAT 350 ‚Äì Chapter 10.1.2 Power Calculations"
          src="https://www.youtube.com/embed/pXRyQQt_v_I?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>


Definition
~~~~~~~~~~~~~~

**Statistical power** is the probability that a test will **correctly reject a false null hypothesis**. 
It represents the test's ability to detect an unusual effect when it actually exists. It is also the **complement of
the Type II error probability**, :math:`\beta`.

.. math::

   \text{Power} = P(\text{Reject } H_0 | H_0 \text{ is false}) = 1 - \beta

Power ranges from 0 to 1, with higher values indicating a better ability to detect false 
null hypotheses. A power of 0.80 means that if the null hypothesis is false, the test
correctly rejects it with 80% chance. 

Visualization of :math:`\alpha, \beta`, and Power
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let us continue with the upper-tailed hypothesis test for the true mean household income of shoppers at a mall.
The dual hypothesis is:

.. math::
   &H_0: \mu \leq 63\\
   &H_a: \mu > 63

where the null value is :math:`\mu_0 = 63`. We agreed to reject the null hypothesis whenever 
:math:`\bar{x}` was "too large", or when

.. math::
   \bar{x} > z_{\alpha}\frac{\sigma}{\sqrt{n}} + 63 = \text{cutoff}_{upper}.

Let us also make the unrealistic assumption that we know the true value of :math:`\mu`; it is
equal to :math:`\mu_a = 65,` making the reality belong to the alternative side. Let us visualize this along with
the resulting locations of :math:`\alpha`, :math:`\beta`, and power:

.. _original-plot:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/power-analysis0.png 
   :figwidth: 90%
   :align: center 
   :alt: Locations of alpha, beta, and power marked on the null and alternative distributions of X bar

   Plots were generated using :math:`n=35, \mu_0 = 63, \mu_a=65, \sigma=4, \alpha =0.05` in 
   `üîó Power Simulator <https://treese41528.github.io/STAT350/ShinyApps/Power_Simulator.html>`_ .

The diagram has two normal densities partially overlapping one another. The left curve, centered at :math:`\mu_0 = 63`, represents the
reality assumed by the null hypothesis, while the right curve, centered at :math:`\mu_a=65,` represents the truth. According to our
decision rule, we draw the cutoff where it leaves an upper-tail area of :math:`\alpha` (red) **under the null distribution** and
reject the null hypothesis whenever we see the sample mean land above it.

What happens if, in the meantime, the sample means are actually being generated from the **right (alternative) curve**? 
With probabiliy :math:`\beta` (purple), it will generate an observed sample mean
that will fail to lead to a rejection of :math:`H_0` (Type II error). All other outcomes
will lead to a correct rejection, with the probability represented by the green area (power).

Let us observe how the sizes of these three regions are influenced by different components of the experiment.


What Influcences Power?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Significance Level, :math:`\alpha`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/power-analysis-alpha.png 
   :figwidth: 50%
   :align: center 
   :alt: alpha, beta, and power as alpha changes

   :math:`\alpha=0.01, 0.05, 0.1` from top to bottom

The central plot with the blue outline is the original plot, identical to
:numref:`original-plot`.
A **smaller** :math:`\alpha` **pushes the cutoff up in an upper-tailed hypothesis test**, since it calls
for more caution against Type I error and requires a stronger evidence (larger :math:`\bar{x}`) 
for rejection of the null hypothesis.
In response, the probability of Type II error increases (purple) and the power decreases (green).

True Mean, :math:`\mu_a`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/power-analysis-mu.png 
   :figwidth: 50%
   :align: center 
   :alt: alpha, beta, and power as the alternative truth mu_a changes

   :math:`\mu_a=64,65,66` from top to bottom

If the hypothesized :math:`\mu_0` and the true effect :math:`\mu_a` are close to each other,
it is naturally harder to separate the two cases. Even though :math:`\alpha` stays constant
(because we explicitly control it), the power decreases and the Type II error probability goes up
as the distance between :math:`\mu_0` and :math:`\mu_a` narrows.

Population Standard Deviation, :math:`\sigma`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/power-analysis-sd.png 
   :figwidth: 50%
   :align: center 
   :alt: alpha, beta, and power as the population sd changes

   :math:`\sigma=2.5, 4, 5` from top to bottom

Recall that :math:`\bar{X}` has the standard deviation :math:`\sigma/\sqrt{n}`.
When :math:`\sigma` decreases while :math:`\mu_0` and :math:`\mu_a` stay constant,
the two densities become narrower around their respective means, creating a stronger separation between the two cases.
This leads to a higher power and smaller Type II error probability.

Sample Size, :math:`n`
^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter10/power-analysis-n.png 
   :figwidth: 50%
   :align: center 
   :alt: alpha, beta, and power as the sample size changes

   :math:`n=13, 35, 70` from top to bottom

The sample size :math:`n` also affects the spread of the distribution of :math:`\bar{X}`, but in the opposite direction 
of :math:`\sigma`. As :math:`n` decreases, :math:`\sigma/\sqrt{n}` increases, making the densities wider. 
Larger overlap between the distributions leads to decreased power and higher Type II error probability.

.. _power-simulator:
.. admonition:: Power Analysis Simulator üéÆ
   :class: interactive
 
   Explore how :math:`\alpha, \beta`, and statistical power
   relate to each other, and reproduce the images used in this section using:
  
   `üîó Interactive Demo <https://treese41528.github.io/STAT350/ShinyApps/Power_Simulator.html>`_ |
   `üìÑ R Code <https://treese41528.github.io/STAT350/ShinyApps/Power_Simulator_Shiny.R>`_

Prospective Power Analysis
-----------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch10-1-3">
      <iframe
         id="video-ch10-1-3"
         title="STAT 350 ‚Äì Chapter 10.1.3 Sample Size Calculations"
          src="https://www.youtube.com/embed/umlrWPs7qlA?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>


From the previous discussion, we find that the only realistic way to control statistical power is through the sample size, :math:`n`.
Before conducting a study, researchers perform **prospective power analysis** to determine the 
sample size needed to ensure adequate power in their tests.

We continue with the upper-tailed hypothesis test on the true mean household income of shoppers at a mall:

.. math::
   &H_0: \mu \leq 63\\
   &H_a: \mu > 63

Suppose that the researchers expect the test to **detect an increase of 2K in the household income effectively**. Specifically,
such jump should be detected **with a probability at least 0.8**. In other words, we want:
  
.. math:: \text{Power} = 1-\beta \geq 0.8

when :math:`\mu = 63 + 2 = 65`. The magnitude of change to be detected, 2 in this case, is also called
the **effect size**.

Additionally, we still assume:

1. :math:`X_1, X_2, \cdots, X_n` forms an *iid* sample from
   a population :math:`X` with mean :math:`\mu` and variance :math:`\sigma^2`.
2. Either the population :math:`X` is normally distributed, or the sample size :math:`n` is
   sufficiently large for the CLT to hold.
3. The population variance :math:`\sigma^2` is known.

Step 1: Mathematically Clarify the Goal
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In general, :math:`\text{Power} = P(\text{Reject} H_0|H_0 \text{ is false})`.

We replace the general definition with the specific conditions given by our problem.
The event of "rejecting :math:`H_0`" is equivalent to the event :math:`\{\bar{X} > \text{cutoff}_{upper}\},`
and the event that :math:`H_0` is false should now reflect the desired effect size. 
Therefore, our goal is to find :math:`n` satisfying: 

.. math::
   \text{Power} = P\left(\bar{X} > \text{cutoff}_{upper} \Bigg| \mu=65\right) \geq 0.8

or, equivalently,

.. math::
   \beta = P\left(\bar{X} \leq \text{cutoff}_{upper} \Bigg| \mu=65\right) < 0.2.

Denote the value :math:`0.2` by :math:`\beta_{max}`, since we do not allow :math:`\beta`
to be larger than :math:`0.2`.

Step 2: Simplify and Calculate
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let us break down the latter form of our mathematical goal. 

* From the conditional information, we know that :math:`\bar{X}` is assumed to follow the 
  distribution :math:`N(65, \sigma/\sqrt{n})`. 
* Since the goal is written with a strict inequality, the cutoff must be a value
  strictly less than the 20th (:math:`\beta_{max}\cdot 100`-th) percentile of 
  :math:`N(65, \sigma/\sqrt{n})`. Mathematically,
 
   .. math::
      \text{cutoff}_{upper} < -z_{\beta_{max}}\frac{\sigma}{\sqrt{n}} + 65

   where :math:`z_{\beta_{max}}` is a :math:`z`-critical value computed for the upper-tail area :math:`\beta_{max}`. 

* Replacing the :math:`\text{cutoff}_{upper}` with its complete formula,

  .. math::
     z_\alpha\frac{\sigma}{\sqrt{n}} + 63 < -z_{\beta_{max}}\frac{\sigma}{\sqrt{n}} + 65.

  Isolate :math:`n`:

  .. math::

     n > \left(\frac{(z_{\alpha} + z_{\beta_{max}}) \sigma}{65 - 63}\right)^2.

Since :math:`n` must be an integer, we take the **smallest integer above this lower bound**.

Summary
~~~~~~~~~

In an **upper-tailed hypothesis test**, 
the minimum sample size for a desired power lower bound :math:`1-\beta_{max}` and 
an **effect size** :math:`|\mu_a-\mu_0|` is the **smallest integer** :math:`n` satisfying:

.. math::

   n > \left(\frac{(z_{\alpha} + z_{\beta_{\max}}) \sigma}{|\mu_a - \mu_0|}\right)^2.

.. admonition:: Prospective Power Analysis for Lower-tailed Hypothesis Tests
   :class: important

   By walking through a mirror argument, confirm that the minimum sample size :math:`n` for a desired
   power lower bound :math:`1-\beta_{max}` and an effect size :math:`|\mu_a-\mu_0|` in
   a **lower-tailed hypothesis test** is determined by the **same formula** as the upper-tailed case.

.. admonition:: Example üí°: Compute Power for SAT Scores
   :class: note

   A teacher at STAT High School believes that their students score **higher** on the SAT 
   than the 2013 national average of 1497. Assume the true standard deviation of SAT scores 
   from this school is 200.
   
   **Q1**: The teacher wants to construct a hypothesis test at 0.01 significance level
   that can detect a 20-point increase in the true mean effectively. 
   If the current sample size is 300, what is the power of this test?

   **Step 1: Identify the Components**

   - The dual hypothesis is:
     
     .. math:: 
        &H_0: \mu \leq 1497\\
        &H_a: \mu > 1497
        
   - :math:`\alpha = 0.01` (:math:`z_{0.01} = 2.326348`)
     
     .. code-block:: r
        
        z_alpha <- qnorm(0.01, lower.tail = FALSE)

   - Effect size: :math:`20` points. This makes :math:`\mu_a = \mu_0 + 20 = 1497 = 1517`. 
   - Population standard deviation is :math:`\sigma = 200` points
   - Current sample size: :math:`n=300`

   **Step 2: Find the Cutoff**

   .. math::

      \text{cutoff}_{upper} = 1497 + \frac{200}{\sqrt{300}}(2.326348) = 1497 + 26.862 = 1523.862

   **Step 3: Calculate Power**

   .. math::

      \text{Power} = P(\bar{X} > 1523.862 | \mu = 1517)
   
   Using the conditional information, we compute the probability assuming
   that :math:`\bar{X} \sim N(1517, \sigma/\sqrt{n})`.
   
   .. math::
      \text{Power} &= P\left(\frac{\bar{X}-1517}{\sigma/\sqrt{n}} > \frac{1523.862-1517}{\sigma/\sqrt{n}}\right)\\
      &= P(Z > 0.5943) = 0.2762

   **Result**: The power is only 27.62%. This test is **not sufficiently sensitive** 
   to reliably detect a 20-point improvement.



.. admonition:: Example üí°: Compute Minimum Sample Size for SAT Scores
   :class: note

   **Q2**: Continuing with the SAT scores problem, what is the minimum sample size required for the test to detect a 20-point increase with
   at least 90% chance?

   **Step 1: Identify the Components**

   - :math:`\text{Power} \geq 0.90` is required, so :math:`\beta = 1 - \text{Power} < 0.10 = \beta_{max}`.  
   - :math:`z_{\beta_{max}} = 1.282`

     .. code-block:: r 

        z_betamax <- qnorm(0.1, lower.tail=FALSE)

   **Step 2: Apply the Formula**

   .. math::

      n &> \left[\frac{(z_{\alpha} + z_{\beta_{max}}) \sigma}{|\mu_a - \mu_0|}\right]^2 
         = \left[\frac{(2.326348 + 1.281552)(200)}{1517-1497}\right]^2\\
        &= \left[\frac{(3.6079)(200)}{20}\right]^2 = (36.079)^2 = 1301.69

   **Result**: We would need at least :math:`n = 1302` students to achieve 90% power‚Äîmuch 
   larger than the available sample of 300.




.. admonition:: Example üí°: Average Recovery Time
   :class: note

   A pharmaceutical company wants to test whether a new drug **reduces** average recovery time from a common illness. Historical data shows the standard recovery time is :math:`\mu_0 = 7` days with :math:`\sigma = 2` days. The company wants to detect a reduction to :math:`\mu_a = 6` days (a 1-day improvement) with 90% power at :math:`\alpha = 0.05` significance.

   **Step 1: Identify the Components**

   - The hypotheses

     .. math:: 
        &H_0: \mu \geq 7\\
        &H_a: \mu < 7

   - The significance level: :math:`\alpha = 0.05` :math:`(z_{\alpha} = 1.645)`

     .. code-block:: r 

        z_alpha <- qnorm(0.05, lower.tail=FALSE)

   - :math:`\text{Power} \geq 0.90` is required, so :math:`\beta = 1 - \text{Power} < 0.10 = \beta_{max}`.  
     :math:`(z_{\beta_{max}} = 1.282)`

     .. code-block:: r 

        z_betamax <- qnorm(0.1, lower.tail=FALSE)

   - Effect size: :math:`|\mu_a - \mu_0| = |6 - 7| = 1` day
   - Population standard deviation: :math:`\sigma = 2` days

   **Step 2: Calculate Required Sample Size**

   .. math::

      n &> \left(\frac{(1.645 + 1.282)(2)}{|6 - 7|}\right)^2 \\
      &= \left(\frac{(2.927)(2)}{1}\right)^2 = (5.854)^2 \approx 34.3

   The company needs at least :math:`n = 35` patients to achieve statistical power of at least 90%.

..
   Interactive Power Analysis Visualization
   ----------------------------------------

   To better understand how the various components of hypothesis testing interact, you can explore an interactive simulation that visualizes the relationship between Type I error, Type II error, power, and the factors that affect them.

   This interactive tool allows you to:

   - **Adjust the null and alternative means** to see how effect size affects power
   - **Change the significance level (:math:`\alpha`)** to observe the trade-off between Type I error and power
   - **Modify the sample size** to see its dramatic effect on power
   - **Alter the population standard deviation** to understand how variability affects detectability
   - **Switch between one-tailed and two-tailed tests** to compare their power characteristics



   .. raw:: html

      <div class="embed-container">
      <iframe
         src="https://treese5.shinyapps.io/Power_Sim/"
         title="Power Analysis Interactive Visualization"
         allowfullscreen>
      </iframe>
      </div>


   **Key Observations to Make:**

   1. **Effect Size Impact**: Move the alternative mean closer to or farther from the null mean. Notice how larger effect sizes (greater separation) dramatically increase power.

   2. **Sample Size Power**: Increase the sample size and watch both distributions become narrower (smaller standard error), making it easier to distinguish between null and alternative hypotheses.

   3. **Alpha Trade-off**: Increase :math:`\alpha` and see how the red region (Type I error) grows, but the green region (power) also increases while the purple region (Type II error) shrinks.

   4. **Standard Deviation Effect**: Increase the population standard deviation and observe how both curves become wider, making it harder to detect differences and reducing power.

..
   Understanding the Code Behind the Visualization
   -----------------------------------------------

   The interactive app is built using R Shiny and demonstrates several key statistical computing concepts. Here's how the power calculation works programmatically:

   **Core Power Calculation Logic**

   .. code-block:: r

      # Calculate standard error of the sampling distribution
      std_error <- std_dev / sqrt(sample_size)
      
      # Find critical value based on hypothesis direction
      z_critical <- if (hypothesis == "greater") {
      qnorm(1 - alpha)  # Upper tail critical value
      } else if (hypothesis == "less") {
      qnorm(alpha)      # Lower tail critical value  
      } else {
      qnorm(1 - alpha/2) # Two-tailed critical value
      }
      
      # Calculate the cutoff value in original units
      cutoff <- null_mean + z_critical * std_error
      
      # Calculate power using the alternative distribution
      power <- if (hypothesis == "greater") {
      1 - pnorm(cutoff, alt_mean, std_error)
      } else if (hypothesis == "less") {
      pnorm(cutoff, alt_mean, std_error)
      } else {
      # Two-tailed case (more complex)
      upper_cutoff <- null_mean + z_critical * std_error
      lower_cutoff <- null_mean - z_critical * std_error
      (1 - pnorm(upper_cutoff, alt_mean, std_error)) + 
      pnorm(lower_cutoff, alt_mean, std_error)
      }

   **Visualization Strategy**

   The app creates two normal distributions:

   - **Null distribution**: Centered at :math:`\mu_0` with standard error :math:`\sigma/\sqrt{n}`
   - **Alternative distribution**: Centered at :math:`\mu_a` with the same standard error

   The colored regions represent:

   - **Red areas**: Type I error (:math:`\alpha`) - rejection region under null distribution
   - **Purple areas**: Type II error (:math:`\beta`) - non-rejection region under alternative distribution  
   - **Green areas**: Power (1-:math:`\beta`) - rejection region under alternative distribution

   **Why This Visualization Matters**

   This interactive approach helps students understand several crucial concepts that are often difficult to grasp from static diagrams alone:

   1. **Dynamic relationships**: See how changing one parameter affects all others simultaneously
   2. **Magnitude of effects**: Understand whether a change in sample size from 30 to 40 matters as much as a change from 30 to 100
   3. **Practical constraints**: Recognize why achieving very high power often requires impractically large sample sizes
   4. **Design decisions**: Appreciate the trade-offs researchers face when planning studies

   The app reinforces that power analysis isn't just a mathematical exercise‚Äîit's a practical tool for making informed decisions about study design and resource allocation.

Bringing It All Together
------------------------------

.. admonition:: Key Takeaways üìù
   :class: important

   1. **Hypothesis testing provides a framework** for evaluating specific claims about population 
      parameters using sample evidence. It consists of formally presenting the **null and alternative hypotheses**,
      determining the **significance level**, computing a **test statistic**, determining its strength,
      and drawing a **decision**.
   
   2. **Type I error** (false positive) occurs when a true null hypothesis is rejected. Its probability,
      denoted :math:`\alpha`, is the significance level of the test.
   
   3. **Type II error** (false negative) occurs whe a false null hypothesis is not rejected.
      It occurs with probability :math:`\beta`.
   
   4. **Statistical power** (1-:math:`\beta`) measures a test's ability to detect false null hypotheses.
      It depends on the sample size, significance level, and population standard deviation.

Exercises
---------

.. admonition:: Exercise 1: Writing Hypotheses Correctly
   :class: note

   For each research scenario, write the appropriate null and alternative hypotheses. Define the parameter of interest and identify whether the test is upper-tailed, lower-tailed, or two-tailed.

   a. A pharmaceutical company claims their new drug reduces average recovery time from 7 days. Researchers want to test this claim.

   b. A quality engineer suspects that a manufacturing process is producing bolts with mean diameter different from the target of 10 mm.

   c. An environmental agency wants to verify that mean pollution levels do not exceed the safety threshold of 50 ppm.

   d. A software company claims their new algorithm reduces average processing time below the industry standard of 200 ms.

   e. A nutritionist wants to test whether a new diet changes average weight loss from the typical 5 pounds per month.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Drug recovery time**

      Let Œº = true mean recovery time (days) with the new drug.

      .. math::
         H_0: \mu \geq 7 \quad \text{vs} \quad H_a: \mu < 7

      **Lower-tailed test** (testing if recovery time is *less than* 7 days)

      **Part (b): Bolt diameter**

      Let Œº = true mean bolt diameter (mm).

      .. math::
         H_0: \mu = 10 \quad \text{vs} \quad H_a: \mu \neq 10

      **Two-tailed test** (testing if diameter is *different from* target)

      **Part (c): Pollution levels**

      Let Œº = true mean pollution level (ppm).

      .. math::
         H_0: \mu \leq 50 \quad \text{vs} \quad H_a: \mu > 50

      **Upper-tailed test** (testing if pollution *exceeds* threshold)

      *Note*: This is the standard regulatory framing‚Äîwe protect against exceeding the threshold by placing it in H‚ÇÄ. Rejecting H‚ÇÄ triggers action.

      **Part (d): Processing time**

      Let Œº = true mean processing time (ms) with new algorithm.

      .. math::
         H_0: \mu \geq 200 \quad \text{vs} \quad H_a: \mu < 200

      **Lower-tailed test** (testing if time is *less than* standard)

      **Part (e): Weight loss**

      Let Œº = true mean monthly weight loss (pounds) with new diet.

      .. math::
         H_0: \mu = 5 \quad \text{vs} \quad H_a: \mu \neq 5

      **Two-tailed test** (testing if weight loss is *different from* typical)

----

.. admonition:: Exercise 2: Identifying Type I and Type II Errors
   :class: note

   For each scenario, describe in context what constitutes a Type I error and a Type II error.

   a. Testing whether a new battery lasts longer than 20 hours on average.

      - :math:`H_0: \mu \leq 20` vs :math:`H_a: \mu > 20`

   b. Testing whether a medical diagnostic test correctly identifies a disease (null: patient is healthy).

      - :math:`H_0:` Patient is healthy vs :math:`H_a:` Patient has disease

   c. Testing whether a defendant is guilty in a criminal trial.

      - :math:`H_0:` Defendant is innocent vs :math:`H_a:` Defendant is guilty

   d. For each scenario above, which error would you consider more serious? Explain.

   .. dropdown:: Solution
      :class-container: sd-border-success

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch10-1/fig1_type_errors.png
         :align: center
         :width: 90%
         :alt: Type I and Type II errors visualization

         Type I error (Œ±): Rejecting H‚ÇÄ when true. Type II error (Œ≤): Failing to reject H‚ÇÄ when false.

      **Part (a): Battery life test**

      - **Type I Error**: Conclude the battery lasts longer than 20 hours when it actually doesn't. The company might market an inferior product based on false claims.

      - **Type II Error**: Fail to detect that the battery lasts longer than 20 hours when it actually does. The company might miss an opportunity to market a superior product.

      **Part (b): Medical diagnostic test**

      - **Type I Error**: Diagnose disease when patient is healthy (false positive). Patient undergoes unnecessary treatment, experiences anxiety, and incurs costs.

      - **Type II Error**: Fail to detect disease when patient is sick (false negative). Patient doesn't receive needed treatment, potentially leading to worse outcomes.

      **Part (c): Criminal trial**

      - **Type I Error**: Convict an innocent person. An innocent person loses freedom and suffers unjust punishment.

      - **Type II Error**: Acquit a guilty person. A criminal goes free and may commit more crimes.

      **Part (d): Which error is more serious?**

      *Note*: The relative seriousness depends on context‚Äîcosts, consequences, and stakeholders vary by situation.

      - **Battery**: Type I error is more serious‚Äîit could lead to customer dissatisfaction, warranty costs, and damage to company reputation.

      - **Medical test**: Type II error is often more serious‚Äîmissing a disease can be life-threatening. However, this depends on the disease severity and treatment side effects.

      - **Criminal trial**: Type I error is generally considered more serious‚Äî"better that ten guilty persons escape than that one innocent suffer" (Blackstone's ratio). Our justice system is designed to minimize convicting innocents (Œ± is very small).

----

.. admonition:: Exercise 3: Understanding Significance Level
   :class: note

   a. Define the significance level (Œ±) in your own words.

   b. A researcher sets Œ± = 0.05. Interpret what this means in the context of hypothesis testing.

   c. If a researcher uses Œ± = 0.01 instead of Œ± = 0.05, how does this affect:

      (i) The probability of Type I error?
      
      (ii) The probability of Type II error?
      
      (iii) The power of the test?

   d. Why don't researchers always use a very small Œ± (like 0.001)?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Definition**

      The significance level (Œ±) is the maximum probability of committing a Type I error that the researcher is willing to tolerate. It represents our tolerance for incorrectly rejecting a true null hypothesis.

      **Part (b): Interpretation of Œ± = 0.05**

      The researcher accepts a 5% chance of rejecting H‚ÇÄ when H‚ÇÄ is actually true. If the test is repeated many times under conditions where H‚ÇÄ is true, about 5% of the tests would incorrectly reject H‚ÇÄ.

      **Part (c): Effects of reducing Œ± from 0.05 to 0.01**

      **(i)** Probability of Type I error **decreases** from 0.05 to 0.01.

      **(ii)** Probability of Type II error **increases**. Making it harder to reject H‚ÇÄ means we're more likely to fail to reject a false H‚ÇÄ.

      **(iii)** Power **decreases**. Since Power = 1 - Œ≤ and Œ≤ increases, power decreases.

      **Part (d): Why not always use very small Œ±?**

      - **Tradeoff with Type II error**: Very small Œ± dramatically increases Œ≤, making it very hard to detect real effects.

      - **Sample size requirements**: Achieving reasonable power with small Œ± requires much larger samples.

      - **Practical significance**: An extremely stringent Œ± may be unnecessary for many applications.

      - **Cost-benefit**: The cost of Type I vs Type II errors should guide Œ± selection, not a desire for extreme caution.

----

.. admonition:: Exercise 4: Error Probability Calculations
   :class: note

   A quality control test has Œ± = 0.05. The test has 80% power to detect when the process mean shifts from 100 to 105.

   a. What is the probability of a Type I error?

   b. What is the probability of a Type II error (when Œº = 105)?

   c. If we test 100 batches where the true mean is actually 100, how many would we expect to incorrectly reject?

   d. If we test 100 batches where the true mean has shifted to 105, how many would we expect to correctly detect this shift?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Type I error probability**

      P(Type I Error) = Œ± = **0.05**

      **Part (b): Type II error probability**

      Power = 1 - Œ≤ = 0.80, so Œ≤ = 1 - 0.80 = **0.20**

      **Part (c): False rejections out of 100 (H‚ÇÄ true)**

      Expected = n √ó Œ± = 100 √ó 0.05 = **5 batches**

      These would be false alarms‚Äîincorrectly flagging batches as having shifted when they haven't.

      **Part (d): Correct detections out of 100 (H‚ÇÄ false, Œº = 105)**

      Expected = n √ó Power = 100 √ó 0.80 = **80 batches**

      The remaining 20 batches (100 √ó 0.20) would fail to be detected despite the shift occurring.

----

.. admonition:: Exercise 5: Power Calculation
   :class: note

   A researcher tests whether a new teaching method improves average test scores. Historical data shows:
   
   - Current mean: Œº‚ÇÄ = 75 points
   - Population standard deviation: œÉ = 10 points
   - The researcher considers a 3-point improvement meaningful (Œº‚Çê = 78)
   - Sample size: n = 50 students
   - Significance level: Œ± = 0.05

   Calculate the power of this test to detect the improvement.

   a. State the hypotheses.

   b. Find the standard error of the sample mean.

   c. Find the critical value and cutoff for xÃÑ (under H‚ÇÄ).

   d. Calculate the power (probability of rejecting H‚ÇÄ when Œº = 78).

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Hypotheses**

      .. math::
         H_0: \mu \leq 75 \quad \text{vs} \quad H_a: \mu > 75

      This is an upper-tailed test.

      **Part (b): Standard error**

      .. math::
         SE = \frac{\sigma}{\sqrt{n}} = \frac{10}{\sqrt{50}} = 1.414

      **Part (c): Critical value and cutoff**

      For Œ± = 0.05 (upper-tailed): :math:`z_{0.05} = 1.645`

      Cutoff for :math:`\bar{x}`:

      .. math::
         \bar{x}_{cutoff} = \mu_0 + z_{\alpha} \times SE = 75 + 1.645 \times 1.414 = 77.33

      We reject H‚ÇÄ if :math:`\bar{x} > 77.33`.

      **Part (d): Power calculation**

      Power = P(reject H‚ÇÄ | Œº = 78) = P(:math:`\bar{X} > 77.33` | Œº = 78)

      Standardize using the alternative distribution (centered at Œº‚Çê = 78):

      .. math::
         P\left(Z > \frac{77.33 - 78}{1.414}\right) = P(Z > -0.474) = 1 - P(Z < -0.474)

      .. math::
         = 1 - 0.3169 = 0.6831

      **Power ‚âà 0.68 (68%)**

      **R verification:**

      .. code-block:: r

         mu_0 <- 75; mu_a <- 78; sigma <- 10; n <- 50; alpha <- 0.05
         SE <- sigma / sqrt(n)                    # 1.414
         z_alpha <- qnorm(alpha, lower.tail = FALSE)  # 1.645
         cutoff <- mu_0 + z_alpha * SE            # 77.33
         power <- pnorm(cutoff, mean = mu_a, sd = SE, lower.tail = FALSE)
         power  # 0.6831

----

.. admonition:: Exercise 6: Sample Size for Desired Power
   :class: note

   Using the scenario from Exercise 5, the researcher wants to achieve 80% power.

   a. What is the formula for calculating the required sample size?

   b. Find :math:`z_{\alpha}` and :math:`z_{\beta}` for 80% power at Œ± = 0.05.

   c. Calculate the minimum sample size needed.

   d. Verify your answer by calculating the power with the new sample size.

   e. How many additional students are needed compared to the original n = 50?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Sample size formula**

      For a one-sided test:

      .. math::
         n = \left(\frac{(z_{\alpha} + z_{\beta}) \sigma}{|\mu_0 - \mu_a|}\right)^2

      For a two-sided test, replace :math:`z_{\alpha}` with :math:`z_{\alpha/2}`:

      .. math::
         n = \left(\frac{(z_{\alpha/2} + z_{\beta}) \sigma}{|\mu_0 - \mu_a|}\right)^2

      **Part (b): Critical values**

      For Œ± = 0.05 (one-sided): :math:`z_{\alpha} = z_{0.05} = 1.645`

      For 80% power: Œ≤ = 0.20, so :math:`z_{\beta} = z_{0.20} = 0.842`

      **Part (c): Sample size calculation**

      .. math::
         n = \left(\frac{(1.645 + 0.842) \times 10}{|75 - 78|}\right)^2 = \left(\frac{2.487 \times 10}{3}\right)^2 = \left(8.29\right)^2 = 68.72

      Round up: **n = 69 students**

      **Part (d): Verification**

      With n = 69:

      .. math::
         SE = \frac{10}{\sqrt{69}} = 1.204

      .. math::
         \bar{x}_{cutoff} = 75 + 1.645 \times 1.204 = 76.98

      .. math::
         \text{Power} = P\left(Z > \frac{76.98 - 78}{1.204}\right) = P(Z > -0.847) = 0.802

      Power ‚âà 80.2% ‚úì

      **Part (e): Additional students needed**

      69 - 50 = **19 additional students**

      **R verification:**

      .. code-block:: r

         z_alpha <- qnorm(0.05, lower.tail = FALSE)  # 1.645
         z_beta <- qnorm(0.20, lower.tail = FALSE)   # 0.842
         sigma <- 10; effect <- 3
         
         n_required <- ((z_alpha + z_beta) * sigma / effect)^2
         ceiling(n_required)  # 69
         
         # Verify power
         SE <- sigma / sqrt(69)
         cutoff <- 75 + z_alpha * SE
         pnorm(cutoff, mean = 78, sd = SE, lower.tail = FALSE)  # 0.802

----

.. admonition:: Exercise 7: Factors Affecting Power
   :class: note

   For each change below, predict whether power will increase, decrease, or stay the same. Assume all other factors remain constant.

   a. Increase the sample size from n = 50 to n = 100.

   b. Decrease the significance level from Œ± = 0.05 to Œ± = 0.01.

   c. Increase the effect size from (Œº‚Çê - Œº‚ÇÄ) = 3 to (Œº‚Çê - Œº‚ÇÄ) = 5.

   d. The population standard deviation is actually œÉ = 15 instead of œÉ = 10.

   e. Change from a two-tailed test to a one-tailed test (in the correct direction).

   f. Explain why sample size has a "diminishing returns" effect on power.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Increase n from 50 to 100**

      Power **increases**. Larger sample ‚Üí smaller SE ‚Üí sampling distribution more concentrated ‚Üí easier to distinguish H‚ÇÄ from H‚Çê.

      **Part (b): Decrease Œ± from 0.05 to 0.01**

      Power **decreases**. Smaller Œ± ‚Üí more stringent rejection criterion ‚Üí harder to reject H‚ÇÄ ‚Üí less likely to detect true effects.

      **Part (c): Increase effect size from 3 to 5**

      Power **increases**. Larger effect ‚Üí H‚ÇÄ and H‚Çê distributions further apart ‚Üí easier to distinguish between them.

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch10-1/fig3_effect_size_power.png
         :align: center
         :width: 80%
         :alt: Effect size impact on power

         Larger effect sizes lead to higher power for the same sample size.

      **Part (d): œÉ = 15 instead of œÉ = 10**

      Power **decreases**. Larger œÉ ‚Üí larger SE ‚Üí more overlap between H‚ÇÄ and H‚Çê distributions ‚Üí harder to detect effects.

      **Part (e): Two-tailed to one-tailed test**

      Power **increases**. One-tailed test puts all Œ± in one direction ‚Üí lower critical value ‚Üí easier to reject H‚ÇÄ in that direction.

      **Part (f): Diminishing returns of sample size**

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch10-1/fig2_power_vs_n.png
         :align: center
         :width: 80%
         :alt: Power vs sample size

         Power increases with sample size but shows diminishing returns.

      Power depends on SE = œÉ/‚àön. Because n is under a square root:

      - Doubling n from 25 to 50 reduces SE by factor of ‚àö2 ‚âà 1.41
      - Doubling n from 100 to 200 also reduces SE by factor of ‚àö2 ‚âà 1.41

      But going from 25 to 50 is a smaller absolute increase (25) than 100 to 200 (100). The proportional improvement in power gets smaller as n increases, while the cost of additional samples stays constant or increases.

----

.. admonition:: Exercise 8: True/False Conceptual Questions
   :class: note

   Determine whether each statement is **True** or **False**. Provide a brief justification.

   a. The significance level Œ± is the probability of making a Type II error.

   b. If we reject H‚ÇÄ, we have proven that H‚ÇÄ is false.

   c. A larger sample size reduces the probability of both Type I and Type II errors.

   d. Power is the probability of correctly rejecting a false null hypothesis.

   e. If a test has 90% power, there is a 10% chance of committing a Type I error.

   f. The null hypothesis always contains an equality sign (=, ‚â§, or ‚â•).

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): False**

      Œ± is the probability of Type I error (rejecting a true H‚ÇÄ), not Type II error. Type II error probability is Œ≤.

      **Part (b): False**

      Rejecting H‚ÇÄ means the data provides sufficient evidence against H‚ÇÄ, but it doesn't *prove* H‚ÇÄ is false. There's always a chance (Œ±) of incorrectly rejecting a true H‚ÇÄ.

      **Part (c): False**

      Larger sample size reduces Type II error (increases power), but Type I error probability is controlled at Œ± regardless of sample size. Œ± is set by the researcher, not determined by n.

      **Part (d): True**

      Power = 1 - Œ≤ = P(reject H‚ÇÄ | H‚ÇÄ is false). It's the probability of detecting a real effect.

      **Part (e): False**

      Power = 1 - Œ≤ = 0.90 means Œ≤ = 0.10, so there's a 10% chance of Type II error (failing to reject a false H‚ÇÄ). Type I error probability is Œ±, which is set separately.

      **Part (f): True**

      H‚ÇÄ always includes an equality because it represents the status quo or baseline assumption. The three forms are: H‚ÇÄ: Œº = Œº‚ÇÄ, H‚ÇÄ: Œº ‚â§ Œº‚ÇÄ, or H‚ÇÄ: Œº ‚â• Œº‚ÇÄ.

----

.. admonition:: Exercise 9: Application - Clinical Trial Planning
   :class: note

   A pharmaceutical company is planning a clinical trial to test whether a new drug reduces blood pressure. Historical data shows:

   - Current treatment mean: Œº‚ÇÄ = 140 mmHg
   - Population standard deviation: œÉ = 15 mmHg
   - Clinically meaningful reduction: 5 mmHg (so Œº‚Çê = 135 mmHg)
   - Desired power: 90%
   - Significance level: Œ± = 0.05

   a. Write the hypotheses for this test.

   b. Calculate the minimum sample size needed.

   c. If the budget only allows for n = 50 patients, what power will the study have?

   d. With n = 50, what is the minimum effect size detectable with 90% power?

   e. Discuss the practical implications of these results for the study design.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Hypotheses**

      Let Œº = true mean blood pressure with new drug.

      .. math::
         H_0: \mu \geq 140 \quad \text{vs} \quad H_a: \mu < 140

      This is a lower-tailed test (testing if blood pressure is reduced).

      **Part (b): Required sample size for 90% power**

      For 90% power: :math:`z_{\beta} = z_{0.10} = 1.282`

      For Œ± = 0.05 (one-sided): :math:`z_{\alpha} = z_{0.05} = 1.645`

      .. math::
         n = \left(\frac{(1.645 + 1.282) \times 15}{5}\right)^2 = \left(\frac{43.905}{5}\right)^2 = (8.781)^2 = 77.1

      **Minimum n = 78 patients**

      **Part (c): Power with n = 50**

      .. math::
         SE = \frac{15}{\sqrt{50}} = 2.121

      Cutoff (lower-tailed): :math:`\bar{x}_{cutoff} = 140 - 1.645 \times 2.121 = 136.51`

      Power = P(:math:`\bar{X} < 136.51` | Œº = 135):

      .. math::
         P\left(Z < \frac{136.51 - 135}{2.121}\right) = P(Z < 0.712) = 0.762

      **Power ‚âà 76.2%** with n = 50

      **Part (d): Minimum detectable effect with n = 50 and 90% power**

      Rearranging the sample size formula:

      .. math::
         |\mu_0 - \mu_a| = \frac{(z_{\alpha} + z_{\beta}) \sigma}{\sqrt{n}} = \frac{(1.645 + 1.282) \times 15}{\sqrt{50}} = \frac{43.905}{7.07} = 6.21

      With n = 50, need at least a **6.2 mmHg reduction** to achieve 90% power.

      **Part (e): Practical implications**

      1. **Budget constraint is significant**: With only 50 patients, power drops from 90% to 76%, meaning there's nearly a 1-in-4 chance of missing a real 5 mmHg effect.

      2. **Recruitment challenge**: Need 78 patients for adequate power‚Äî56% more than budget allows.

      3. **Effect size consideration**: A 5 mmHg reduction may be clinically meaningful, but the study as designed can only reliably detect a 6+ mmHg effect.

      4. **Recommendations**:
         - Seek additional funding for larger sample
         - Consider whether 76% power is acceptable given the study's importance
         - Explore whether a larger effect size is realistic based on mechanism of action
         - Consider adaptive trial designs

      **R verification:**

      .. code-block:: r

         # Sample size
         z_alpha <- qnorm(0.05, lower.tail = FALSE)  # 1.645
         z_beta_90 <- qnorm(0.10, lower.tail = FALSE)  # 1.282
         n_req <- ((z_alpha + z_beta_90) * 15 / 5)^2
         ceiling(n_req)  # 78
         
         # Power with n = 50
         SE_50 <- 15 / sqrt(50)
         cutoff <- 140 - z_alpha * SE_50
         pnorm(cutoff, mean = 135, sd = SE_50)  # 0.762
         
         # Minimum detectable effect
         (z_alpha + z_beta_90) * 15 / sqrt(50)  # 6.21

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. Type I error is also called a "false negative."

   ‚ìâ or ‚íª

2. Increasing the significance level Œ± increases statistical power.

   ‚ìâ or ‚íª

3. The alternative hypothesis is what we assume to be true until proven otherwise.

   ‚ìâ or ‚íª

4. Power and Œ≤ are complements (Power = 1 - Œ≤).

   ‚ìâ or ‚íª

5. A two-tailed test has more power than a one-tailed test (same Œ±).

   ‚ìâ or ‚íª

6. The sample size needed for a given power depends on the effect size.

   ‚ìâ or ‚íª

**Multiple Choice Questions** (2 points each)

7. A Type II error occurs when we:

   ‚í∂ Reject H‚ÇÄ when H‚ÇÄ is true
   
   ‚í∑ Fail to reject H‚ÇÄ when H‚ÇÄ is true
   
   ‚í∏ Reject H‚ÇÄ when H‚ÇÄ is false
   
   ‚íπ Fail to reject H‚ÇÄ when H‚ÇÄ is false

8. If a test has power = 0.85, then Œ≤ equals:

   ‚í∂ 0.85
   
   ‚í∑ 0.15
   
   ‚í∏ 0.05
   
   ‚íπ Cannot be determined

9. Which action would INCREASE statistical power?

   ‚í∂ Decrease sample size
   
   ‚í∑ Decrease Œ± from 0.05 to 0.01
   
   ‚í∏ Increase population standard deviation
   
   ‚íπ Increase effect size

10. For the hypotheses H‚ÇÄ: Œº ‚â§ 50 vs H‚Çê: Œº > 50, this is a:

    ‚í∂ Lower-tailed test
    
    ‚í∑ Upper-tailed test
    
    ‚í∏ Two-tailed test
    
    ‚íπ None of the above

11. To achieve 80% power, :math:`z_{\beta}` equals:

    ‚í∂ 0.80
    
    ‚í∑ 0.20
    
    ‚í∏ 0.84
    
    ‚íπ 1.28

12. If the effect size doubles (with all else constant), the required sample size:

    ‚í∂ Doubles
    
    ‚í∑ Halves
    
    ‚í∏ Quadruples
    
    ‚íπ Reduces to one-quarter

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **False** ‚Äî Type I error is a "false positive" (incorrectly rejecting). Type II is the "false negative."

   2. **True** ‚Äî Larger Œ± makes it easier to reject H‚ÇÄ, increasing power (but also increasing Type I error risk).

   3. **False** ‚Äî The null hypothesis is assumed true until evidence suggests otherwise. The alternative is what we're trying to find evidence for.

   4. **True** ‚Äî Power = P(reject H‚ÇÄ | H‚ÇÄ false) = 1 - P(fail to reject | H‚ÇÄ false) = 1 - Œ≤.

   5. **False** ‚Äî A one-tailed test has more power (in the specified direction) because all of Œ± is concentrated in one tail.

   6. **True** ‚Äî Larger effects are easier to detect, requiring smaller samples.

   **Multiple Choice Answers:**

   7. **‚íπ** ‚Äî Type II error = failing to reject a false H‚ÇÄ.

   8. **‚í∑** ‚Äî Power = 1 - Œ≤, so 0.85 = 1 - Œ≤ ‚Üí Œ≤ = 0.15.

   9. **‚íπ** ‚Äî Larger effects are easier to detect, increasing power.

   10. **‚í∑** ‚Äî H‚Çê: Œº > 50 is an upper-tailed (right-tailed) alternative.

   11. **‚í∏** ‚Äî For 80% power, Œ≤ = 0.20, and z‚ÇÄ.‚ÇÇ‚ÇÄ = qnorm(0.20, lower.tail = FALSE) ‚âà 0.84.

   12. **‚íπ** ‚Äî n ‚àù 1/(effect size)¬≤, so doubling effect size reduces n by factor of 4.