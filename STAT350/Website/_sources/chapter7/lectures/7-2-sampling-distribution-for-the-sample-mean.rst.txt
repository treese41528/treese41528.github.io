.. _7-2-sampling-distribution-for-the-sample-mean:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch7-2">
      <iframe
         id="video-ch7-2"
         title="STAT 350 â€“ Chapter 7.2 Sampling Distribution for the Sample Mean Video"
         src="https://www.youtube.com/embed/TIOCX2hjXqw?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides ðŸ“Š
   :class: tip

   `Download Chapter 7 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/
   Chapter%207%20Sampling%20Distributions/Sampling%20Distributions%20%28Chapter7%29_AC.pptx>`_
   
Sampling Distribution for the Sample Mean
========================================================

Having established that statistics are random variables with their own distributions, we now focus on the most 
important statistic in all of statistical inference: the sample mean :math:`\bar{X}`. 

.. admonition:: Road Map ðŸ§­
   :class: important

   * View the **sample mean** :math:`\bar{X}` as a function of :math:`n` independent and identically distributed
     random variables. Establish :math:`E[\bar{X}]` and :math:`\text{Var}(\bar{X})` in relation to the distributional properties of
     these building blocks.
   * Define the standard deviation :math:`\sigma_{\bar{X}}` of the sample mean as the **standard error** and understand
     how it is influenced by the population standard deviation and sample size.

A New Perspective on the Data-Generating Procedure
-----------------------------------------------------------

So far, we've pictured the sampling procedure as drawing
individual datapoints :math:`n` different times from a single random variable :math:`X` (left of 
:numref:`new-sampling-framework`).

.. _new-sampling-framework:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter7/new-sampling-framework.png
   :alt: A new perspective of understanding how data points are sampled
   :figwidth: 70%
   :align: center

   Left represents how we used to think of the sampling procedure;
   we now think in the perspective on the right



For the formal understanding of the sampling distribution of :math:`\bar{X}`, we need to
begin with a new perspective. Imagine that there are :math:`n` **independent and identically distributed (iid) 
copies of the population**, :math:`X_1, X_2, \cdots, X_n`, and a sample
is constructed by taking one data point from each copy (right of :numref:`new-sampling-framework`). 

Through this shift, we can now express the sample mean :math:`\bar{X}` as a function of :math:`n` random variables:

.. math::

   \bar{X} = \frac{1}{n}\sum_{i=1}^n X_i

This allows us to break down the properties of the random variable :math:`\bar{X}` in terms of its building blocks
:math:`X_1, X_2, \cdots, X_n`, with which we are more familiar.

Visualizing Sampling Distributions
------------------------------------------------------------

Let's get a feel for how sampling distributions behave with a concrete visual example.

The Population: Exponential Distribution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a population that follows an exponential distribution with parameter :math:`\lambda = 1`. 
Recall that this distribution is highly right-skewed, with most values bunched near 0 and a 
long tail extending to the right. The population mean is :math:`\mu = 1/\lambda = 1`, and the population 
standard deviation is :math:`\sigma = 1/\lambda = 1`.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter7/exponential-pdf.png
   :alt: The pdf of exponential distribution
   :width: 70%
   :align: center
   
   The exponential population: highly right-skewed with mean :math:`\mu=1`

When we conduct statistical inference in practice, we won't know the population follows an 
exponential distribution or what its parameter value is. For now, we'll assume this knowledge 
so we can compare our sample results to the known truth.

Sampling with n = 5
~~~~~~~~~~~~~~~~~~~~~~~

Let's start by taking one sample of size :math:`n = 5` from this population. The code below
samples five numbers from the population and computes the average: 

.. code-block:: r

   # Take one sample of size 5
   sample1 <- rexp(5, rate = 1)
   sample_mean1 <- mean(sample1)
   # Result: 0.39

We repeat this process many times (``num_samples``). Each repetition samples a different set of
five numbers and thus produces a different sample mean.

.. code-block:: r

   # Simulate the sampling distribution
   num_samples <- 1000000
   n <- 5
   sample_means <- replicate(num_samples, mean(rexp(n, rate = 1)))

When we plot the distribution of these million sample means, we see something remarkable. 
The distribution no longer looks like the original exponential distribution. It's still somewhat right-skewed,
but the degree of skewness has diminished. The sample means cluster more tightly around the true 
population mean :math:`\mu=1`.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter7/sampling-dist-n5.png
   :alt: Histogram of sample means when :math:`n=5`
   :width: 70%
   :align: center

The Effect of Increasing Sample Size
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now let's see what happens when we increase the sample size to :math:`n = 25`:

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter7/sampling-dist-n25.png
   :alt: Histogram of sample means when :math:`n=25`
   :width: 70%
   :align: center

The transformation is dramatic. The sampling distribution is now roughly symmetric and centered around :math:`\mu = 1`. 
It bears little resemblance to the original exponential population. The sample means are much more concentrated 
around the true valueâ€”most fall between 0.5 and 1.5.

With :math:`n = 65`, the pattern becomes even more pronounced:

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter7/sampling-dist-n65.png
   :alt: Histogram of sample means when :math:`n=65`
   :width: 70%
   :align: center

Now the distribution is highly concentrated around :math:`\mu = 1` and appears very symmetric. 
The sample means rarely stray far from the true population mean.

Key Insights
~~~~~~~~~~~~~~~

1. **The sample mean targets the population mean**: All sampling distributions center around :math:`\mu = 1`, 
   regardless of sample size.

2. **Larger samples produce more precise estimates**: As :math:`n` increases, the sampling distribution becomes 
   more concentrated around :math:`\mu`.

3. **Shape changes with sample size**: Even though the population is highly skewed, the sampling distribution 
   becomes more symmetric as :math:`n` increases.

4. **The magic of averaging**: By averaging multiple observations, we reduce the impact of extreme values and 
   create estimators that behave better than individual observations.


Deriving the Mathematical Properties
---------------------------------------

To deepen our understanding of the sample mean's behavior, we derive its key distributional properties: 
the mean, variance, and standard deviation. For clarity,
all population parameters are written with a subscript :math:`X` and all
sampling distribution parameters with a subscript :math:`\bar{X}`.

A. Expected Value of the Sample Mean
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

   \mu_{\bar{X}} = E[\bar{X}] = E\left[\frac{1}{n}\sum_{i=1}^n X_i\right]
   = \frac{1}{n} E\left[\sum_{i=1}^n X_i\right] = \frac{1}{n} \sum_{i=1}^n E[X_i]

Since all :math:`X_i`'s come from the same distribution with :math:`E[X_i] = \mu_X`,

.. math::

   E[\bar{X}] = \frac{1}{n} \sum_{i=1}^n \mu_X = \frac{1}{n} \cdot n\mu_X = \mu_X

Unbiasedness of Sample Mean
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The **expected value of the sample mean equals the population mean** (:math:`\mu_{\bar{X}} = \mu_X`).
When an estimator equals its target on average, we call it an **unbiased** estimator. 
Individual sample means may be too high or too low, but they center around the correct target.

B. Variance and Standard Error of the Sample Mean
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

   \sigma^2_{\bar{X}}=\text{Var}(\bar{X}) =\text{Var}\left(\frac{1}{n}\sum_{i=1}^n X_i\right)
   = \frac{1}{n^2} \text{Var}\left(\sum_{i=1}^n X_i\right)

Since the :math:`X_i`'s are independent, the variance of the sum equals the sum of the variances.
Also, all :math:`X_i`'s have the same variance :math:`\sigma_X^2`:

.. math::

   \text{Var}(\bar{X}) = \frac{1}{n^2} \sum_{i=1}^n\text{Var}(X_i)
   = \frac{1}{n^2} \cdot n\sigma^2_X = \frac{\sigma^2_X}{n}

We call the standard deviation of the sample mean the **standard error**. It is the positive square root
of the variance of :math:`\bar{X}`.

.. math::

   \sigma_{\bar{X}} = \sqrt{\text{Var}(\bar{X})} = \frac{\sigma_X}{\sqrt{n}}

Understanding the Standard Error
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For even modest sample sizes, **sample means are much less variable** than individual observations. 
With :math:`n = 25`, for example, the sample mean has standard error :math:`\frac{\sigma}{5}`, making it five times 
more precise than any single observation with standard deviation :math:`\sigma`.

This concentration effect explains why averaging is such a powerful statistical technique 
and why larger samples are usually better. By combining information from multiple observations, we create estimators that are more 
reliable than an individual measurement.


C. Summary of Basic Distributional Properties of :math:`\bar{X}`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. flat-table::
   :header-rows: 1
   :align: center
   :width: 90%

   * - Name 
     - Notation
     - Formula
   
   * - **Expected Value**
     - :math:`E[\bar{X}]` or :math:`\mu_{\bar{X}}`
     - :math:`\mu_X`

   * - **Variance**
     - :math:`\text{Var}(\bar{X})` or :math:`\sigma_{\bar{X}}^2`
     - :math:`\frac{\sigma_X^2}{n}`

   * - **Standard Error**
     - :math:`\sigma_{\bar{X}}`
     - :math:`\frac{\sigma_X}{\sqrt{n}}`

.. admonition:: ExampleðŸ’¡: Maze Navigation Times
   :class: note 

   Researchers study how long it takes rats of a certain subspecies to navigate through a maze. 
   Previous research suggests that navigation times have a mean :math:`\mu_X = 1.5` minutes and a
   standard deviation :math:`\sigma_X=0.35` minutes.

   The researchers select five rats at random and want to understand the **behavior of the average navigation 
   time for their sample**. What are the mean and the standard error of the sampling distribution for the 
   sample mean?

   **Setting Up the Problem**

   We have:

   - :math:`X_i` are *iid* with :math:`E[X_i]=1.5` and :math:`\text{Var}(X_i)=0.35^2`
     for each :math:`i \in \{1,2,3,4,5\}`
   - :math:`n = 5`

   **Mean of the Sample Mean**

   .. math::

      \mu_{\bar{X}} = \mu_X = 1.5 \text{ minutes}

   **Standard Error of the Sample Mean**

   .. math::

      \sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}} = \frac{0.35}{\sqrt{5}} = \frac{0.35}{2.236} = 0.1565 \text{ minutes}


The Special Case: Normal Populations
---------------------------------------

While our mathematical results **apply to any population with finite mean and variance**, 
there's **one special case where we can say much more** about the shape of the sampling 
distribution: when the population follows a normal distribution.

Linear Combinations of Normal Random Variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A key property of normal distributions is that **linear combinations of normal random 
variables are themselves normal**. That is, if :math:`X` and :math:`Y` are 
normal random variables, then any linear combination of the form :math:`aX + bY + c` is also normal.

The sample mean is exactly such a linear combination:

.. math::

   \bar{X} = \frac{1}{n}X_1 + \frac{1}{n}X_2 + \cdots + \frac{1}{n}X_n

The Exact Distribution of :math:`\bar{X}` from Normal Population
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If :math:`X_1, X_2, \cdots, X_n` are iid from a normal distribution with mean 
:math:`\mu_X` and standard deviation :math:`\sigma_X`, then:

.. math::

   \bar{X} \sim N\left(\mu_X, \frac{\sigma_X^2}{n}\right) \quad \text{ or equivalently,} \quad
   \bar{X} \sim N\left(\mu_X, \frac{\sigma_X}{\sqrt{n}}\right)   

This result is remarkable because it tells us the **exact** sampling distribution, 
not just its mean and variance.

.. admonition:: ExampleðŸ’¡: Maze Navigation Times, Continued
   :class: note 

   Researchers study how long it takes rats of a certain subspecies to navigate through a maze. 
   In addition to the parameters  :math:`\mu = 1.5` minutes and  :math:`\sigma = 0.35` minutes,
   it is known that **the population of navigation times follow normal distribution**.

   **Setting Up the Problem**

   From the previous example, we have

   - :math:`\mu_{\bar{X}} = 1.5`
   - :math:`\sigma_{\bar{X}} = 0.1565`

   Since the population follows normal distribution, the sampling distribution for
   the sample mean must also be normal. We have:

   .. math:: \bar{X} \sim N(\mu_{\bar{X}} = 1.5, \sigma^2_{\bar{X}} = 0.1565^2)

   **Computing Probabilities**

   What's the probability that the average navigation time for five rats exceeds 1.75 minutes?
      
   We need to find :math:`P(\bar{X} > 1.75)`. Since :math:`\bar{X} \sim N(1.5, 0.1565^2)`, we use
   the standardization technique and the Z-table (or a statistical software) to compute:

   .. math::

      &P(\bar{X} > 1.75) = P\left(\frac{\bar{X} - 1.5}{0.1565} > \frac{1.75 - 1.5}{0.1565}\right)\\
      &= P(Z > 1.60) = 1 - \Phi(1.60) = 1 - 0.9452 = 0.0548

   There's 0.0548 probability that the average navigation time for five randomly selected 
   rats will exceed 1.75 minutes.

Additional Example: Quality Control in Manufacturing
-------------------------------------------------------

Let us conclude this section by solving a problem applying the CLT to decision-making in
in quality control.

.. admonition:: Example ðŸ’¡: Quality Control in Manufacturing
   :class: note

   The Bulls Eye Production company manufactures a number of high precision tools. 
   Under the usual production process, one of these tools has a **mean diameter of 5mm**. 
   The measurement varies **normally** aroud this mean, with **standard deviation of 0.5mm**.

   However, the machine will need to be frequently recalibrated due to the strenuous operating conditions. 
   Recalibration is required anytime the difference between 
   the **observed sample mean diameter** and **the ideal diameter** is too large. "Large" is 
   measured probabilistically; if the probability of the deviation is *rarer*
   than 0.05, then the difference is considered too large.

   A **random sample of size 64** is taken to assess the need for recalibration. It is found that the average diameter of 
   the sample is **4.85mm**. Is recalibration necessary?

   **Setting Up the Problem**
   
   It is given that 

   * :math:`n=64`
   * :math:`\mu_X = 5` and :math:`\sigma_X = 0.5`
   * The population is normally distributed.
   * The sample is randomly collected from the same population, which allows us to assume the iid condition.
   * A single realization from :math:`\bar{X}` has value :math:`\bar{x} = 4.85`.

   **Solving the Problem**

   We must compute a probability representing how *rare* the current difference :math:`|\bar{x}-\mu_X|` is
   when compared with the general behavior of :math:`|\bar{X}-\mu_X|`.

   .. math::
      &P(|\bar{X}-\mu_X| > |\bar{x}-\mu_X|)\\
      &= P(|\bar{X}-\mu_X| > |4.85-5|)\\
      &=P(\Bigg|\frac{\bar{X}-\mu_X}{\sigma_X/\sqrt{n}}\Bigg| > \frac{|4.85-5|}{0.5/\sqrt{64}})\\
      &=P(|Z| > 2.4) = P(Z > 2.4) + P(Z < -2.4)\\
      &=\underbrace{2P(Z < -2.4)}_{\text{by symmetry around } 0} = 0.0164

   The probability of seeing an even larger difference than the current observation is only 0.0164,
   which is smaller than 0.05. Therefore, the machine must be recalibrated.

Bringing It All Together
-----------------------------

.. admonition:: Key Takeaways ðŸ“
   :class: important

   1. The sample mean :math:`\bar{X}` is a random variable. Its probability distribution is 
      called the sampling distribution of the sample mean.
   
   2. If the population has mean :math:`\mu_X` and variance :math:`\sigma_X^2`, then 
      :math:`\mu_{\bar{X}} = E[\bar{X}] = \mu_X` and :math:`\sigma^2_{\bar{X}} = Var(\bar{X}) = \sigma_X^2/n`.
   
   3. If the population has a distribution :math:`N(\mu_X, \sigma_X^2)`, 
      then the sampling distribution of :math:`\bar{X}` is completely known: :math:`\bar{X} \sim N(\mu_X, \sigma_X^2/n)`.


Exercises
---------

These exercises develop your skills in working with the sampling distribution of the sample mean, including computing the expected value and standard error, finding probabilities when the population is normal, and determining sample sizes for desired precision.

.. admonition:: Key Formulas
   :class: tip

   For a random sample of size :math:`n` from a population with mean :math:`\mu` and standard deviation :math:`\sigma`:

   - **Expected Value**: :math:`E[\bar{X}] = \mu_{\bar{X}} = \mu`
   - **Variance**: :math:`\text{Var}(\bar{X}) = \sigma^2_{\bar{X}} = \frac{\sigma^2}{n}`
   - **Standard Error**: :math:`\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}`

   **Special Case â€” Normal Population**: If the population is :math:`N(\mu, \sigma^2)`, then:

   .. math::
      \bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)

.. admonition:: R Functions for Normal Probabilities and Quantiles
   :class: tip

   For :math:`\bar{X} \sim N(\mu_{\bar{X}}, \sigma_{\bar{X}})`:

   .. code-block:: r

      # Probability calculations
      pnorm(x, mean = mu, sd = sigma_xbar)               # P(XÌ„ â‰¤ x)
      pnorm(x, mean = mu, sd = sigma_xbar, lower.tail = FALSE)  # P(XÌ„ > x)

      # Quantile (inverse CDF) - find x such that P(XÌ„ â‰¤ x) = p
      qnorm(p, mean = mu, sd = sigma_xbar)

   **Example**: For :math:`\bar{X} \sim N(100, 4)` (mean 100, SD 2):

   .. code-block:: r

      pnorm(102, mean = 100, sd = 2)                # P(XÌ„ â‰¤ 102) = 0.8413
      pnorm(102, mean = 100, sd = 2, lower.tail = FALSE)  # P(XÌ„ > 102) = 0.1587
      qnorm(0.95, mean = 100, sd = 2)              # 95th percentile = 103.29

.. admonition:: Important Note
   :class: warning

   The formulas for :math:`E[\bar{X}]` and :math:`\text{Var}(\bar{X})` hold for **any** population with finite mean and variance. However, we can only determine the **exact shape** of the sampling distribution when the population is normal. For non-normal populations, the Central Limit Theorem (Section 7.3) provides an approximation for large samples.

.. admonition:: Exercise 1: Basic Properties of the Sampling Distribution
   :class: note

   The tensile strength of a certain type of steel cable is normally distributed with mean :math:`\mu = 850` pounds and standard deviation :math:`\sigma = 40` pounds. A quality control engineer selects a random sample of :math:`n = 16` cables for testing.

   a. What is the expected value of the sample mean tensile strength?

   b. What is the standard error of the sample mean?

   c. What is the complete sampling distribution of :math:`\bar{X}`?

   d. Compare the standard error to the population standard deviation. What does this tell you about the precision of the sample mean versus a single observation?

   .. dropdown:: Solution
      :class-container: sd-border-success

      Let :math:`X` = tensile strength (pounds), where :math:`X \sim N(\mu = 850, \sigma = 40)` and :math:`n = 16`.

      **Part (a): Expected value of XÌ„**

      .. math::
         E[\bar{X}] = \mu = 850 \text{ pounds}

      The sample mean is centered at the population mean.

      **Part (b): Standard error**

      .. math::
         \sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}} = \frac{40}{\sqrt{16}} = \frac{40}{4} = 10 \text{ pounds}

      **Part (c): Complete sampling distribution**

      Since the population is normal, the sampling distribution is also normal:

      .. math::
         \bar{X} \sim N\left(850, 10^2\right) \quad \text{or equivalently} \quad \bar{X} \sim N(850, 100)

      **Part (d): Precision comparison**

      The standard error (10 pounds) is one-fourth of the population standard deviation (40 pounds). This means the sample mean of 16 cables is **4 times more precise** than a single cable measurement. Averaging reduces variability by a factor of :math:`\sqrt{n} = \sqrt{16} = 4`.

----

.. admonition:: Exercise 2: Probability Calculations with Normal Population
   :class: note

   The diameter of ball bearings produced by a machine is normally distributed with mean :math:`\mu = 5.00` mm and standard deviation :math:`\sigma = 0.10` mm. A random sample of :math:`n = 25` ball bearings is selected.

   a. Find :math:`P(\bar{X} > 5.03)`.

   b. Find :math:`P(\bar{X} < 4.96)`.

   c. Find :math:`P(4.97 < \bar{X} < 5.03)`.

   d. Find the value :math:`c` such that :math:`P(\bar{X} > c) = 0.10`.

   .. dropdown:: Solution
      :class-container: sd-border-success

      Let :math:`X` = diameter (mm), where :math:`X \sim N(5.00, 0.10^2)` and :math:`n = 25`.

      First, find the sampling distribution parameters:

      - :math:`E[\bar{X}] = 5.00` mm
      - :math:`\sigma_{\bar{X}} = \frac{0.10}{\sqrt{25}} = \frac{0.10}{5} = 0.02` mm

      Since the population is normal: :math:`\bar{X} \sim N(5.00, 0.02^2)`.

      **Part (a): P(XÌ„ > 5.03)**

      .. math::
         P(\bar{X} > 5.03) = P\left(Z > \frac{5.03 - 5.00}{0.02}\right) = P(Z > 1.50)

      .. math::
         = 1 - \Phi(1.50) = 1 - 0.9332 = 0.0668

      **R verification:**

      .. code-block:: r

         pnorm(5.03, mean = 5.00, sd = 0.02, lower.tail = FALSE)
         # [1] 0.0668

      **Part (b): P(XÌ„ < 4.96)**

      .. math::
         P(\bar{X} < 4.96) = P\left(Z < \frac{4.96 - 5.00}{0.02}\right) = P(Z < -2.00)

      .. math::
         = \Phi(-2.00) = 0.0228

      **R verification:**

      .. code-block:: r

         pnorm(4.96, mean = 5.00, sd = 0.02)
         # [1] 0.0228

      **Part (c): P(4.97 < XÌ„ < 5.03)**

      .. math::
         P(4.97 < \bar{X} < 5.03) = P\left(\frac{4.97 - 5.00}{0.02} < Z < \frac{5.03 - 5.00}{0.02}\right)

      .. math::
         = P(-1.50 < Z < 1.50) = \Phi(1.50) - \Phi(-1.50) = 0.9332 - 0.0668 = 0.8664

      **R verification:**

      .. code-block:: r

         pnorm(5.03, mean = 5.00, sd = 0.02) - pnorm(4.97, mean = 5.00, sd = 0.02)
         # [1] 0.8664

      **Part (d): Find c such that P(XÌ„ > c) = 0.10**

      We need the 90th percentile of :math:`\bar{X}`.

      From the Z-table: :math:`\Phi(1.28) = 0.8997 \approx 0.90`, so :math:`z_{0.90} \approx 1.28`.

      .. math::
         c = \mu + z_{0.90} \cdot \sigma_{\bar{X}} = 5.00 + 1.28(0.02) = 5.00 + 0.0256 = 5.026 \text{ mm}

      **R verification:**

      .. code-block:: r

         qnorm(0.90, mean = 5.00, sd = 0.02)
         # [1] 5.0256

----

.. admonition:: Exercise 3: Standard Error and Sample Size
   :class: note

   A biomedical engineer is measuring the response time of a neural sensor. The population standard deviation is known to be :math:`\sigma = 8` milliseconds.

   a. If a sample of :math:`n = 16` measurements is taken, what is the standard error of :math:`\bar{X}`?

   b. If the sample size is increased to :math:`n = 64`, what is the new standard error?

   c. By what factor did the standard error decrease when the sample size was quadrupled?

   d. What sample size is needed to achieve a standard error of at most 1 millisecond?

   e. A colleague claims that doubling the sample size will cut the standard error in half. Is this correct? Explain.

   .. dropdown:: Solution
      :class-container: sd-border-success

      Given: :math:`\sigma = 8` ms.

      **Part (a): SE with n = 16**

      .. math::
         \sigma_{\bar{X}} = \frac{8}{\sqrt{16}} = \frac{8}{4} = 2 \text{ ms}

      **Part (b): SE with n = 64**

      .. math::
         \sigma_{\bar{X}} = \frac{8}{\sqrt{64}} = \frac{8}{8} = 1 \text{ ms}

      **Part (c): Factor of decrease**

      The standard error decreased from 2 ms to 1 ms, a factor of **2**.

      When sample size quadruples (Ã—4), the standard error decreases by :math:`\sqrt{4} = 2`.

      In general: :math:`\frac{\text{SE}_{\text{old}}}{\text{SE}_{\text{new}}} = \frac{\sigma/\sqrt{n_{\text{old}}}}{\sigma/\sqrt{n_{\text{new}}}} = \sqrt{\frac{n_{\text{new}}}{n_{\text{old}}}} = \sqrt{\frac{64}{16}} = 2`.

      **Part (d): Sample size for SE â‰¤ 1 ms**

      We need :math:`\frac{\sigma}{\sqrt{n}} \leq 1`, which gives :math:`\sqrt{n} \geq \sigma = 8`, so :math:`n \geq 64`.

      Minimum sample size: **n = 64**.

      **Part (e): Does doubling n halve the SE?**

      **No, this is incorrect.** Doubling the sample size reduces the standard error by a factor of :math:`\sqrt{2} \approx 1.41`, not 2.

      If :math:`n \to 2n`, then :math:`\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}} \to \frac{\sigma}{\sqrt{2n}} = \frac{\sigma}{\sqrt{2}\sqrt{n}}`.

      To **halve** the standard error, you must **quadruple** the sample size.

----

.. admonition:: Exercise 4: Comparing Individual Observations to Sample Means
   :class: note

   CPU processing times for a certain task are normally distributed with mean :math:`\mu = 120` ms and standard deviation :math:`\sigma = 15` ms.

   a. What is the probability that a **single** randomly selected task takes more than 130 ms?

   b. What is the probability that the **average** of :math:`n = 9` randomly selected tasks exceeds 130 ms?

   c. What is the probability that the **average** of :math:`n = 36` randomly selected tasks exceeds 130 ms?

   d. Explain why the probabilities in (a), (b), and (c) are different.

   .. dropdown:: Solution
      :class-container: sd-border-success

      Let :math:`X` = processing time (ms), where :math:`X \sim N(120, 15^2)`.

      **Part (a): Single observation, P(X > 130)**

      For a single observation, we use the population distribution directly:

      .. math::
         P(X > 130) = P\left(Z > \frac{130 - 120}{15}\right) = P(Z > 0.67)

      .. math::
         = 1 - \Phi(0.67) = 1 - 0.7486 = 0.2514

      **Part (b): Sample mean with n = 9**

      :math:`\sigma_{\bar{X}} = \frac{15}{\sqrt{9}} = 5` ms, and :math:`\bar{X} \sim N(120, 5^2)`.

      .. math::
         P(\bar{X} > 130) = P\left(Z > \frac{130 - 120}{5}\right) = P(Z > 2.00)

      .. math::
         = 1 - \Phi(2.00) = 1 - 0.9772 = 0.0228

      **Part (c): Sample mean with n = 36**

      :math:`\sigma_{\bar{X}} = \frac{15}{\sqrt{36}} = 2.5` ms, and :math:`\bar{X} \sim N(120, 2.5^2)`.

      .. math::
         P(\bar{X} > 130) = P\left(Z > \frac{130 - 120}{2.5}\right) = P(Z > 4.00)

      .. math::
         = 1 - \Phi(4.00) \approx 1 - 0.99997 \approx 0.00003

      **R verification:**

      .. code-block:: r

         # Part (a): Single observation
         pnorm(130, mean = 120, sd = 15, lower.tail = FALSE)
         # [1] 0.2525

         # Part (b): Sample mean, n = 9
         pnorm(130, mean = 120, sd = 15/sqrt(9), lower.tail = FALSE)
         # [1] 0.0228

         # Part (c): Sample mean, n = 36
         pnorm(130, mean = 120, sd = 15/sqrt(36), lower.tail = FALSE)
         # [1] 3.167e-05

      **Part (d): Explanation**

      The probabilities decrease dramatically as sample size increases because:

      - Individual observations have high variability (:math:`\sigma = 15` ms)
      - Sample means have reduced variability (:math:`\sigma_{\bar{X}} = \sigma/\sqrt{n}`)
      - Larger samples produce means that cluster more tightly around :math:`\mu = 120`

      A single observation exceeding 130 ms is fairly common (25% chance), but a sample mean of 36 observations exceeding 130 ms is extremely rare (0.003% chance) because extreme values in individual observations tend to cancel out when averaging.

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch7-2/fig4_sampling_distributions.png
         :alt: Comparison of distributions for n=1, 9, and 36
         :align: center
         :width: 80%

         As sample size increases, the sampling distribution becomes more concentrated around Î¼ = 120.

----

.. admonition:: Exercise 5: Quality Control Application
   :class: note

   A pharmaceutical company fills capsules with an active ingredient. The filling process is normally distributed with a target mean of :math:`\mu = 500` mg and standard deviation :math:`\sigma = 12` mg. To monitor quality, a random sample of :math:`n = 9` capsules is tested each hour.

   a. What is the sampling distribution of :math:`\bar{X}`?

   b. If the process is operating correctly (at :math:`\mu = 500`), what is the probability that the sample mean falls between 492 mg and 508 mg?

   c. The quality control protocol triggers an investigation if :math:`\bar{X}` falls outside the interval :math:`[492, 508]`. What is the probability of triggering an investigation when the process is operating correctly? (This is called a "false alarm" rate.)

   d. Suppose the process drifts so that :math:`\mu = 506` mg (but :math:`\sigma` remains 12 mg). What is the probability that :math:`\bar{X}` falls within :math:`[492, 508]`? (This represents failing to detect a problem.)

   .. dropdown:: Solution
      :class-container: sd-border-success

      Given: Target :math:`\mu = 500` mg, :math:`\sigma = 12` mg, :math:`n = 9`.

      **Part (a): Sampling distribution**

      .. math::
         \sigma_{\bar{X}} = \frac{12}{\sqrt{9}} = 4 \text{ mg}

      Since the population is normal:

      .. math::
         \bar{X} \sim N(500, 4^2) \quad \text{or} \quad \bar{X} \sim N(500, 16)

      **Part (b): P(492 < XÌ„ < 508) when Î¼ = 500**

      .. math::
         P(492 < \bar{X} < 508) = P\left(\frac{492 - 500}{4} < Z < \frac{508 - 500}{4}\right)

      .. math::
         = P(-2.00 < Z < 2.00) = \Phi(2.00) - \Phi(-2.00) = 0.9772 - 0.0228 = 0.9544

      **Part (c): False alarm rate**

      The probability of triggering an investigation when the process is correct:

      .. math::
         P(\bar{X} < 492 \text{ or } \bar{X} > 508) = 1 - P(492 < \bar{X} < 508) = 1 - 0.9544 = 0.0456

      About **4.56%** of the time, an investigation will be triggered even when the process is operating correctly.

      **Part (d): P(492 < XÌ„ < 508) when Î¼ = 506 (process drift)**

      Now :math:`\bar{X} \sim N(506, 4^2)`:

      .. math::
         P(492 < \bar{X} < 508) = P\left(\frac{492 - 506}{4} < Z < \frac{508 - 506}{4}\right)

      .. math::
         = P(-3.50 < Z < 0.50) = \Phi(0.50) - \Phi(-3.50)

      .. math::
         = 0.6915 - 0.0002 = 0.6913

      There is about a **69%** probability that the sample mean falls in the acceptable range even though the process has drifted. This means the protocol fails to detect the problem about 69% of the timeâ€”a concern for quality control effectiveness.

      **R verification:**

      .. code-block:: r

         se <- 12/sqrt(9)  # Standard error = 4

         # Part (b): P(492 < XÌ„ < 508) when Î¼ = 500
         pnorm(508, mean = 500, sd = se) - pnorm(492, mean = 500, sd = se)
         # [1] 0.9545

         # Part (c): False alarm rate
         1 - (pnorm(508, mean = 500, sd = se) - pnorm(492, mean = 500, sd = se))
         # [1] 0.0455

         # Part (d): P(492 < XÌ„ < 508) when Î¼ = 506
         pnorm(508, mean = 506, sd = se) - pnorm(492, mean = 506, sd = se)
         # [1] 0.6915

----

.. admonition:: Exercise 6: Working Backward â€” Finding Population Parameters
   :class: note

   For a normally distributed population, a sample of size :math:`n = 25` yields a sampling distribution for :math:`\bar{X}` with standard error :math:`\sigma_{\bar{X}} = 3`.

   a. What is the population standard deviation :math:`\sigma`?

   b. If the sample size were increased to :math:`n = 100`, what would be the new standard error?

   c. If the population standard deviation were actually :math:`\sigma = 20`, what sample size would be needed to achieve the original standard error of 3?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Find Ïƒ from SE**

      Given: :math:`\sigma_{\bar{X}} = 3` and :math:`n = 25`.

      .. math::
         \sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}} \implies 3 = \frac{\sigma}{\sqrt{25}} = \frac{\sigma}{5}

      .. math::
         \sigma = 3 \times 5 = 15

      **Part (b): New SE with n = 100**

      .. math::
         \sigma_{\bar{X}} = \frac{15}{\sqrt{100}} = \frac{15}{10} = 1.5

      **Part (c): Sample size for SE = 3 when Ïƒ = 20**

      .. math::
         3 = \frac{20}{\sqrt{n}} \implies \sqrt{n} = \frac{20}{3} \implies n = \left(\frac{20}{3}\right)^2 = \frac{400}{9} = 44.44

      Since sample size must be a whole number, we need :math:`n \geq 45` to achieve a standard error of at most 3.

----

.. admonition:: Exercise 7: Symmetric Probability Bounds
   :class: note

   The weight of packages shipped by an e-commerce company is normally distributed with mean :math:`\mu = 2.5` kg and standard deviation :math:`\sigma = 0.4` kg. For a random sample of :math:`n = 16` packages:

   a. Find the value :math:`d` such that :math:`P(|\bar{X} - \mu| < d) = 0.95`.

   b. Interpret this result in context.

   c. How would :math:`d` change if the sample size were increased to :math:`n = 64`?

   .. dropdown:: Solution
      :class-container: sd-border-success

      Given: :math:`\mu = 2.5` kg, :math:`\sigma = 0.4` kg, :math:`n = 16`.

      First, find the standard error:

      .. math::
         \sigma_{\bar{X}} = \frac{0.4}{\sqrt{16}} = 0.1 \text{ kg}

      **Part (a): Find d such that P(|XÌ„ âˆ’ Î¼| < d) = 0.95**

      We need :math:`P(-d < \bar{X} - \mu < d) = 0.95`.

      Standardizing:

      .. math::
         P\left(-\frac{d}{\sigma_{\bar{X}}} < Z < \frac{d}{\sigma_{\bar{X}}}\right) = 0.95

      For this symmetric interval around 0, we need :math:`\frac{d}{\sigma_{\bar{X}}} = z_{0.975}`.

      From the Z-table: :math:`z_{0.975} = 1.96`.

      .. math::
         d = 1.96 \times \sigma_{\bar{X}} = 1.96 \times 0.1 = 0.196 \text{ kg}

      **R verification:**

      .. code-block:: r

         # Find z_{0.975}
         qnorm(0.975)
         # [1] 1.96

         # Calculate d
         1.96 * 0.1
         # [1] 0.196

      **Part (b): Interpretation**

      There is a 95% probability that the sample mean weight of 16 packages will be within **0.196 kg** (about 196 grams) of the true population mean. In other words, 95% of all possible sample means will fall in the interval :math:`[2.304, 2.696]` kg.

      **Part (c): Effect of increasing n to 64**

      New standard error:

      .. math::
         \sigma_{\bar{X}} = \frac{0.4}{\sqrt{64}} = 0.05 \text{ kg}

      New :math:`d`:

      .. math::
         d = 1.96 \times 0.05 = 0.098 \text{ kg}

      The bound **halves** (from 0.196 to 0.098 kg) when sample size quadruples. Larger samples produce sample means that stay closer to the population mean.

----

.. admonition:: Exercise 8: The iid Framework
   :class: note

   Consider the sampling framework where :math:`X_1, X_2, \ldots, X_n` are independent and identically distributed (iid) random variables, each with mean :math:`\mu` and variance :math:`\sigma^2`.

   a. Explain in your own words what "independent" means in this context.

   b. Explain what "identically distributed" means in this context.

   c. Why is the iid assumption important for deriving the variance formula :math:`\text{Var}(\bar{X}) = \sigma^2/n`?

   d. Give an example of a sampling scenario where the iid assumption might be violated.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Independence**

      "Independent" means that the value of one observation does not affect or provide information about the values of other observations. Mathematically, knowing :math:`X_1 = x_1` does not change the probability distribution of :math:`X_2, X_3, \ldots, X_n`.

      In practice, this typically requires random sampling where each unit is selected without regard to other selected units.

      **Part (b): Identically distributed**

      "Identically distributed" means all observations come from the same probability distributionâ€”they have the same mean :math:`\mu`, same variance :math:`\sigma^2`, and same distributional shape. This ensures we're sampling from a single, well-defined population.

      **Part (c): Importance of iid for variance formula**

      The derivation of :math:`\text{Var}(\bar{X}) = \sigma^2/n` relies on:

      1. **Independence**: This allows us to write :math:`\text{Var}(X_1 + X_2 + \cdots + X_n) = \text{Var}(X_1) + \text{Var}(X_2) + \cdots + \text{Var}(X_n)`. Without independence, we would need to include covariance terms.

      2. **Identical distribution**: This ensures each :math:`\text{Var}(X_i) = \sigma^2`, so the sum of variances equals :math:`n\sigma^2`.

      Together, these give :math:`\text{Var}(\bar{X}) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}`.

      **Part (d): Violation example**

      *Cluster sampling*: If we sample households and then measure all individuals within each household, observations within the same household are likely correlated (not independent)â€”family members may share similar characteristics.

      *Time series data*: Measurements taken over time (e.g., daily stock prices) often exhibit dependence, where today's value is related to yesterday's value.

      *Sampling without replacement from a small population*: If the population is small relative to the sample, observations are not truly independent because removing one unit changes the composition of remaining units.

----

.. admonition:: Exercise 9: Comprehensive Application â€” Engine Performance
   :class: note

   A mechanical engineer is testing fuel efficiency of a new engine design. Based on extensive prior testing, fuel efficiency (in miles per gallon) is known to be normally distributed with mean :math:`\mu = 32` mpg and standard deviation :math:`\sigma = 3` mpg.

   a. For a single test run, what is the probability of observing fuel efficiency above 35 mpg?

   b. The engineer conducts :math:`n = 12` test runs and computes the sample mean. What is the distribution of :math:`\bar{X}`?

   c. Find :math:`P(\bar{X} > 33)` for the sample of 12 runs.

   d. Find the 5th and 95th percentiles of the sampling distribution of :math:`\bar{X}`.

   e. The engineer wants the standard error to be at most 0.5 mpg. How many test runs are needed?

   f. If the engineer observes :math:`\bar{x} = 34.2` mpg from 12 test runs, should this be considered unusual? Calculate the probability of observing a sample mean at least this far from :math:`\mu = 32`.

   .. dropdown:: Solution
      :class-container: sd-border-success

      Given: :math:`X \sim N(\mu = 32, \sigma = 3)` mpg.

      **Part (a): P(X > 35) for single observation**

      .. math::
         P(X > 35) = P\left(Z > \frac{35 - 32}{3}\right) = P(Z > 1.00) = 1 - 0.8413 = 0.1587

      **Part (b): Distribution of XÌ„ with n = 12**

      .. math::
         \sigma_{\bar{X}} = \frac{3}{\sqrt{12}} = \frac{3}{3.464} = 0.866 \text{ mpg}

      .. math::
         \bar{X} \sim N(32, 0.866^2) \quad \text{or} \quad \bar{X} \sim N(32, 0.75)

      **Part (c): P(XÌ„ > 33)**

      .. math::
         P(\bar{X} > 33) = P\left(Z > \frac{33 - 32}{0.866}\right) = P(Z > 1.15)

      .. math::
         = 1 - \Phi(1.15) = 1 - 0.8749 = 0.1251

      **Part (d): 5th and 95th percentiles**

      From Z-table: :math:`z_{0.05} = -1.645` and :math:`z_{0.95} = 1.645`.

      5th percentile:

      .. math::
         \bar{x}_{0.05} = \mu + z_{0.05} \cdot \sigma_{\bar{X}} = 32 + (-1.645)(0.866) = 32 - 1.42 = 30.58 \text{ mpg}

      95th percentile:

      .. math::
         \bar{x}_{0.95} = 32 + (1.645)(0.866) = 32 + 1.42 = 33.42 \text{ mpg}

      90% of sample means will fall between **30.58 and 33.42 mpg**.

      **Part (e): Sample size for SE â‰¤ 0.5 mpg**

      .. math::
         \frac{\sigma}{\sqrt{n}} \leq 0.5 \implies \frac{3}{\sqrt{n}} \leq 0.5 \implies \sqrt{n} \geq 6 \implies n \geq 36

      At least **36 test runs** are needed.

      **Part (f): Is xÌ„ = 34.2 unusual?**

      The observed sample mean is 34.2 âˆ’ 32 = 2.2 mpg away from :math:`\mu`.

      We compute :math:`P(|\bar{X} - 32| \geq 2.2)`:

      .. math::
         P(|\bar{X} - 32| \geq 2.2) = P(\bar{X} \leq 29.8) + P(\bar{X} \geq 34.2)

      .. math::
         = P\left(Z \leq \frac{29.8 - 32}{0.866}\right) + P\left(Z \geq \frac{34.2 - 32}{0.866}\right)

      .. math::
         = P(Z \leq -2.54) + P(Z \geq 2.54) = 2 \times P(Z \leq -2.54)

      .. math::
         = 2 \times 0.0055 = 0.011

      **Yes, this is unusual.** There is only about a **1.1%** probability of observing a sample mean at least 2.2 mpg away from the true mean if :math:`\mu = 32`. This result suggests the engine may actually have different fuel efficiency than assumed, or something unusual occurred during testing.

      **R verification:**

      .. code-block:: r

         mu <- 32
         sigma <- 3
         n <- 12
         se <- sigma / sqrt(n)  # 0.866

         # Part (a): Single observation P(X > 35)
         pnorm(35, mean = mu, sd = sigma, lower.tail = FALSE)
         # [1] 0.1587

         # Part (c): P(XÌ„ > 33)
         pnorm(33, mean = mu, sd = se, lower.tail = FALSE)
         # [1] 0.1241

         # Part (d): 5th and 95th percentiles
         qnorm(0.05, mean = mu, sd = se)
         # [1] 30.58
         qnorm(0.95, mean = mu, sd = se)
         # [1] 33.42

         # Part (f): Two-tailed probability
         2 * pnorm(29.8, mean = mu, sd = se)
         # [1] 0.0110

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. The expected value of the sample mean equals the population mean for any sample size.

   â“‰ or â’»

2. The standard error of :math:`\bar{X}` increases as sample size increases.

   â“‰ or â’»

3. If the population is normally distributed, then :math:`\bar{X}` is exactly normally distributed regardless of sample size.

   â“‰ or â’»

4. Doubling the sample size will cut the standard error in half.

   â“‰ or â’»

5. The variance of the sample mean is :math:`\sigma^2/n`, where :math:`\sigma^2` is the population variance.

   â“‰ or â’»

6. The sampling distribution of :math:`\bar{X}` has the same standard deviation as the population.

   â“‰ or â’»

**Multiple Choice Questions** (2 points each)

7. A population has :math:`\mu = 100` and :math:`\sigma = 20`. For samples of size :math:`n = 25`, the standard error of :math:`\bar{X}` is:

   â’¶ 0.8
   
   â’· 4
   
   â’¸ 20
   
   â’¹ 100

8. If the population is normal with :math:`\mu = 50` and :math:`\sigma = 10`, and :math:`n = 4`, then :math:`\bar{X}` follows:

   â’¶ :math:`N(50, 100)`
   
   â’· :math:`N(50, 25)`
   
   â’¸ :math:`N(50, 10)`
   
   â’¹ :math:`N(12.5, 2.5)`

9. To reduce the standard error by half, you must:

   â’¶ Double the sample size
   
   â’· Triple the sample size
   
   â’¸ Quadruple the sample size
   
   â’¹ Halve the population standard deviation

10. For a normal population with :math:`\sigma = 12`, what sample size gives :math:`\sigma_{\bar{X}} = 2`?

    â’¶ 6
    
    â’· 24
    
    â’¸ 36
    
    â’¹ 144

11. If :math:`X \sim N(80, 16)` (variance = 16), and :math:`n = 4`, then :math:`P(\bar{X} > 82)` equals:

    â’¶ :math:`P(Z > 0.5)`
    
    â’· :math:`P(Z > 1)`
    
    â’¸ :math:`P(Z > 2)`
    
    â’¹ :math:`P(Z > 4)`

12. The formula :math:`\text{Var}(\bar{X}) = \sigma^2/n` requires which assumption?

    â’¶ The population must be normal
    
    â’· The sample size must be at least 30
    
    â’¸ The observations must be independent
    
    â’¹ The population mean must be known

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **True** â€” :math:`E[\bar{X}] = \mu` always holds when sampling from a population with mean :math:`\mu`.

   2. **False** â€” The standard error :math:`\sigma/\sqrt{n}` **decreases** as :math:`n` increases.

   3. **True** â€” Linear combinations of normal random variables are normal, so :math:`\bar{X}` is exactly normal when the population is normal.

   4. **False** â€” Doubling :math:`n` reduces SE by factor of :math:`\sqrt{2} \approx 1.41`, not 2. To halve SE, you must quadruple :math:`n`.

   5. **True** â€” This is the variance formula for the sample mean.

   6. **False** â€” The sampling distribution has standard deviation :math:`\sigma/\sqrt{n}`, which is smaller than :math:`\sigma` when :math:`n > 1`.

   **Multiple Choice Answers:**

   7. **â’·** â€” :math:`\sigma_{\bar{X}} = 20/\sqrt{25} = 20/5 = 4`.

   8. **â’·** â€” :math:`\bar{X} \sim N(\mu, \sigma^2/n) = N(50, 100/4) = N(50, 25)`.

   9. **â’¸** â€” To halve SE, multiply :math:`n` by 4 (since :math:`\sqrt{4} = 2`).

   10. **â’¸** â€” :math:`2 = 12/\sqrt{n} \implies \sqrt{n} = 6 \implies n = 36`.

   11. **â’·** â€” :math:`\sigma = 4`, so :math:`\sigma_{\bar{X}} = 4/\sqrt{4} = 2`. Then :math:`z = (82-80)/2 = 1`.

   12. **â’¸** â€” Independence allows :math:`\text{Var}(\sum X_i) = \sum \text{Var}(X_i)`. Normality is not required for this formula.