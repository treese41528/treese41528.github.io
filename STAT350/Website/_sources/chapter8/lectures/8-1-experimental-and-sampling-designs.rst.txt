.. _8-1-experimental-and-sampling-designs:


.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch8-1">
      <iframe
         id="video-ch8-1"
         title="STAT 350 ‚Äì Chapter 8.1 Experimental and Sampling Designs Video"
         src="https://www.youtube.com/embed/6iP17gg247k?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

Experimental and Sampling Designs
===============================================


We're now standing at a crucial bridge in our statistical journey. We've explored data through descriptive methods, 
built probability models to understand uncertainty, and learned how sample statistics behave through sampling 
distributions and the Central Limit Theorem. These tools have equipped us to understand how information flows 
from populations to samples. But before we can confidently make the reverse journey‚Äîinferring from samples back 
to populations‚Äîwe need one final, essential piece: understanding how to collect data properly in the first place.

Statistical inference is powerful, but it's only as reliable as the data it's based on. The most sophisticated 
mathematical tools cannot rescue conclusions drawn from poorly designed studies or biased samples. 
This chapter focuses on the critical foundation that makes valid statistical inference possible: thoughtful, 
principled approaches to study design and data collection.

.. admonition:: Road Map üß≠
   :class: important

   ‚Ä¢ **Problem**: Not all studies are created equal‚Äîsome can establish causation, others only association, and poor design can invalidate even perfect analysis
   ‚Ä¢ **Tool**: Framework for understanding different study types, their strengths and limitations, and how design choices affect the validity of our conclusions
   ‚Ä¢ **Pipeline**: This sets the foundation for choosing appropriate designs that enable reliable Sample ‚Üí Population inferences


Statistical Questions and the Need for Data
---------------------------------------------

Our journey toward statistical inference begins with recognizing what makes a question statistical in nature. A **statistical question** is one that seeks to quantify or explain relationships among variables and requires data with inherent variability to answer it. This variability isn't a flaw to be eliminated‚Äîit's the fundamental characteristic that makes statistical methods both necessary and powerful.

**The Nature of Statistical Variation**

Statistical variation arises from multiple sources that create uncertainty in our observations:

- **Subject differences**: Individual units in our study naturally vary in their characteristics, responses, and behaviors
- **Measurement errors**: Even the most precise instruments introduce some degree of measurement uncertainty
- **Random chance**: Inherent randomness in natural processes and human behavior

Ideally, we want the primary source of variation in our data to be random chance, with other sources minimized through careful design. We aim to reduce measurement errors through precise instruments and standardized procedures, and we handle subject differences through proper randomization and control strategies.

**Statistical vs. Deterministic Relationships**

Not every question involving data is statistical. If we can predict an outcome with certainty given the inputs‚Äîlike calculating the area of a rectangle from its length and width‚Äîwe're dealing with a deterministic relationship. Statistical questions, by contrast, involve relationships where perfect prediction is impossible due to inherent variability, but where we can still identify meaningful patterns and quantify uncertainty.

The Spectrum of Data Sources
----------------------------

Before conducting any study, researchers face a fundamental decision: where will their data come from? This choice shapes everything that follows‚Äîthe types of conclusions that can be drawn, the statistical methods that are appropriate, and the confidence we can place in our results. Understanding the characteristics, advantages, and limitations of different data sources is essential for making informed research decisions.

**Anecdotal Data: The Starting Point for Scientific Inquiry**

**Anecdotal data** represents the most basic form of information‚Äîobservations from personal experiences, casual reports, or informal accounts shared through news media and social networks. While lacking scientific rigor, anecdotal evidence plays a surprisingly important role in the research ecosystem.

*Historical Context and Examples*

Throughout history, many significant scientific discoveries began with anecdotal observations. Alexander Fleming's discovery of penicillin started with his casual observation that bacterial cultures were being killed by contaminating mold. The connection between smoking and lung cancer was first suggested by doctors' anecdotal observations of their patients before formal epidemiological studies confirmed the relationship.

In medicine, patients often report symptoms or experiences that don't fit established patterns, leading physicians to investigate new conditions or treatment effects. In environmental science, community members frequently notice changes in their local ecosystem before researchers begin systematic monitoring. In technology, user complaints about software bugs or unexpected device behavior guide developers toward systematic testing.

*The Psychology of Anecdotal Evidence*

Anecdotal data captures our attention precisely because it's memorable and emotionally engaging. We naturally remember vivid stories more easily than statistical summaries‚Äîa phenomenon psychologists call the "availability heuristic." A single dramatic case often feels more compelling than abstract statistical evidence, even when the statistics are based on thousands of observations.

This psychological appeal makes anecdotal evidence particularly powerful for generating hypotheses and motivating research. A striking case study can inspire researchers to investigate whether the observed pattern holds more generally. However, this same appeal makes anecdotal evidence dangerous when used to support general conclusions.

*Limitations and Biases*

The fundamental problem with anecdotal data is that unusual, dramatic, or memorable cases are much more likely to be reported, remembered, and shared than typical cases. This creates a distorted view where rare events seem common and exceptional outcomes appear normal.

Consider media coverage of air travel. News reports focus intensively on airplane accidents, creating the impression that flying is dangerous, even though statistical analysis shows commercial aviation to be remarkably safe. The anecdotal evidence (dramatic crash reports) contradicts the systematic evidence (comprehensive safety statistics), and relying on anecdotes alone would lead to incorrect conclusions about aviation risk.

Similarly, testimonials about alternative medical treatments often highlight dramatic recoveries while ignoring cases where the treatment failed or patients experienced no benefit. Without systematic data collection that captures all outcomes‚Äînot just the memorable ones‚Äîwe cannot distinguish between genuine effects and natural variation.

*Appropriate Uses*

Despite these limitations, anecdotal data serves legitimate scientific purposes:

- **Hypothesis generation**: Identifying phenomena that deserve systematic investigation
- **Case studies**: Documenting rare conditions or unusual responses that wouldn't appear in typical samples
- **Quality control**: Identifying problems with established procedures or treatments
- **Preliminary insights**: Suggesting relationships worth exploring through formal research

The key is recognizing anecdotal data as a starting point for inquiry rather than evidence for conclusions.

**Available Data: Navigating the Information Ocean**

The modern research landscape is dominated by an unprecedented availability of existing data. **Available data** includes any information that has already been collected and can be accessed for research purposes, ranging from government statistics and published study datasets to corporate databases and social media archives.

*The Big Data Revolution*

We live in an era of information abundance that would have been unimaginable to earlier generations of researchers. Every digital interaction‚Äîweb searches, social media posts, online purchases, mobile phone locations, sensor readings‚Äîgenerates data that can potentially inform research questions.

The scale is genuinely staggering. By 2025, approximately 463 exabytes of data are generated globally each day. To put this in perspective, one exabyte equals one billion gigabytes‚Äîenough to store about 250 million high-definition movies. This daily data generation includes:

- **Social media content**: Billions of posts, comments, likes, and shares across platforms
- **Transaction records**: Financial purchases, online orders, subscription renewals
- **Sensor data**: GPS locations, weather measurements, traffic patterns, energy usage
- **Scientific measurements**: Satellite imagery, genomic sequences, particle physics experiments
- **Administrative records**: Government databases, health records, educational data
- **Media content**: News articles, videos, podcasts, digital publications

*Traditional Sources of Available Data*

Beyond the recent explosion in digital data, researchers have long relied on established sources of available information:

**Government Statistics**: Census data, economic indicators, health surveillance systems, crime statistics, and regulatory filings provide comprehensive, standardized information collected systematically over time. These datasets often represent entire populations rather than samples, offering unprecedented scope and reliability.

**Academic Research Archives**: Many published studies make their datasets publicly available, allowing other researchers to verify results, explore new questions, or combine data across studies. Repositories like the Inter-university Consortium for Political and Social Research (ICPSR) and the National Center for Health Statistics provide access to high-quality research data.

**Scientific Databases**: Specialized fields maintain comprehensive databases of observations‚Äîastronomical catalogs, genetic sequences, chemical properties, climate measurements‚Äîthat enable research questions impossible to address through individual studies.

**Historical Records**: Archives, libraries, and museums preserve information from past eras, enabling longitudinal studies that track changes over decades or centuries.

*Leveraging Available Data for Research*

The key to successfully using available data lies in understanding how to extract meaningful insights while working within the constraints of data you didn't collect yourself. This requires both technical skills and creative thinking about how existing information can address new questions.

**Secondary Analysis Approaches**: Researchers often use available data to answer questions that weren't part of the original study design. For example, a dataset originally collected to study employment patterns might be re-analyzed to examine gender wage gaps, geographic mobility, or the effects of economic recessions. The challenge is ensuring that the available variables adequately capture the concepts you want to study.

**Data Integration and Linking**: Powerful insights can emerge from combining multiple available datasets. Researchers might link census data with health records to study environmental health effects, or combine economic indicators with educational data to examine socioeconomic influences on academic achievement. This approach can reveal relationships that wouldn't be visible in any single dataset.

**Longitudinal Analysis**: Some available datasets track the same subjects over many years, enabling researchers to study how variables change over time in ways that would be impossible with new data collection. Panel studies, repeated surveys, and administrative records can reveal developmental patterns, long-term effects, and causal relationships that emerge only over extended periods.

**Meta-Analysis and Systematic Reviews**: When multiple studies have collected similar data, researchers can combine results across studies to detect patterns that might not be apparent in individual datasets. This approach is particularly valuable for identifying small effects that require large sample sizes to detect reliably.

**Exploratory Data Mining**: Large available datasets can be explored to identify unexpected patterns or relationships that suggest new hypotheses for testing. While this approach requires careful validation to avoid spurious findings, it can reveal insights that wouldn't emerge from theory-driven research alone.

*Critical Challenges and Limitations*

However, using available data requires navigating significant challenges:

**Data Quality Assessment**: The most critical challenge is evaluating whether the available data is suitable for your research question. Key considerations include:

- **Collection methodology**: How were the original data gathered? Were appropriate sampling methods used? What was the response rate? Were there systematic patterns in who participated versus who declined?

- **Data processing history**: What cleaning, filtering, or transformation procedures were applied? Are missing values handled appropriately? Have variables been recoded or aggregated in ways that might affect your analysis?

- **Measurement validity**: Do the available variables actually measure what you want to study? Are there important differences between how concepts were operationalized in the original study versus your research question?

- **Population relevance**: Does the available data come from the population you want to study? If the data comes from a specific geographic region, time period, or demographic group, how well do those characteristics match your target population?

**Documentation and Metadata**: High-quality available data includes comprehensive documentation about collection procedures, variable definitions, known limitations, and processing history. Unfortunately, much available data‚Äîparticularly from commercial or informal sources‚Äîlacks adequate documentation, making quality assessment difficult or impossible.

**Ethical and Legal Considerations**: Data that was collected for one purpose may raise ethical concerns when used for different purposes, particularly if the original participants didn't consent to such uses. Additionally, data sharing agreements, privacy regulations, and institutional review board requirements may limit access to potentially valuable datasets.

**Bias and Representativeness**: Available data often suffers from systematic patterns that limit its representativeness:

- **Self-selection effects**: Social media data, online surveys, and voluntary reporting systems overrepresent people who choose to participate
- **Digital divide effects**: Internet-based data sources may underrepresent older adults, lower-income populations, and communities with limited technology access
- **Survivorship effects**: Historical records may overrepresent successful individuals, organizations, or outcomes while underrepresenting failures
- **Reporting patterns**: Administrative data may reflect institutional priorities or incentives rather than true population patterns

*Best Practices for Using Available Data*

Successful use of available data requires systematic evaluation and transparent reporting:

1. **Thorough documentation review**: Understand exactly how the data were collected, processed, and cleaned before beginning analysis
2. **Exploratory data analysis**: Examine the data carefully for unexpected patterns, outliers, or inconsistencies that might indicate quality problems
3. **Bias assessment**: Consider what populations or outcomes might be systematically excluded from the available data
4. **Sensitivity analysis**: Test whether your conclusions change when you make different assumptions about data quality or missing information
5. **Transparent reporting**: Clearly describe the source and limitations of your data so readers can appropriately interpret your results

*The Future of Available Data*

As data collection becomes increasingly automated and comprehensive, available data will likely become even more central to research across disciplines. However, this trend also raises important questions about privacy, consent, and the concentration of valuable data resources in the hands of large technology companies and government agencies.

Researchers must balance the tremendous opportunities offered by available data against the responsibility to use such data ethically and to acknowledge its limitations honestly. The goal is not to avoid available data‚Äîwhich would mean ignoring valuable resources‚Äîbut to use it thoughtfully and appropriately.

Observational Studies: Learning from Natural Variation
--------------------------------------------------------

When available data is insufficient or inappropriate for answering our research question, we must collect new data. **Observational studies** represent one major approach to data collection, characterized by researchers observing and recording behavior, characteristics, or outcomes without making any active interventions.

**The Observational Approach**

In an observational study, researchers act as careful observers rather than active manipulators. They identify subjects of interest, contact them, and collect measurements, but they do not impose treatments or attempt to influence the study environment. This approach is particularly valuable when interventions would be unethical, impractical, or impossible, or when the research goal is to understand naturally occurring phenomena.

The observational approach follows a systematic process:

1. **Define the research question** with clear specification of the relationships to be studied
2. **Identify the target population** and important subgroups that may exhibit different behaviors
3. **Specify variables of interest**, including both the primary variables of focus and potential confounding variables that might influence the results
4. **Design and implement random sampling procedures** to obtain a representative sample from the target population
5. **Observe and measure** the variables of interest without intervention
6. **Apply statistical inference methods** to draw conclusions about the broader population

**Strengths and Limitations of Observational Studies**

Observational studies excel at documenting naturally occurring relationships and patterns. They allow researchers to study phenomena in realistic settings where all the complex factors that influence outcomes in the real world remain present. This ecological validity makes observational studies particularly valuable for understanding how variables relate in natural environments.

However, observational studies face a fundamental limitation: they can establish that **associations** exist between variables, but they cannot definitively establish **causal relationships**. The inability to control which subjects receive which conditions means that observed associations might be due to the variables of primary interest, or they might result from other unmeasured factors that influence both the apparent cause and effect.

This limitation doesn't make observational studies less valuable‚Äîit simply means their conclusions must be interpreted appropriately. Strong associations documented consistently across multiple well-designed observational studies can provide compelling evidence for relationships, even without definitive proof of causation.

**A Whimsical Tale: The Curious Case of Falling Cats**

Consider one of the most charming examples of observational research in the scientific literature: the study of "feline high-rise syndrome" by Whitney and Mehlhaff, published in the Journal of the American Veterinary Medical Association in 1987. This research emerged from veterinarians' observations of an intriguing pattern among their patients.

The researchers wondered: when cats fall from buildings, how does the height of the fall relate to the severity of their injuries? To investigate this question, they conducted a systematic observational study by examining records from the Animal Medical Center in New York City.

Between June and November 1984, they identified 132 cats that had been brought to the veterinary hospital after falling from multi-story buildings. For each case, they carefully documented the cat's injuries, the height from which it fell (measured in stories, with each story approximately 12 feet), and the outcome of treatment.

Their findings were surprising. About 90% of the cats survived their falls with appropriate veterinary care, which was reassuring news for cat owners. But more intriguingly, they observed that cats falling from seven stories or higher didn't sustain significantly more injuries than those falling from lower heights. In fact, cats falling from very high stories (nine floors or more) showed remarkably few limb fractures compared to those falling from intermediate heights.

The researchers proposed what they called the "terminal velocity hypothesis" to explain this pattern. They theorized that cats reach their maximum falling speed (around 60 mph) after about five stories. Once they achieve this terminal velocity and realize they're in for a long fall, cats may relax into a spread-eagle "flying squirrel" posture that distributes impact forces more evenly across their body, reducing the likelihood of concentrated injuries like broken limbs.

This explanation is certainly plausible and fits with what we know about feline physiology and behavior. Cats are remarkably well-designed for falling, with flexible spines, a strong righting reflex, and relatively low body weight for their size. The idea that they might naturally adopt a more protective posture during longer falls makes intuitive sense.

However, as Whitney and Mehlhaff acknowledged, this was an observational study, and therefore they could only document **associations** between fall height and injury patterns‚Äîthey couldn't establish **causation**. The terminal velocity hypothesis remains exactly that: a hypothesis consistent with their observations, but not definitively proven by them.

**Why This Had to Be Observational**

This study illustrates perfectly why observational research is sometimes the only ethical option. Testing the terminal velocity hypothesis experimentally would require deliberately dropping cats from various heights‚Äîan approach that would be both unethical and illegal. Even if researchers could design some sort of controlled falling scenario with safety nets or other protections, such an artificial setup would fundamentally change the phenomenon being studied.

Instead, the researchers had to rely on the unfortunate but naturally occurring "experiments" that resulted from cats' own decisions to venture onto high ledges, windowsills, and fire escapes. By carefully documenting these cases, they could identify patterns that might otherwise go unnoticed, even though they couldn't definitively prove what caused those patterns.

**The Value and Limitations Illustrated**

The cat study beautifully demonstrates both the value and limitations of observational research. On the positive side, it:

- Documented an important and previously unrecognized pattern in veterinary medicine
- Provided practical guidance for veterinarians treating high-rise syndrome cases
- Generated testable hypotheses about feline biomechanics and behavior
- Delivered reassuring news to cat owners about survival rates

But it also shows the inherent limitations:

- The terminal velocity hypothesis, while plausible, remains unproven
- Alternative explanations for the height-injury relationship cannot be ruled out
- The study relied on cases that came to veterinary attention, potentially missing cats that died at the scene or were treated elsewhere
- Various confounding factors (cat age, health, landing surface, building design) weren't controlled for

This example helps us understand why observational studies are both valuable and limited. They can reveal important patterns and generate insights that wouldn't be obtainable any other way, but they require careful interpretation and often motivate additional research using different methods.

Experimental Studies: The Gold Standard for Causation
---------------------------------------------------------

When the research question involves understanding causal relationships and when ethical and practical constraints allow, **experimental studies** represent the most powerful approach for statistical inference. Unlike observational studies, experiments involve the deliberate manipulation of one or more variables to observe their effects on other variables.

**The Experimental Approach**

An **experimental study** involves actively imposing treatments or interventions on study subjects and then observing the outcomes. This active manipulation is what distinguishes experiments from observational studies and what makes experiments uniquely capable of establishing causal relationships.

The key insight behind experimental design is control. By systematically manipulating the variables we're interested in while holding other factors constant (or randomizing their effects), we can isolate the specific influence of our treatments. If we observe differences in outcomes between treatment groups, and if our experiment is properly designed, we can confidently conclude that those differences were caused by the treatments we imposed.

**Why Experiments Enable Causal Inference**

The power of experiments to establish causation comes from their ability to satisfy three key criteria for causal relationships:

1. **Temporal precedence**: The cause must occur before the effect. In experiments, we impose treatments before measuring outcomes, establishing clear temporal ordering.

2. **Covariation**: The cause and effect must be associated. Experiments let us observe whether different treatments produce different outcomes.

3. **Elimination of alternative explanations**: Other factors that might explain the relationship must be ruled out. Proper experimental design achieves this through randomization and control.

By satisfying all three criteria simultaneously, well-designed experiments provide the strongest evidence for causal relationships that science can offer.

**The Machine Analogy**

Think of an experiment like controlling the settings on a machine and observing what it produces. If you have complete control over the inputs, can observe all the outputs, and know that nothing else is influencing the system, then you can establish definitive input-output relationships. Experiments attempt to create this level of control in research settings, isolating specific factors to understand their causal effects.

Connecting Study Design to Statistical Inference
---------------------------------------------------

The choice between observational and experimental approaches has profound implications for statistical inference. Our probability models, sampling distributions, and inferential procedures all depend on assumptions about how the data were collected.

**The Population ‚Üí Sample ‚Üí Population Cycle**

Statistical inference relies on a cycle of reasoning:

1. We start with a target population we want to understand
2. We sample from that population using appropriate methods
3. We analyze the sample data using statistical procedures
4. We use the results to draw conclusions about the original population

Each step in this cycle must be carefully designed. Chapter 7 taught us about step 3‚Äîhow sample statistics behave and how we can use probability models to understand their properties. This chapter focuses on steps 1 and 2‚Äîhow we define populations and sample from them in ways that make step 4 (inference back to the population) valid and reliable.

**Design Determines Analysis**

The type of study we conduct determines which statistical methods are appropriate for analysis. Random sampling enables us to use probability-based inference methods. Experimental manipulation allows us to make causal claims. Observational studies limit us to associational conclusions but may be the only ethical or practical option for certain questions.

Poor design choices can invalidate even the most sophisticated statistical analysis. If our sample is biased, our probability models don't apply. If important confounding variables are uncontrolled, our conclusions may be wrong regardless of statistical significance. If our treatments aren't properly randomized, we can't establish causation.

**The Foundation for Valid Inference**

Understanding study design is essential because it determines the scope and validity of our statistical conclusions. The mathematical tools we'll learn in subsequent chapters‚Äîconfidence intervals, hypothesis tests, regression analysis‚Äîare powerful, but they depend entirely on having data collected through appropriate methods.

A perfectly executed statistical analysis of poorly collected data will produce precisely wrong conclusions. Conversely, even simple statistical methods applied to well-designed studies can yield profound insights and reliable knowledge.

The Bridge to Inference Methods
----------------------------------

This chapter serves as the essential bridge between probability theory and statistical inference. We've learned how to model uncertainty and understand how statistics behave, but those tools only work when applied to properly collected data.

In the chapters that follow, we'll explore the specific methods for drawing conclusions from sample data: constructing confidence intervals that capture population parameters with known reliability, performing hypothesis tests that control error rates at predetermined levels, and using regression analysis to model relationships between variables.

But all of these methods depend on the foundation we're building here: understanding how to design studies that collect data in ways that support valid statistical inference. The most elegant mathematical procedures are useless if applied to data that doesn't meet their assumptions.

Bringing It All Together
----------------------------------

.. admonition:: Key Takeaways üìù
   :class: important

   1. **Statistical questions** require data with inherent variability and seek to quantify relationships among variables, distinguishing them from deterministic questions.
   
   2. **Data sources vary in quality and appropriateness**: anecdotal data provides inspiration but not evidence; available data offers efficiency but requires careful quality assessment; new data collection provides control but demands resources.
   
   3. **Observational studies** can establish associations between variables but cannot definitively prove causal relationships, making them valuable for studying naturally occurring phenomena where intervention is impossible or unethical.
   
   4. **Experimental studies** can establish causal relationships by actively manipulating variables while controlling other factors, making them the gold standard when ethical and practical constraints allow.
   
   5. **Study design determines the scope of valid conclusions**: the methods used to collect data determine which statistical analyses are appropriate and what kinds of inferences can be drawn.
   
   6. **Proper design is the foundation of valid inference**: sophisticated statistical methods cannot compensate for fundamentally flawed data collection procedures.
   
   7. **The choice between observational and experimental approaches** depends on research goals, ethical considerations, practical constraints, and the types of conclusions desired.

The transition from probability theory to statistical inference requires more than mathematical sophistication‚Äîit demands careful attention to how data are collected and what assumptions are met. By understanding the strengths and limitations of different study designs, we prepare ourselves to collect data that can reliably inform conclusions about the populations and processes we seek to understand.

As we continue through this chapter, we'll develop the specific skills needed to design studies that produce trustworthy data, recognize common pitfalls that can invalidate research, and connect design choices to the statistical methods that will be most appropriate for analysis. This foundation will prove essential as we move into the methods of statistical inference in subsequent chapters.

In the remaining sections of this chapter, we'll explore:

- The fundamental principles that make experiments reliable (control, randomization, replication)
- Specific experimental design frameworks for different research scenarios
- Common design problems and how to address them
- Practical examples of design choices in action
- Sampling methods for obtaining representative data from populations
- Sources of bias that can invalidate even well-intentioned studies

Each topic builds toward the same goal: enabling you to design studies that produce data worthy of statistical analysis and conclusions worthy of scientific confidence.

Exercises
~~~~~~~~~~~~~~

1. **Study Classification**: For each scenario below, identify whether the research question is statistical or deterministic, and explain your reasoning:
   
   a) Does the amount of fertilizer applied to tomato plants affect their yield?
   b) What is the area of a circular garden with radius 5 feet?
   c) Are taller basketball players more likely to be successful free-throw shooters?
   d) How much interest will $1000 earn in one year at 3% annual interest?

2. **Data Source Evaluation**: A researcher wants to study the relationship between social media usage and academic performance among college students. Evaluate the advantages and disadvantages of using each data source:
   
   a) Available data from a social media platform showing time spent by users
   b) Anecdotal reports from professors about student behavior
   c) A new observational study surveying students about their habits and grades
   d) An experimental study where researchers control students' social media access

3. **Observational vs. Experimental**: Explain why each research question would likely require an observational rather than experimental approach:
   
   a) Do people who smoke have higher rates of lung cancer?
   b) Are children from single-parent households more likely to experience academic difficulties?
   c) Does exposure to air pollution affect respiratory health?
   d) Do people who exercise regularly live longer?

4. **The Falling Cats Study**: Based on the Whitney and Mehlhaff study described in this chapter:
   
   a) Identify the population, sample, and variables of interest
   b) Explain why this had to be an observational study
   c) What alternative explanations might account for the observed height-injury relationship besides the terminal velocity hypothesis?
   d) How might the researchers have improved the study design within ethical constraints?

5. **Statistical vs. Deterministic**: Create your own examples of one statistical question and one deterministic question related to each topic:
   
   a) Student performance in college
   b) Effectiveness of a new medication
   c) Fuel efficiency of automobiles
   d) Weather patterns

6. **Design Implications**: For a research question of your choice, describe how the conclusions you could draw would differ if you used:
   
   a) Anecdotal data only
   b) An observational study
   c) An experimental study
   
   Explain what each approach could and could not tell you about causation versus association.