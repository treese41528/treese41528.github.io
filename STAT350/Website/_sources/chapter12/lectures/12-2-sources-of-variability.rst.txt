.. _12-2-sources-of-variability:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch12-2">
      <iframe
         id="video-ch12-2"
         title="STAT 350 ‚Äì Chapter 12.2 One-Way ANOVA Model and the Sources of Variability Video"
         src="https://www.youtube.com/embed/BKEQadpmPzw?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip

   `Download Chapter 12 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/
   Chapter%2012%20ANOVA/OneWayANOVA_AC.pptx>`_
   
Different Sources of Variability in ANOVA
===============================================================================

In our previous exploration using side-by-side boxplots, we learned that comparing means in 
isolation was not sufficient‚Äîwe had to consider how much variability existed within each group
in comparison with the spread of the group means. In this lesson, we formalize this idea mathematically.


.. admonition:: Road Map üß≠
   :class: important

   * Identify the **three sources of variability** in ANOVA. Quantify the variability from each source using
     formal notation.
   * Recognize the roles of **sum of squares, degrees of freedom and mean of squares** in constructing a
     sample variance.
   * Learn the **special relationship** between the three sums of squares and their degrees of freedom.
   * Organize the components into an **ANOVA table**.

The One-Way ANOVA Model
------------------------------

What is a Statistical Model?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As we progress to statistical inference methods of higher complexity, it becomes essential to
define a corresponding *statistical model* to concisely express the core ideas. A statistical model 
provides a structural decomposition of the data that must hold under the assumptions of the analysis method.

The One-Way ANOVA Model 
~~~~~~~~~~~~~~~~~~~~~~~~~~~

One-way ANOVA assumes that an observation :math:`X_{ij}` takes the following form:

.. math::

   X_{ij} = \mu_i + \varepsilon_{ij}.

Anove, :math:`\mu_{i}` is the unknown true mean of Group :math:`i`, and :math:`\varepsilon_{ij}`
is the random error that captures everything not explained by the group mean. 

According to the ANOVA assumptions, the random errors are mutually independent and have
an equal variance of :math:`\sigma^2`. Since we have extracted the group means out of the random term,
:math:`\varepsilon_{ij}` would also have an expected value of zero. 

In the ideal case where all populations are normally distributed, therefore, we can write:

.. math::

   \varepsilon_{ij} \sim_{iid} N(0, \sigma^2)

Why Is the Model Helpful?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ANOVA model allows us to view each data point as the outcome of two components,
each contributing distinctly. 
If all population means are truly equal, the only source of randomness
among observations would be the :math:`\varepsilon_{ij}` terms, whose 
variance is :math:`\sigma^2`.
If the observed variance in the data is significantly larger than :math:`\sigma^2`, therefore, we
must consider the possibility that differences in group means also contribute. 

To formalize this, we first construct the three key measures of variation in ANOVA: variation between groups,
variation within groups, and the total variation.

Three Types of Variability
--------------------------------

As with any sample variance we have seen before, the three measures of variation will take the following common form:

.. math::

   S^2 = \frac{\text{Sum of Squares}}{\text{df}},

where the degrees of freedom are chosen to make each statistic an unbiased estimator of its target.
The degrees of freedom will show the pattern we have previously seen‚Äîit is equal to the difference between
the number of data points and the number of estimated means used for construction of the statistic.

Since each sample variance of this form is an "average" of squares, 
we also call them a **mean of squares (MS)**.

1. SSA and MSA: Between-Group Variation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We first consider the sum of sqaures for between-group variation, or SSA.

.. math::

   \text{SSA} = \sum_{i=1}^k n_i (\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})^2

Note that SSA is the sum of squared deviations of group means from the overall mean, each weighted
proportionally to the group size. The degrees of freedom appropriate for SSA is :math:`df_A = k-1` since 
there are :math:`k` group means deviating from a single overall mean.

It follows that the mean of squares for the variation between groups is:

.. math::

   \text{MSA} = \frac{\text{SSA}}{df_A} = \frac{\sum_{i=1}^k n_i (\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})^2}{k-1}.

MSA has a special property:

* If :math:`H_0` is true and the equal variance assumption holds, then the MSA is an unbiased estimator of :math:`\sigma^2`. 
* However, if :math:`H_0` is false, then the MSA estimates :math:`\sigma^2` plus additional 
  variation due to differences in population means.

Therefore, an MSA significantly larger than an estimate of :math:`\sigma^2` indicates the existence of 
additional variance due to distinct group means, whereas an MSA comparable to an estimated :math:`\sigma^2`
indicates an absence of strong evidence against the null hypothesis.

.. admonition:: Why Do We call It SS"A"?
   :class: important

   The name SSA stands for "Sum of Squares for Factor A". It originates from the multi-way ANOVA context involving
   multiple factors (Factor A, Factor B, etc.).
   By convention, the sum of squares for the "first" factor is labeled SSA, even when there is only one factor
   in the anlaysis.

.. admonition:: Example üí°: Coffeehouse SSA & MSA ‚òïÔ∏è
   :class: note 

   In the coffeehouse example, SSA and MSA measure how much the store-wise average ages vary from the overall mean.
   If all the coffeehouses attract similar demographics, SSA and MSA should be small. If they attract 
   different age groups, they should be large.

2. SSE and MSE: Within-Group Variation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The sum of squares of errors, or SSE, is an unscaled measure of how observations within each group deviate from their 
respective group means due to the random error, :math:`\varepsilon_{ij}`.

.. math::

   \text{SSE} = \sum_{i=1}^k \sum_{j=1}^{n_i} (X_{ij} - \bar{X}_{i \cdot})^2 = \sum_{i=1}^k (n_i - 1) S^2_{i\cdot}

Confirm the second equality by replacing :math:`S^2_{i\cdot}` with its explicit formula.
The SSE consists of the squared distances of :math:`n` observations from one of :math:`k` group means,
giving us the degrees of freedom :math:`df_E=n-k`.

Bringing SSE to the correct scale with the degrees of freedom, we obtain:

.. math::

   \text{MSE} = \frac{\text{SSE}}{df_E} = \frac{\sum_{i=1}^k \sum_{j=1}^{n_i} (X_{ij} - \bar{X}_{i \cdot})^2}{n-k}.

**MSE is an unbiased estimator of the variance within populations,** :math:`\sigma^2`, 
**regardless of whether** :math:`H_0` **is true.**
As a result, we always estimate the within-population variance with :math:`S^2 = MSE`. The estimator for
population-wise standard deviation is :math:`S= \sqrt{MSE}`.

.. admonition:: Connecting ANOVA and Indepdent Two-Sample Analysis
   :class: important

   MSE is a multi-way extension of the pooled variance estimator for independent two-sample analysis. Confirm this
   by plugging in :math:`k=2` to recover :math:`S^2_p`.

.. admonition:: Example üí°: Coffeehouse SSE & MSE ‚òïÔ∏è
   :class: note 

   In the coffeehouse example, the SSE and MSE measure how much individual customer ages vary within each
   coffeehouse. They represent the natural variation in customer ages that exists regardless of
   any systematic differences between coffeehouses.

3. Total Sum of Squares (SST)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Finally, we also define a measure for the **overall** variability in the data.

.. math::

   \text{SST} = \sum_{i=1}^k \sum_{j=1}^{n_i} (X_{ij} - \bar{X}_{\cdot \cdot})^2

Note that this would be the numerator for the sample variance if the entire dataset was treated as a
single sample. The distances of :math:`n` total observations are measured against one overall mean estimator, 
giving us the degrees of freedom :math:`df_T = n - 1`.

We do not define a mean of sqaures for the total variation, as it does not hold significance in the ANOVA framework.

.. admonition:: Example üí°: Coffeehouse SST ‚òïÔ∏è
   :class: note 

   In the coffeehouse example, SST measures the degree of variation of all customer ages around the single 
   overall sample mean.

The Fundamental ANOVA Identity
--------------------------------

The remarkable mathematical result that makes ANOVA possible is that the sums of squares are related by:

.. math::

   \text{SST} = \text{SSA} + \text{SSE}

Moreover, the degrees of freedom decompose in the same way:

.. math::

   (n - 1) = (k - 1) + (n - k)

Why This Decomposition Works
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We use the trick of adding and subtracting the same terms inside each pair of parentheses in SST:

.. math::

   \text{SST} &= \sum_{i=1}^k \sum_{j=1}^{n_i} (X_{ij} - \bar{X}_{\cdot \cdot})^2 \\
   &= \sum_{i=1}^k \sum_{j=1}^{n_i} [(X_{ij} - \bar{X}_{i \cdot}) + (\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})]^2\\
   &= \sum_{i=1}^k \sum_{j=1}^{n_i} [(X_{ij} - \bar{X}_{i \cdot})^2  + (\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})^2 
      + 2(X_{ij} - \bar{X}_{i \cdot})(\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})] \\
   &= SSE + SSA + \sum_{i=1}^k \sum_{j=1}^{n_i}2(X_{ij} - \bar{X}_{i \cdot})(\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})

The cross-product term can be shown to equal zero by taking the following steps:

* Since :math:`2(\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})` does not depend on
  :math:`j`, we can factor it out of the inner sum.
* The inner sum is then
  
  .. math:: 
     \sum_{j=1}^{n_i} (X_{ij} - \bar{X}_{i \cdot}) = \sum_{j=1}^{n_i}X_{ij} - n_{i}\cdot\frac{\sum_{j=1}^{n_i}X_{ij}}{n_i} = 0.


The ANOVA Table
------------------

The components of ANOVA are often organized into a table, with rows representing the 
three difference sources of variability, and the columns representing various characteristics of 
each source.

.. flat-table::
   :header-rows: 1
   :width: 95%
   :widths: 15 10 25 10 10 10

   * - Source
     - df
     - SS
     - MS
     - F
     - :math:`p`-value

   * - Factor A
     - :math:`k-1`
     - :math:`\sum_{i=1}^k n_i(\bar{x}_{i \cdot} - \bar{x}_{\cdot \cdot})^2`
     - :math:`\frac{\text{SSA}}{k-1}`
     - ?
     - ?

   * - Error
     - :math:`n-k`
     - :math:`\sum_{i=1}^k \sum_{j=1}^{n_i}(x_{ij} - \bar{x}_{i \cdot})^2`
     - :math:`\frac{\text{SSE}}{n-k}`
     - 
     - 

   * - Total
     - :math:`n-1`
     - :math:`\sum_{i=1}^k \sum_{j=1}^{n_i}(x_{ij} - \bar{x}_{\cdot \cdot})^2`
     - 
     - 
     - 

The total row is often omitted for conciseness as their entries can be computed by adding up
the other dfs and the sums of squares.
The entries corresponding to :math:`F` and :math:`p`-value will be discussed in the upcoming lesson.


.. admonition:: Example üí°: Coffeehouse ANOVA Table ‚òïÔ∏è
   :class: note

   Using the the data summary (:numref:`coffeehouse-summary1`) and
   the partial ANOVA table (:numref:`partial-table`) of the coffeehouse example,

   * Fill in the blank entries of the ANOVA table.
   * Provide an estimate of the population standard deviation, :math:`\sigma`.

   **Data Summary** 

   .. _coffeehouse-summary1:

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter12/coffeehouse-summary.png
      :width: 90%
      :align: center

      Coffeehouse example summary

   
   **Partially Complete ANOVA Table** 

   .. _partial-table:

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter12/coffeehouse-partial-table.png
      :width: 80%
      :align: center

      Partial ANOVA table for the cofeehouse example

   
   Let us begin with the degrees of freedom since they can be obtained directly from the data summary.
   We have :math:`n=200` and :math:`k=5`. Therefore,

   * **(1)** :math:`df_A = k - 1 = 4`
   * **(2)** :math:`df_E = n - k = 200 - 5 = 195`
   * **(3)** :math:`df_T = n - 1 = 199`

   We can use :math:`df_T = df_A + df_E` as a second check.
   
   **(5)** :math:`MSE = 99.8 = \frac{SSE}{195}`, so
     
     .. math::
        SSE = 99.8*195 = 19451.

   **(4)** Then, using the fact that SSA and SSE add up to SST, 

     .. math:: 
        SSA = SST - SSE = 28285 - 19451 = 8834

   **(6)** Finally, 

   .. math::
      MSA = \frac{SSA}{df_A} = \frac{8834}{4} = 2208.5

   The MSE as a random variable is an unbiased estimator of :math:`\sigma^2`. We can use the square root 
   of its observed value as an estimate for :math:`\sigma`.

   .. math:: 
      \hat{\sigma} = \sqrt{MSE} = \sqrt{99.8} = 9.99

Bringing It All Together
---------------------------

.. admonition:: Key Takeaways üìù
   :class: important

   1. The **ANOVA model** decomposes each observation 
      into a group effect plus random error: :math:`X_{ij} = \mu_i + \varepsilon_{ij}`.

   2. Total Sum of Squares (SST) satisfies :math:`SST = SSA + SSE`. 
      Their degrees of freedom have a similar association: :math:`df_T = df_A + df_E`.

   3. Since :math:`E(MSE) = \sigma^2` always holds, we use the **MSE as the estimator of the
      common variance** :math:`\sigma^2`, and also denote its observed value as :math:`s^2`. 

   4. MSA is an unbiased astimator of :math:`\sigma^2` **only when** :math:`H_0` **is true**
      ‚Äîits true target is greater than :math:`\sigma^2` when :math:`H_0` is false. Therefore,
      comparing MSA against MSE gives us a measure of data evidence against the null hypothesis.

Exercises
~~~~~~~~~~~~~~

1. **Model Understanding**: Consider the ANOVA model :math:`X_{ij} = \mu_i + \varepsilon_{ij}` for a study comparing four different exercise programs with 12 participants per program.

   a) How many :math:`\mu_i` parameters are there and what do they represent?
   b) How many :math:`\varepsilon_{ij}` terms are there and what assumptions do we make about them?

2. **Sum of Squares Calculation**: Given summary data for three groups:

   - Group 1: :math:`n_1 = 8`, :math:`\bar{x}_1 = 15.2`, :math:`s_1 = 2.3`
   - Group 2: :math:`n_2 = 10`, :math:`\bar{x}_2 = 12.8`, :math:`s_2 = 1.9`  
   - Group 3: :math:`n_3 = 7`, :math:`\bar{x}_3 = 18.1`, :math:`s_3 = 2.7`

   a) Calculate the overall sample mean :math:`\bar{x}_{\cdot \cdot}`.
   b) Calculate SSA.
   c) Calculate SSE.
   d) Calcalate SST.