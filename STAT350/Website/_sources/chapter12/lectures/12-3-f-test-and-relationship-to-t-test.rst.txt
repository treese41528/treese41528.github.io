.. _12-3-f-test-and-relationship-to-t-test:


.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch12-3">
      <iframe
         id="video-ch12-3"
         title="STAT 350 – Chapter 12.3 One-Way Hypothesis Test and F-Test Statistic Video"
         src="https://www.youtube.com/embed/wr-jFQm3DzM?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides
   :class: tip

   `Download Chapter 12 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/Chapter%2012%20ANOVA/OneWayANOVA_AC.pptx>`_


One-Way ANOVA F-Test and Its Relationship to Two-Sample t-Tests
===================================================================

We've developed the theoretical foundation for ANOVA by decomposing total variability into 
between-group and within-group components. Now we can construct the actual hypothesis test 
that will tell us whether observed differences in sample means are statistically significant 
or could reasonably be attributed to random variation.

.. admonition:: Road Map 🧭
   :class: important

   • **Problem we will solve** – How to construct a formal hypothesis test using the variance decomposition to compare multiple population means simultaneously

   • **Tools we'll learn** – The F-test statistic, F-distribution properties, and the complete ANOVA testing procedure, plus the connection between F-tests and t-tests

   • **How it fits** – This transforms our variance decomposition into a practical decision-making tool while showing how ANOVA generalizes familiar two-sample procedures

The F-Test Statistic: Formalizing Our Intuition
-------------------------------------------------

We've established that when the null hypothesis is true, both MSA and MSE estimate the same quantity (:math:`\sigma^2`). When the null hypothesis is false, MSE still estimates :math:`\sigma^2`, but MSA estimates something larger. This suggests we should compare these two quantities using a ratio.

The ANOVA Test Statistic
~~~~~~~~~~~~~~~~~~~~~~~~

Our test statistic is:

.. math::

   F_{TS} = \frac{\text{MSA}}{\text{MSE}} = \frac{\text{Between-group variability}}{\text{Within-group variability}}

Expanding this using our formulas:

.. math::

   F_{TS} = \frac{\frac{1}{k-1}\sum_{i=1}^k n_i(\bar{X}_{i \cdot} - \bar{X}_{\cdot \cdot})^2}{\frac{1}{n-k}\sum_{i=1}^k (n_i - 1)s_i^2}

This ratio captures exactly what we were looking for visually in our boxplots—it compares how spread out the group means are relative to the natural variability within groups.

Interpreting the F-Statistic
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**When :math:`H_0` is true** (all population means equal):

- Both MSA and MSE estimate :math:`\sigma^2`
- The ratio :math:`F_{TS}` should be close to 1
- Values much larger than 1 would be unusual

**When :math:`H_0` is false** (at least one mean differs):

- MSE still estimates :math:`\sigma^2`  
- MSA estimates :math:`\sigma^2` plus additional variation from group differences
- The ratio :math:`F_{TS}` should be substantially larger than 1

Large values of :math:`F_{TS}` provide evidence against the null hypothesis. But how large is "large enough"? We need to understand the sampling distribution of this statistic.

The F-Distribution
--------------------

Under the null hypothesis and when all assumptions are satisfied, the test statistic :math:`F_{TS}` follows what's called an F-distribution. This distribution has some unique characteristics that make it perfect for ANOVA.

Properties of the F-Distribution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The F-distribution :math:`F(df_A, df_E)` has two parameters—the numerator degrees of freedom (:math:`df_A = k-1`) and denominator degrees of freedom (:math:`df_E = n-k`).

Key properties:

1. **Always positive**: Since we're taking a ratio of squared terms, :math:`F_{TS} \geq 0`
2. **Right-skewed**: The distribution is not symmetric like the normal or t-distributions
3. **Mean approximately 1**: When :math:`H_0` is true, the expected value is close to 1
4. **Shape controlled by degrees of freedom**: Both :math:`df_A` and :math:`df_E` affect the distribution's shape

The shape becomes less skewed as the degrees of freedom increase, but it remains right-skewed unlike the symmetric distributions we've used before.

Using the F-Distribution in R
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Just like other distributions, we can work with the F-distribution using R functions:

- **pf()**: Calculates probabilities (p-values)
- **qf()**: Finds critical values
- **df()**: Density function (rarely used in practice)

For ANOVA, we always use ``lower.tail = FALSE`` in the pf() function because we're interested in the probability of observing an F-statistic as large or larger than what we calculated.

The Complete ANOVA Hypothesis Testing Procedure
-------------------------------------------------

Now we can formalize the complete four-step hypothesis testing procedure for one-way ANOVA.

Step 1: Define Parameters and Hypotheses
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Parameters of interest**: :math:`\mu_1, \mu_2, \ldots, \mu_k` representing the population means for the :math:`k` different groups.

**Hypotheses**:

:math:`H_0: \mu_1 = \mu_2 = \cdots = \mu_k` (all population means are equal)

:math:`H_a:` At least one :math:`\mu_i` differs from the others

The alternative hypothesis can be written in several equivalent ways:

- At least one :math:`\mu_i` is different from the rest
- Not all population means are equal  
- :math:`\mu_i \neq \mu_j` for some :math:`i \neq j`

Step 2: Check Assumptions
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before proceeding, verify that the ANOVA assumptions are reasonable:

1. **Independence**: Observations within and between groups are independent
2. **Normality**: Each population is normally distributed (or sample sizes are large enough for CLT)
3. **Equal variances**: All populations have the same variance :math:`\sigma^2`

For the equal variance assumption, use the rule of thumb:

.. math::

   \frac{\max(s_1, s_2, \ldots, s_k)}{\min(s_1, s_2, \ldots, s_k)} \leq 2

Step 3: Calculate Test Statistic and P-Value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Test statistic**:

.. math::

   F_{TS} = \frac{\text{MSA}}{\text{MSE}}

**Degrees of freedom**:
- Numerator: :math:`df_A = k - 1`
- Denominator: :math:`df_E = n - k`

**P-value**:

.. code-block:: r

   pf(F_TS, df1 = df_A, df2 = df_E, lower.tail = FALSE)

**In practice**, you'll typically use R's built-in ANOVA function:

.. code-block:: r

   fit <- aov(response_variable ~ factor_variable, data = dataframe)
   summary(fit)

Step 4: Make Decision and State Conclusion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Decision rule**: 

- If p-value ≤ :math:`\alpha`, reject :math:`H_0`
- If p-value > :math:`\alpha`, fail to reject :math:`H_0`

**Conclusion template**: 

"The data [does/does not] give [weak/moderate/strong] support (p-value = [value]) to the claim that [statement of :math:`H_a` in context]."

The ANOVA Table
------------------

The standard way to present ANOVA results is through an ANOVA table that summarizes all the variance decomposition components:

.. list-table:: ANOVA Table Format
   :header-rows: 1
   :widths: 20 15 25 20 15 10

   * - Source
     - df
     - Sum of Squares  
     - Mean Square
     - F-value
     - Pr(>F)
   * - Factor A
     - :math:`k-1`
     - :math:`\text{SSA} = \sum_{i=1}^k n_i(\bar{x}_{i \cdot} - \bar{x}_{\cdot \cdot})^2`
     - :math:`\text{MSA} = \frac{\text{SSA}}{k-1}`
     - :math:`\frac{\text{MSA}}{\text{MSE}}`
     - p-value
   * - Error
     - :math:`n-k`
     - :math:`\text{SSE} = \sum_{i=1}^k \sum_{j=1}^{n_i}(x_{ij} - \bar{x}_{i \cdot})^2`
     - :math:`\text{MSE} = \frac{\text{SSE}}{n-k}`
     - 
     - 
   * - Total
     - :math:`n-1`
     - :math:`\text{SST} = \sum_{i=1}^k \sum_{j=1}^{n_i}(x_{ij} - \bar{x}_{\cdot \cdot})^2`
     - :math:`\text{MST} = \frac{\text{SST}}{n-1}`
     - 
     - 

The total row is often omitted from R output but can be calculated since SST = SSA + SSE and the degrees of freedom also add up.

**Estimating the common standard deviation**: :math:`s = \sqrt{\text{MSE}}`

Coffeehouse Example: Complete Analysis
-----------------------------------------

Let's work through our coffeehouse example using the complete ANOVA procedure to see how everything fits together.

Setting Up the Problem
~~~~~~~~~~~~~~~~~~~~~~

**Research question**: Do the five coffeehouses around campus attract customers of different average ages?

**Study design**: A reporter surveys approximately 50 random customers at each coffeehouse, asking for their age.

**Data summary** (from previous analysis):

- Total sample size: :math:`n = 200`
- Number of groups: :math:`k = 5`
- Group sample sizes: :math:`n_1 = 39, n_2 = 38, n_3 = 42, n_4 = 38, n_5 = 43`
- Sample means: :math:`\bar{x}_1 = 39.13, \bar{x}_2 = 46.66, \bar{x}_3 = 40.50, \bar{x}_4 = 26.42, \bar{x}_5 = 34.07`
- Sample variances: :math:`s_1^2 = 7.90, s_2^2 = 12.97, s_3^2 = 10.94, s_4^2 = 6.99, s_5^2 = 9.92`

Step 1: Parameters and Hypotheses
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Parameters**: :math:`\mu_{\text{Age}_1}, \mu_{\text{Age}_2}, \mu_{\text{Age}_3}, \mu_{\text{Age}_4}, \mu_{\text{Age}_5}` representing the mean customer age at coffeehouses 1, 2, 3, 4, and 5 respectively.

**Hypotheses**:

:math:`H_0: \mu_{\text{Age}_1} = \mu_{\text{Age}_2} = \mu_{\text{Age}_3} = \mu_{\text{Age}_4} = \mu_{\text{Age}_5}`

:math:`H_a: \mu_{\text{Age}_i} \neq \mu_{\text{Age}_j}` for some :math:`i \neq j`

Step 2: Check Assumptions
~~~~~~~~~~~~~~~~~~~~~~~~

**Equal variance check**:

.. math::

   \frac{\max(s_i)}{\min(s_i)} = \frac{\sqrt{12.97}}{\sqrt{6.99}} = \frac{3.60}{2.64} = 1.36 \leq 2 ✓

Since 1.36 < 2, the equal variance assumption appears reasonable.

**Normality**: Visual inspection of histograms for each group shows approximate normality with no extreme deviations. With sample sizes around 40 in each group, the Central Limit Theorem helps ensure the sampling distribution of means is approximately normal.

Step 3: Calculate Test Statistic and P-Value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Degrees of freedom**:
- :math:`df_A = k - 1 = 5 - 1 = 4`
- :math:`df_E = n - k = 200 - 5 = 195`

Using R's ANOVA function on the full dataset:

.. code-block:: r

   fit <- aov(Age ~ Coffeehouse, data = coffeehouse_df)
   summary(fit)

**ANOVA Table Results**:

.. list-table:: Coffeehouse ANOVA Results
   :header-rows: 1

   * - Source
     - df
     - Sum Sq
     - Mean Sq
     - F value
     - Pr(>F)
   * - Coffeehouse
     - 4
     - 8834
     - 2208.4
     - 22.14
     - 4.4e-15
   * - Residuals
     - 195
     - 19451
     - 99.8
     - 
     - 

**Test statistic**: :math:`F_{TS} = 22.14`

**P-value**: :math:`4.4 \times 10^{-15}` (extremely small)

Step 4: Decision and Conclusion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Using :math:`\alpha = 0.01`:

**Decision**: Since p-value = :math:`4.4 \times 10^{-15} < 0.01 = \alpha`, we reject :math:`H_0`.

**Conclusion**: The data gives strong support (p-value = :math:`4.4 \times 10^{-15}`) to the claim that at least one of the coffee shops around campus differs in the mean age of customers from the rest.

The F-statistic of 22.14 is much larger than 1, indicating that the between-group variability is more than 22 times the within-group variability—strong evidence that the coffeehouses attract customers of systematically different ages.



The Connection Between F-Tests and t-Tests
----------------------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch12-3">
      <iframe
         id="video-ch12-3"
         title="STAT 350 – Chapter 12.3.1 One-Way ANOVA and Two Independent Sample t-test Relationship"
         src="https://www.youtube.com/embed/8hNoZPqspq0?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>


An important theoretical connection exists between ANOVA and the two-sample t-test. When we have exactly two groups (:math:`k = 2`), the one-way ANOVA F-test is mathematically equivalent to the two-sample t-test with equal variance assumption.

Relationship When k = 2
~~~~~~~~~~~~~~~~~~~~~~~

For two groups, our F-statistic becomes:

.. math::

   F_{TS} = \frac{\text{MSA}}{\text{MSE}} = \frac{n_1(\bar{X}_1 - \bar{X}_{\cdot \cdot})^2 + n_2(\bar{X}_2 - \bar{X}_{\cdot \cdot})^2}{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}

Through algebraic manipulation (which involves expressing the overall mean :math:`\bar{X}_{\cdot \cdot}` as a weighted average of the group means), this simplifies to:

.. math::

   F_{TS} = \frac{(\bar{X}_1 - \bar{X}_2)^2}{s_p^2\left(\frac{1}{n_1} + \frac{1}{n_2}\right)} = t_{TS}^2

where :math:`t_{TS}` is the two-sample t-statistic with pooled variance:

.. math::

   t_{TS} = \frac{\bar{X}_1 - \bar{X}_2}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}

This means **F-statistic = (t-statistic)²** when comparing two groups.

Comparing F-Tests and t-Tests
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. list-table:: F-test vs t-test Comparison
   :header-rows: 1
   :widths: 25 35 35

   * - Feature
     - Two-Sample t-Test
     - One-Way ANOVA
   * - **Variance Assumption**
     - Can assume equal or unequal variances
     - Assumes equal variances
   * - **Alternative Hypothesis**
     - Directional (>, <) or non-directional (≠)
     - Non-directional only
   * - **Distribution**
     - t-distribution (symmetric)
     - F-distribution (right-skewed)
   * - **Null Value**
     - Can test :math:`\mu_1 - \mu_2 = \Delta_0`
     - Tests only :math:`\mu_1 = \mu_2` (no null value)
   * - **Number of Groups**
     - Exactly 2 groups
     - 2 or more groups

When to Use Each Test
~~~~~~~~~~~~~~~~~~~~~~~

**Use two-sample t-test when**:
- You have exactly 2 groups
- You want to test directional alternatives (one-sided tests)
- You prefer not to assume equal variances (Welch's t-test)
- You want to test a specific difference (:math:`\mu_1 - \mu_2 = \Delta_0`)

**Use one-way ANOVA when**:
- You have 3 or more groups
- You want to test for any differences among groups
- You're willing to assume equal variances
- You want an overall test before looking at specific comparisons

The beauty of this connection is that it shows ANOVA as a natural generalization of the two-sample procedures we've already mastered.

What Happens After Rejecting H₀?
------------------------------------

When ANOVA indicates that "at least one mean differs from the others," this naturally leads to the question: "Which specific groups are different?" ANOVA doesn't tell us which means differ—only that they're not all equal.

This limitation leads us to **multiple comparison procedures**, which we'll explore in the next section. These methods allow us to make specific pairwise comparisons while controlling the overall error rate.

For now, it's important to understand that ANOVA serves as a "gatekeeper" test. If we fail to reject the null hypothesis in ANOVA, we typically stop there and conclude there's no evidence for group differences. If we do reject the null hypothesis, then we proceed to investigate which specific groups differ.


Bringing It All Together
-------------------------------

.. admonition:: Key Takeaways 📝
   :class: important

   1. **The F-test statistic** :math:`\frac{\text{MSA}}{\text{MSE}}` compares between-group to within-group variability, with large values providing evidence against :math:`H_0`.

   2. **The F-distribution** is right-skewed, always positive, with mean approximately 1 when :math:`H_0` is true, and shape controlled by two degrees of freedom parameters.

   3. **The complete ANOVA procedure** follows the standard four-step hypothesis testing framework, with R's ``aov()`` function providing convenient computation.

   4. **The ANOVA table** organizes all variance decomposition components and provides the F-statistic and p-value for decision making.

   5. **F-tests and t-tests are equivalent** when comparing exactly two groups: F-statistic = (t-statistic)² under equal variance assumptions.

   6. **ANOVA serves as a gatekeeper test** that determines whether any group differences exist before investigating specific pairwise comparisons.

   7. **Practical significance** depends not just on statistical significance but also on the magnitude of differences relative to within-group variability.

Exercises
~~~~~~~~~~~~

1. **F-Distribution Properties**: An ANOVA comparing 4 groups with 60 total observations yields an F-statistic of 2.8.

   a) What are the degrees of freedom for this F-statistic?
   
   b) Would you expect this F-value to be statistically significant at :math:`\alpha = 0.05`? Why?
   
   c) How would the shape of this F-distribution compare to F(1,56)?

2. **ANOVA Table Completion**: Complete the missing entries in this ANOVA table:

   .. list-table::
      :header-rows: 1

      * - Source
        - df
        - Sum Sq
        - Mean Sq
        - F value
        - Pr(>F)
      * - Treatment
        - 3
        - 450
        - ?
        - ?
        - 0.008
      * - Error
        - ?
        - ?
        - 25
        - 
        - 
      * - Total
        - 47
        - ?
        - ?
        - 
        - 

3. **F-test vs t-test Connection**: In a study comparing two teaching methods with 20 students each, the two-sample t-statistic (equal variance) is -2.4.

   a) What would the F-statistic be for the equivalent one-way ANOVA?
   
   b) What are the degrees of freedom for both tests?
   
   c) How do the p-values compare between the two-sided t-test and the F-test?

4. **Complete ANOVA Analysis**: A researcher studies the effect of four different fertilizers on plant height, with the following summary data:

   - Fertilizer A: :math:`n_1 = 12, \bar{x}_1 = 18.5, s_1 = 3.2`
   - Fertilizer B: :math:`n_2 = 15, \bar{x}_2 = 22.1, s_2 = 2.8`  
   - Fertilizer C: :math:`n_3 = 10, \bar{x}_3 = 19.8, s_3 = 3.6`
   - Fertilizer D: :math:`n_4 = 13, \bar{x}_4 = 25.2, s_4 = 3.0`

   a) Check the equal variance assumption
   
   b) Set up appropriate hypotheses
   
   c) Calculate the overall sample mean
   
   d) Would you expect to reject :math:`H_0` based on the sample means? Explain your reasoning.

5. **Interpretation Questions**:

   a) Explain why F-values are always non-negative but t-values can be negative
   
   b) Why do we always use ``lower.tail = FALSE`` when calculating p-values for F-tests?
   
   c) What does it mean when MSA is much larger than MSE?
   
   d) How would the F-statistic change if all group means increased by the same amount?