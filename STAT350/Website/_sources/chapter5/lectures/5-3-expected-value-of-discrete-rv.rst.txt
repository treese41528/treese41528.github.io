.. _5-3-expected-value-of-discrete-rv:


.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch5-3">
     <iframe
       id="video-ch5-3"
       title="STAT 350 â€“ Chapter 5.3 Expected Value of a Discrete Random Variable Video"
       src="https://www.youtube.com/embed/hTusBEM88fA?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
       allowfullscreen>
     </iframe>
   </div>

.. admonition:: Slides ðŸ“Š
   :class: tip
   
   `Download Chapter 5 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%205%20Discrete%20Distributions/L9-11-RandomVariables%20DiscreteProbabilityDistributions%28Chapter%205%29_AC.pptx>`_
   
Expected Value of a Discrete Random Variable
==========================================================

A probability mass function provides the probabilities of individual outcomes at a fundamental level, 
but it also conveys much richer information about the overall behavior of a random variable. 
The first property we will study is the **expected value**, which represents the "center" of the 
values generated by the random variable.

.. admonition:: Road Map ðŸ§­
   :class: important

   â€¢ Define the **expected value** of a discrete random variable as a weighted average.
   â€¢ Master the **Law of the Unconscious Statistician (LOTUS)** for computing expectations of 
     functions of random variables.
   â€¢ Discover key **properties of expectation** that simplify calculations.

From Samples to Populations: Expected Value
--------------------------------------------------------------

When analyzing a data set, we use the *sample* mean to represent the "center" of the data. 
It is computed by summing all observed values and dividing by the sample sizeâ€”effectively 
giving *each observation equal weight*. If certain values are more likely to occur than others, 
this tendency is usually reflected in their higher frequencies within the sample.

Behind every generated data set is a random variable and its probability mass function (PMF), 
which govern how frequently different values are likely to appear. So how do we compute the 
"center" of this *true*, or *population*, distribution?

The key insight is to use probabilities as weights. If certain outcomes are more 
likely than others, then the center of the distribution should be pulled toward those values.

Definition
~~~~~~~~~~~~~

The expected value of a discrete random variable :math:`X`, denoted :math:`E[X]` or :math:`\mu_X`, 
is the weighted average of all possible values in its support, with weights equal to their probabilities:

.. math::

   \mu_X = E[X] = \sum_{x \in \text{supp}(X)} x \, p_X(x) = \sum_{x \in \text{supp}(X)} x \, P(X = x)

This formula makes intuitive sense; values that occur with higher probability contribute more to the average, 
while rare outcomes have less influence. 

.. admonition:: ExampleðŸ’¡: Salamander Insurance Company
   :class: note

   Salamander Insurance Company (SIC) has collected data on moving violations among 
   its customers over a three-year period. They want to understand the typical number 
   of violations to better price their policies.

   Let :math:`X` denote the number of moving violations for a randomly selected customer, with the following distribution:

   .. flat-table::
      :header-rows: 1
      :align: center
      :width: 60%

      * - :math:`x`
        - 0
        - 1
        - 2
        - 3
      * - :math:`p_X(x)`
        - 0.60
        - 0.25
        - 0.10
        - 0.05

   Most customers have no violations, while progressively fewer have one, two, or three violations. 
   The expected number of violations is:

   .. math::

      E[X] &= (0)(0.60) + (1)(0.25) + (2)(0.10) + (3)(0.05) \\
      &= 0.25 + 0.20 + 0.15 = 0.60

   On average, a customer has 0.6 moving violations over three years. This value helps SIC understand the typical violation behavior, even though no individual customer ever has exactly 0.6 violations.


Do Expected Values Always Exist?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For a random variable with finite support, say :math:`\text{supp}(X) = \{x_1, x_2, \cdots, x_n\}` , 
the expected value 

.. math::

   E[X] = \sum_{i=1}^{n} x_i \, p_X(x_i)

is a sum of finitely many finite terms and is therefore always well-defined.

However, when dealing with a random variable with countably infinite support, we need an additional condition 
to ensure the expected value is well-defined. Specifically, the sum must be **absolutely convergent**:

.. math::

   \sum_{x \in \text{supp}(X)} |x| \, p_X(x) < \infty

If this condition is not met, the expected value might not exist or might depend on the order of 
summation, creating mathematical ambiguities. Fortunately, **all the random variables we encounter 
in this course have well-defined expectations**.

Important Facts About Expected Values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* The expected value does **not necessarily correspond to a value in the support**. 
  
  For instance, the expected number of heads in one flip of a fair coin is 0.5, even though any 
  single flip must result in either 0 or 1 heads, never 0.5. 
  The expected value reflects the **long-term average behavior** rather than any single outcome.

* The expected value has a few different names. **Expectation, mean of a random variable, and average
  of a random variable** are all alternative expressions for expected value. 
 
  Note that we are now working with two types of means, each with its own definition and formulaâ€”the 
  sample mean (from data) and the expected value (from a probability distribution). When confused
  between the two, pay attention to whether the mean is for data or a random variable.


Expected Value of Functions of a Random Variable
--------------------------------------------------------

In practice, we're often interested not only in a random variable itself, but also in its functions. For example, 

* If :math:`X` represents a measurement in inches, we might want to convert 
  it to centimeters (:math:`2.54X`).
* If :math:`Y` represents a count, we might be interested in :math:`Y^2`. 

We use the Law of the Unconscious Statistician (LOTUS) to understand the central behavior of functions of
a random variable. 

Law of the Unconscious Statistician (LOTUS) 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

If :math:`X` is a discrete random variable and :math:`g(Â·)` is a real-valued function defined 
on the support of :math:`X`, then

.. math::

   E[g(X)] = \sum_{x\in \text{supp}(X)} g(x) \, p_X(x)

This theorem tells us that to find the expected value of :math:`g(X)`, we don't need to derive the PMF 
of the new random variable :math:`Y = g(X)`. Instead, we can simply apply the function :math:`g` to each value 
in the support of :math:`X`, **weight these transformed values by their original probabilities**, and sum them up.

LOTUS greatly simplifies calculations involving functions of random variables, 
as we'll see in the examples that follow.

.. admonition:: ExampleðŸ’¡: Salamander Insurance Company, Continued
   :class: note

   Compute the expectation of :math:`X^2`, where :math:`X` represents the 
   number of moving violations made by a randomly selected customer.

   .. flat-table::
      :header-rows: 1
      :align: center
      :width: 60%

      * - :math:`x`
        - 0
        - 1
        - 2
        - 3
      * - :math:`p_X(x)`
        - 0.60
        - 0.25
        - 0.10
        - 0.05

   Let :math:`g(x) = x^2`. Using LOTUS,

   .. math:: 

      E[X^2] &= g(0)p_X(0) + g(1)p_X(1) + g(2)p_X(2)+ g(3)p_X(3)\\
      &=(0^2)(0.6) + (1^2)(0.25) +(2^2)(0.1) + (3^2)(0.05) =  1.1

.. admonition:: Be cautious ðŸ›‘
   :class: danger

   It is generally NOT true that :math:`E[g(X)] = g(E[X])`. In the Salamander Insurance Company example,
   :math:`(E[X])^2 = (0.6)^2` which is not equal to :math:`E[X^2] = 1.1.`

Leveraging Properties of Expected Value
-----------------------------------------

The expected value has several elegant properties that streamline calculations. 
Let's explore the most important ones.

A. Linearity of Expectation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If :math:`g` is a linear function of the form :math:`g(x) = ax + b`, 
where :math:`a` and :math:`b` are constants, we have:

.. math::

   E[aX + b] = a \, E[X] + b.

This property is particularly useful because it allows us to "push" the expected 
value operation through linear operations. For example, if we know :math:`E[X] = 3` and 
we want to find :math:`E[2X + 5]`, we can directly calculate :math:`E[2X + 5] = 2Â·3 + 5 = 11`, 
without repeating the summation.

We can verify this property using LOTUS:

.. math::

   E[aX + b] &= \sum_{x\in \text{supp}(X)} (ax + b) \, p_X(x) \\
   &= a \sum_{x\in \text{supp}(X)} x \, p_X(x) + b \sum_{x\in \text{supp}(X)} p_X(x) \\
   &= a \, E[X] + b \cdot 1 \\
   &= a \, E[X] + b

:math:`\sum_{x\in \text{supp}(X)} p_X(x)=1` due to the second condition for a valid PMF.

B. Additivity of Expectation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The expected value operation also distributes over sums and differences of 
random variables. For any two random variables :math:`X` and :math:`Y`:

.. math::

   E[X \pm Y] = E[X] \pm E[Y].

This property extends naturally to any finite collection of random variables:

.. math::

   E[X_1 \pm X_2 \pm \cdots \pm X_n] = E[X_1] \pm E[X_2] \pm \cdots \pm E[X_n].

A remarkable aspect of this property is that it holds regardless of whether the random 
variables are independent. While many properties involving multiple random variables
depend on independence, additivity of expectation does not.

C. Monotonicity
~~~~~~~~~~~~~~~~~

There's one more property worth mentioning briefly: monotonicity. If a random variable :math:`X` is always 
less than or equal to another random variable :math:`Y` (:math:`P(X > Y) = 0`), then:

.. math::

   E[X] \leq E[Y].


.. admonition:: ExampleðŸ’¡: SIC, Continued

   SIC also tracks the number of accidents (Y) and has compiled the joint 
   distribution of moving violations and accidents:

   .. flat-table:: 
      :header-rows: 1
      :width: 80%
      :align: center

      * - :math:`x` \\ :math:`y`
        - 0
        - 1
        - 2
        - :math:`p_X(x)`
      * - 0
        - 0.58
        - 0.015
        - 0.005
        - 0.60
      * - 1
        - 0.18
        - 0.058
        - 0.012
        - 0.25
      * - 2
        - 0.02
        - 0.078
        - 0.002
        - 0.10
      * - 3
        - 0.02
        - 0.029
        - 0.001
        - 0.05
      * - :math:`p_Y(y)`
        - 0.80
        - 0.18
        - 0.02
        - 1

   The company determines monthly premiums based on both factors, using the formula:

   .. math::

      g(X,Y) = 95 + 10Y^3 + 120Y + 25X

   This formula incorporates a base rate ($95), 
   a penalty that scales linearly with violations ($25X), 
   and terms that increase dramatically with accidents (both linear and cubic terms for Y).

   To find the expected monthly premium across all customers, we use the additivity and linearity properties:

   .. math::

      E[g(X,Y)] &= E[95 + 10Y^3 + 120Y + 25X] \\
      &= 95 + 10E[Y^3] + 120E[Y] + 25E[X].

   We already calculated :math:`E[X] = 0.6`. For the remaining terms,

   .. math::

      E[Y] &= (0)(0.80) + (1) (0.18) + (2) (0.02) = 0.22 \\
      E[Y^3] &= (0^3) (0.80) + (1^3) (0.18) + (2^3) (0.02) \\
      &= 0 + 0.18 + 0.16 = 0.34.

   Substituting these values,

   .. math::

      E[g(X,Y)] &= 95 + (10) (0.34) + (120) (0.22) + (25) (0.6) \\
      &= 95 + 3.4 + 26.4 + 15 = 139.8.

   Therefore, the expected monthly premium is $139.80.

Bringing It All Together
--------------------------

.. admonition:: Key Takeaways ðŸ“
   :class: important

   1. The **expected value** of a discrete random variable represents its long-term average 
      behaviorâ€”a weighted average of all possible outcomes with weights equal to their probabilities.
   
   2. The **Law of the Unconscious Statistician (LOTUS)** provides a method for finding 
      the expected value of a function of a random variable.
   
   3. Expected values follow **linearity**: :math:`E[aX + b] = aE[X] + b`, allowing us to simplify 
      calculations involving linear transformations.
   
   4. Expected values are **additive**: :math:`E[X + Y] = E[X] + E[Y]`, regardless of whether X and Y 
      are independent, which greatly simplifies calculations involving sums of random variables.

To fully characterize a random variable, we need more than just its expected value. 
In the next section, we'll explore how to quantify the spread or variability of a discrete random 
variable around its expected valueâ€”a concept analogous to the sample variance and standard deviation of a dataset.


Exercises
---------

These exercises develop your skills in computing expected values, applying the Law of the Unconscious Statistician (LOTUS), and using the linearity and additivity properties of expectation.

.. admonition:: Exercise 1: Basic Expected Value Calculation
   :class: note

   A software quality assurance team tracks the number of bugs :math:`X` found per code review session. Based on historical data, the PMF is:

   .. flat-table:: PMF for Bugs per Code Review
      :header-rows: 1
      :widths: 20 16 16 16 16 16

      * - :math:`x`
        - 0
        - 1
        - 2
        - 3
        - 4
      * - :math:`p_X(x)`
        - 0.30
        - 0.35
        - 0.20
        - 0.10
        - 0.05

   a. Calculate :math:`E[X]`, the expected number of bugs per review.

   b. Calculate :math:`E[X^2]`.

   c. If each bug takes an average of 2 hours to fix, and there's a 30-minute (0.5 hour) overhead for each review session regardless of bugs found, calculate the expected total time spent per review session. That is, find :math:`E[2X + 0.5]`.

   d. Verify your answer to part (c) using the linearity property.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[X]**

      .. math::

         E[X] = \sum_{x} x \cdot p_X(x) = (0)(0.30) + (1)(0.35) + (2)(0.20) + (3)(0.10) + (4)(0.05)

      .. math::

         = 0 + 0.35 + 0.40 + 0.30 + 0.20 = 1.25

      The expected number of bugs per review is **1.25 bugs**.

      **Part (b): E[XÂ²]**

      Using LOTUS with :math:`g(x) = x^2`:

      .. math::

         E[X^2] = \sum_{x} x^2 \cdot p_X(x) = (0)^2(0.30) + (1)^2(0.35) + (2)^2(0.20) + (3)^2(0.10) + (4)^2(0.05)

      .. math::

         = 0 + 0.35 + 0.80 + 0.90 + 0.80 = 2.85

      **Part (c): E[2X + 0.5] using LOTUS directly**

      Using LOTUS with :math:`g(x) = 2x + 0.5`:

      .. math::

         E[2X + 0.5] = \sum_{x} (2x + 0.5) \cdot p_X(x)

      .. math::

         = (0.5)(0.30) + (2.5)(0.35) + (4.5)(0.20) + (6.5)(0.10) + (8.5)(0.05)

      .. math::

         = 0.15 + 0.875 + 0.90 + 0.65 + 0.425 = 3.0 \text{ hours}

      **Part (d): Verification using linearity**

      By linearity of expectation: :math:`E[aX + b] = aE[X] + b`

      .. math::

         E[2X + 0.5] = 2 \cdot E[X] + 0.5 = 2(1.25) + 0.5 = 2.5 + 0.5 = 3.0 \text{ hours}

      âœ“ Both methods give the same answer: **3.0 hours** expected time per review session.

----

.. admonition:: Exercise 2: LOTUS with Non-Linear Functions
   :class: note

   A data center monitors server response times. Due to system architecture, the actual processing time :math:`T` (in milliseconds) follows this distribution:

   .. flat-table:: PMF for Processing Time
      :header-rows: 1
      :widths: 25 25 25 25

      * - :math:`t`
        - 10
        - 20
        - 50
      * - :math:`p_T(t)`
        - 0.70
        - 0.20
        - 0.10

   The server's power consumption (in watts) is modeled as :math:`P(t) = 5 + 0.1t^2`.

   a. Calculate :math:`E[T]`, the expected processing time.

   b. Calculate the expected power consumption :math:`E[P(T)] = E[5 + 0.1T^2]`.

   c. A naive engineer calculates power consumption by plugging the expected time into the formula: :math:`P(E[T]) = 5 + 0.1(E[T])^2`. What value do they get?

   d. Explain why :math:`E[P(T)] \neq P(E[T])`. Which is the correct expected power consumption?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[T]**

      .. math::

         E[T] = (10)(0.70) + (20)(0.20) + (50)(0.10) = 7 + 4 + 5 = 16 \text{ ms}

      **Part (b): E[P(T)] = E[5 + 0.1TÂ²]**

      Using linearity and LOTUS:

      .. math::

         E[5 + 0.1T^2] = 5 + 0.1 \cdot E[T^2]

      First, find :math:`E[T^2]`:

      .. math::

         E[T^2] = (10)^2(0.70) + (20)^2(0.20) + (50)^2(0.10) = 70 + 80 + 250 = 400

      Therefore:

      .. math::

         E[P(T)] = 5 + 0.1(400) = 5 + 40 = 45 \text{ watts}

      **Part (c): Naive calculation P(E[T])**

      .. math::

         P(E[T]) = 5 + 0.1(16)^2 = 5 + 0.1(256) = 5 + 25.6 = 30.6 \text{ watts}

      **Part (d): Explanation**

      The naive calculation gives **30.6 watts**, but the correct expected power is **45 watts**.

      This illustrates a fundamental principle: **E[g(X)] â‰  g(E[X])** in general, unless :math:`g` is a linear function.

      The function :math:`P(t) = 5 + 0.1t^2` is **convex** (curves upward). For convex functions, Jensen's inequality tells us :math:`E[g(X)] \geq g(E[X])`. Indeed, 45 > 30.6.

      **The correct expected power consumption is 45 watts** (using LOTUS). The naive approach underestimates power usage because it ignores the variability in processing timesâ€”occasional long processes (50 ms) consume disproportionately more power due to the squared term.

----

.. admonition:: Exercise 3: Expected Value with Negative Outcomes
   :class: note

   A venture capital firm evaluates startup investments. For a typical $100,000 investment, let :math:`X` represent the return multiplier (how many times the investment is returned):

   .. flat-table:: PMF for Return Multiplier
      :header-rows: 1
      :widths: 17 17 17 17 17 17

      * - :math:`x`
        - 0
        - 0.5
        - 1
        - 3
        - 10
      * - :math:`p_X(x)`
        - 0.40
        - 0.25
        - 0.20
        - 0.10
        - 0.05

   Note: :math:`x = 0` means total loss, :math:`x = 1` means breaking even, :math:`x = 3` means tripling the investment, etc.

   a. Calculate :math:`E[X]`, the expected return multiplier.

   b. If the firm invests $100,000, what is the expected dollar return? (Hint: The dollar return is :math:`100000 \cdot X`)

   c. What is the expected profit (return minus initial investment)? Is this a profitable investment strategy on average?

   d. The firm considers a "safe" alternative that guarantees :math:`x = 1.1` (10% return). Which option has higher expected return multiplier?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[X]**

      .. math::

         E[X] = (0)(0.40) + (0.5)(0.25) + (1)(0.20) + (3)(0.10) + (10)(0.05)

      .. math::

         = 0 + 0.125 + 0.20 + 0.30 + 0.50 = 1.125

      The expected return multiplier is **1.125**.

      **Part (b): Expected dollar return**

      .. math::

         E[100000 \cdot X] = 100000 \cdot E[X] = 100000 \times 1.125 = \$112,500

      **Part (c): Expected profit**

      Profit = Return - Investment = :math:`100000X - 100000 = 100000(X - 1)`

      .. math::

         E[\text{Profit}] = 100000 \cdot E[X - 1] = 100000 \cdot (E[X] - 1) = 100000(1.125 - 1) = \$12,500

      Yes, this is a profitable investment strategy on average, with an expected profit of **$12,500** per investment (12.5% expected return).

      **Part (d): Comparison with safe alternative**

      - Risky strategy: :math:`E[X] = 1.125` (12.5% expected return)
      - Safe alternative: :math:`X = 1.1` with certainty (10% guaranteed return)

      The risky strategy has a higher expected return multiplier (**1.125 > 1.1**).

      However, the risky strategy has a 40% chance of total loss and only a 15% chance of exceeding the safe return. The choice depends on risk toleranceâ€”expected value alone doesn't capture the full picture.

----

.. admonition:: Exercise 4: Additivity of Expectation
   :class: note

   A manufacturing plant has two production lines. Let :math:`X` = number of defective items from Line A per hour and :math:`Y` = number of defective items from Line B per hour.

   The marginal PMFs are:

   .. flat-table:: PMF for Line A Defects
      :header-rows: 1
      :widths: 25 25 25 25

      * - :math:`x`
        - 0
        - 1
        - 2
      * - :math:`p_X(x)`
        - 0.70
        - 0.20
        - 0.10

   .. flat-table:: PMF for Line B Defects
      :header-rows: 1
      :widths: 25 25 25 25

      * - :math:`y`
        - 0
        - 1
        - 2
      * - :math:`p_Y(y)`
        - 0.60
        - 0.30
        - 0.10

   a. Calculate :math:`E[X]` and :math:`E[Y]`.

   b. Using additivity of expectation, find :math:`E[X + Y]`, the expected total defects per hour.

   c. Each defective item costs $50 to rework. Line A has a fixed hourly operating cost of $200, and Line B has a fixed cost of $150. Find the expected total hourly cost: :math:`E[200 + 50X + 150 + 50Y]`.

   d. Does your calculation in part (c) require knowing whether X and Y are independent? Explain.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[X] and E[Y]**

      .. math::

         E[X] = (0)(0.70) + (1)(0.20) + (2)(0.10) = 0 + 0.20 + 0.20 = 0.40

      .. math::

         E[Y] = (0)(0.60) + (1)(0.30) + (2)(0.10) = 0 + 0.30 + 0.20 = 0.50

      **Part (b): E[X + Y] using additivity**

      By additivity of expectation:

      .. math::

         E[X + Y] = E[X] + E[Y] = 0.40 + 0.50 = 0.90 \text{ defects per hour}

      **Part (c): Expected total hourly cost**

      Using linearity and additivity:

      .. math::

         E[200 + 50X + 150 + 50Y] &= 200 + 50E[X] + 150 + 50E[Y] \\
         &= 350 + 50(0.40) + 50(0.50) \\
         &= 350 + 20 + 25 = \$395

      The expected total hourly cost is **$395**.

      **Part (d): Independence not required**

      **No**, the calculation does not require knowing whether X and Y are independent.

      The additivity property :math:`E[X + Y] = E[X] + E[Y]` holds **regardless of whether X and Y are independent**. This is one of the most powerful aspects of expected valueâ€”we can compute expected values of sums using only the marginal distributions.

----

.. admonition:: Exercise 5: Insurance Premium Calculation
   :class: note

   An auto insurance company models claims using random variable :math:`N` = number of claims per year for a policyholder:

   .. flat-table:: PMF for Annual Claims
      :header-rows: 1
      :widths: 20 20 20 20 20

      * - :math:`n`
        - 0
        - 1
        - 2
        - 3
      * - :math:`p_N(n)`
        - 0.75
        - 0.15
        - 0.07
        - 0.03

   Each claim has a cost :math:`C` that is independent of :math:`N`, with:

   .. flat-table:: PMF for Claim Cost
      :header-rows: 1
      :widths: 25 25 25 25

      * - :math:`c`
        - $500
        - $2000
        - $5000
      * - :math:`p_C(c)`
        - 0.60
        - 0.30
        - 0.10

   a. Calculate :math:`E[N]`, the expected number of claims per year.

   b. Calculate :math:`E[C]`, the expected cost per claim.

   c. The total annual payout for a policyholder is :math:`N \cdot C` if we assume each claim costs the expected amount. Using the approximation Total Payout â‰ˆ :math:`N \cdot E[C]`, find :math:`E[N \cdot E[C]]`.

   d. The company adds a $100 administrative fee plus a 20% markup on expected payouts to set premiums. What annual premium should they charge?

   e. What profit does the company expect per policyholder?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[N]**

      .. math::

         E[N] = (0)(0.75) + (1)(0.15) + (2)(0.07) + (3)(0.03)

      .. math::

         = 0 + 0.15 + 0.14 + 0.09 = 0.38 \text{ claims/year}

      **Part (b): E[C]**

      .. math::

         E[C] = (500)(0.60) + (2000)(0.30) + (5000)(0.10)

      .. math::

         = 300 + 600 + 500 = \$1400 \text{ per claim}

      **Part (c): E[N Â· E[C]]**

      Since :math:`E[C] = 1400` is a constant:

      .. math::

         E[N \cdot E[C]] = E[N] \cdot E[C] = 0.38 \times 1400 = \$532

      The expected annual payout is **$532** per policyholder.

      **Part (d): Annual premium calculation**

      Premium = Administrative fee + (1 + markup) Ã— Expected payout

      .. math::

         \text{Premium} = 100 + 1.20 \times 532 = 100 + 638.40 = \$738.40

      The company should charge **$738.40** annually.

      **Part (e): Expected profit per policyholder**

      .. math::

         E[\text{Profit}] = \text{Premium} - E[\text{Payout}] - \text{Admin cost}

      .. math::

         = 738.40 - 532 - 100 = \$106.40

      Alternatively: The 20% markup on $532 is $106.40.

      Expected profit per policyholder is **$106.40** (representing the 20% margin).

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. The expected value of a discrete random variable must be one of the values in its support.

   â“‰ or â’»

2. For any function :math:`g`, :math:`E[g(X)] = g(E[X])`.

   â“‰ or â’»

3. :math:`E[X + Y] = E[X] + E[Y]` holds only when X and Y are independent.

   â“‰ or â’»

4. If :math:`E[X] = 5`, then :math:`E[3X - 2] = 13`.

   â“‰ or â’»

5. The expected value can be interpreted as the long-run average of repeated observations.

   â“‰ or â’»

6. If :math:`X` is a random variable with :math:`E[X] = 3`, then :math:`E[X^2] = 9`.

   â“‰ or â’»

**Multiple Choice Questions** (2 points each)

7. A random variable :math:`X` has :math:`E[X] = 4` and :math:`E[X^2] = 20`. What is :math:`E[X^2 - 2X + 1]`?

   â’¶ 13
   
   â’· 9
   
   â’¸ 17
   
   â’¹ 5

8. If :math:`X` has PMF :math:`p_X(1) = 0.5` and :math:`p_X(3) = 0.5`, what is :math:`E[X]`?

   â’¶ 1
   
   â’· 2
   
   â’¸ 3
   
   â’¹ 4

9. A game costs $5 to play. You win $20 with probability 0.2 and $0 otherwise. What is the expected net gain?

   â’¶ $4
   
   â’· -$1
   
   â’¸ $0
   
   â’¹ -$5

10. A random variable :math:`X` takes values 1, 2, and 3 with equal probability. What is :math:`E[X^2]`?

    â’¶ 2
    
    â’· 4
    
    â’¸ 14/3
    
    â’¹ 6

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **False** â€” The expected value is a weighted average and need not be in the support. Example: E[X] = 0.5 for a fair coin flip where X âˆˆ {0, 1}.

   2. **False** â€” This only holds for linear functions. In general, E[g(X)] â‰  g(E[X]). Example: E[XÂ²] â‰  (E[X])Â² unless X is constant.

   3. **False** â€” Additivity of expectation E[X + Y] = E[X] + E[Y] holds **regardless** of independence. This is one of the most useful properties of expected value.

   4. **True** â€” By linearity: E[3X - 2] = 3E[X] - 2 = 3(5) - 2 = 15 - 2 = 13.

   5. **True** â€” This is the frequentist interpretation of expected value. By the Law of Large Numbers, the sample mean converges to E[X] as the number of observations increases.

   6. **False** â€” In general, :math:`E[X^2] \neq (E[X])^2`. This is only true if X is a constant (has zero variance). For example, if X takes values 0 and 6 with equal probability, E[X] = 3 but E[XÂ²] = (0Â² + 6Â²)/2 = 18 â‰  9.

   **Multiple Choice Answers:**

   7. **â’¶** â€” Using linearity:
      
      E[XÂ² - 2X + 1] = E[XÂ²] - 2E[X] + 1 = 20 - 2(4) + 1 = 20 - 8 + 1 = 13.

   8. **â’·** â€” E[X] = (1)(0.5) + (3)(0.5) = 0.5 + 1.5 = 2.

   9. **â’·** â€” Expected winnings = (20)(0.2) + (0)(0.8) = 4. Net gain = 4 - 5 = -$1.
      
      This is an unfavorable game for the player.

   10. **â’¸** â€” E[XÂ²] = (1Â²)(1/3) + (2Â²)(1/3) + (3Â²)(1/3) = (1 + 4 + 9)/3 = 14/3 â‰ˆ 4.67.
       
       Note that E[X] = (1 + 2 + 3)/3 = 2, and (E[X])Â² = 4 â‰  14/3 = E[XÂ²], illustrating that E[XÂ²] â‰  (E[X])Â² in general.