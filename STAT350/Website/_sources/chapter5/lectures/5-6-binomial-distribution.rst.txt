.. _5-6-binomial-distribution:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch5-6">
     <iframe
       id="video-ch5-6"
       title="STAT 350 ‚Äì Chapter 5.6 The Binomial Distribution Video"
       src="https://www.youtube.com/embed/1WON80Ut7lc?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
       allowfullscreen>
     </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip
   
   `Download Chapter 5 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%205%20Discrete%20Distributions/L9-11-RandomVariables%20DiscreteProbabilityDistributions%28Chapter%205%29_AC.pptx>`_
   
The Binomial Distribution
=========================================

Certain patterns in probability occur so frequently that they deserve special attention. 
When we flip coins repeatedly, test multiple products for defects, or survey people about 
their preferences, we encounter the same underlying structure: a fixed number of independent 
trials, each with two possible outcomes. The binomial distribution captures this common scenario, 
providing us with ready-made formulas that eliminate the need to build probability models 
from scratch every time.

.. admonition:: Road Map üß≠
   :class: important

   ‚Ä¢ Identify when to use the **binomial distribution** through the BInS criteria.
   ‚Ä¢ Master the **binomial probability mass function** and its parameters.
   ‚Ä¢ Calculate **expected value and variance** using distribution properties.
   ‚Ä¢ Apply binomial models to **real-world counting problems**.
   ‚Ä¢ Connect binomial random variables to **sums of independent Bernoulli trials**.

Binomial Experiments
---------------------------------------------

The binomial distribution emerges naturally when we perform the same basic 
experiment multiple times under controlled conditions. Think about flipping a 
coin ten times and counting heads, or testing twenty manufactured items for defects. 
These scenarios share a common structure that statisticians have studied extensively.

The BInS Criteria
~~~~~~~~~~~~~~~~~~

A binomial experiment must satisfy four key properties, which we can remember 
using the acronym **BInS**:

.. flat-table:: 
   :header-rows: 2
   :widths: 8 18 60
   :align: center

   * - :cspan:`2` The BInS checklist

   * - Letter
     - Word
     - Meaning
   * - **B**
     - **Binary**
     - Each trial has exactly two possible outcomes (success & failure).
   * - **I**
     - **Independent**
     - Outcomes of different trials don‚Äôt influence one another.

   * - **n**
     - **Fixed \(n\) trials**
     - The number of trials is set in advance.
   * - **S**
     - **Same \(p\)**
     - The probability of success is identical for every trial.


.. admonition:: Examplesüí°: Identifying a Binomial Experiment throuhg BInS
   :class: note 

   For each given scenario, determine whether the experiment fits the BInS
   criteria.

   **Example 1: Rolling a Die**

   Suppose we roll a fair four-sided die five times and observe whether each outcome is a 1 or not.

   - **Binary**: Each roll either shows a 1 (success) or doesn't (failure). ‚úì
   - **Independent**: Each roll doesn't affect subsequent rolls. ‚úì  
   - **n-trials**: We perform exactly 5 rolls. ‚úì
   - **Success**: The probability of rolling a 1 remains 1/4 on every trial. ‚úì

   This satisfies all BInS criteria, so it's a binomial experiment.

   **Example 2: Drug Trial**

   Twenty patients with the same condition receive either a drug or placebo, 
   and we measure whether the treatment is effective.

   - **Binary**: This seems binary (effective/not effective), but there's more complexity
     since patients are given different types of of treatment. ü§î
   - **Independent**: Patient outcomes are independent. ‚úì
   - **n-trials**: We have 20 patients. ‚úì
   - **Success**: The success probabilities may or may not be the same for
     different patients, depending on the effectiveness of the drug. ‚ùå

   The experiment is not a binomial experiment.

   **Example 3: Quality Control**

   We randomly sample 15 products from different assembly lines in a factory
   and classify each as acceptable or not acceptable.

   - **Binary**: Each product is either acceptable or not. ‚úì
   - **Independent**: Random sampling makes outcomes independent. ‚úì
   - **n-trials**: We test exactly 15 products. ‚úì
   - **Success**: Different machines may have different success rates. ‚ùå

   This is not a binomial experiment.

   **Example 4: A different drug trial**

   A pharmaceutical company has developed a new medication for a common illness. 
   In a clinical trial, 10 randomly selected patients receive the new medication
   to see if they recover within a fixed amount of time.

   - **Binary**: Each patient either recovers or doesn't. ‚úì
   - **Independent**: One patient's recovery doesn't affect another's. ‚úì  
   - **n-trials**: We observe exactly 10 patients. ‚úì
   - **Success**: Recovery probability is equal for each patient since they are given
     the same medication. ‚úì

   This is a binomial experiment.

The Binomial Distribution
--------------------------------

When an experiment satisfies the BInS criteria, we can model **the number 
of successes** using a binomial distribution.

Definition
~~~~~~~~~~~~~~

A binomial random variable :math:`X` maps each outcome in a binomial experiment 
to the number of successes in :math:`n` trials. We write this as:

.. math::

   X \sim \text{Bin}(n, p)

where

* :math:`n` a positive integer representing the number of trials and
* :math:`p \in [0,1]` is the probability of success on each trial.

.. admonition:: Parameters
   :class: important

   The set of quantities that specify a complete PMF in a known family
   of distributions are called **parameters**. :math:`n` and :math:`p`
   are the parameters of the binomial distribution. 

Probability Mass Function
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The binomial PMF gives us the probability of exactly :math:`x` successes in :math:`n` trials:

.. math::

   p_X(x) = \binom{n}{x} p^x (1-p)^{n-x}

for :math:`x \in \text{supp}(X) = \{0, 1, 2, ..., n\}`.

This formula has three components:

1. **Combinations**: 
   
   :math:`\binom{n}{x} = \frac{n!}{x!(n-x)!}` counts the number of ways to arrange 
   :math:`x` successes among :math:`n` trials.

2. **Success probability**: 
   
   :math:`p^x` accounts for the probability of getting exactly :math:`x` successes.

3. **Failure probability**: 

   :math:`(1-p)^{n-x}` accounts for the probability of getting :math:`(n-x)` failures.

The beauty of this formula lies in its generality. Once we know :math:`n` and :math:`p`, 
we can calculate the probability for any number of successes without listing all possible outcomes.

.. admonition:: Validating the binomial PMF
   :class: important

   It is clear that for each :math:`x \in \text{supp}(X)`,
   :math:`p_X(x)` gives a non-negative value, since
   the three components which make up its formula are non-negative. 
   By verifying these terms sum to 1,
   we will also be able to argue that each :math:`p_X(x)` is at most 1.

   We compute the sum of all :math:`p_X(x)` terms using the binomial theorem:

   .. math::

      \sum_{x=0}^{n} \binom{n}{x} p^x (1-p)^{n-x} = (p + (1-p))^n = 1^n = 1.
   
   So, the binomial PMF satsifies both conditions for validity.

Expected Value and Variance: Building from Bernoulli Trials
-------------------------------------------------------------

Rather than deriving the expected value and variance from the PMF directly, 
we can use a clever approach that connects the binomial distribution to 
simpler building blocks.

Bernoulli Random Variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A Bernoulli random variable :math:`B` represents the **outcome of 
a single trial** in a binomial experiment:

.. math::

   B = \begin{cases}
   1 & \text{with probability } p \\
   0 & \text{with probability } 1-p
   \end{cases}

For this simple random variable:

.. math::

   E[B] &= 1 \cdot p + 0 \cdot (1-p) = p, \\
   E[B^2] &= 1^2 \cdot p + 0^2 \cdot (1-p) = p, \\
   \text{Var}(B) &= E[B^2] - (E[B])^2 = p - p^2 = p(1-p).

Connecting Bernoulli to Binomial
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A binomial random variable is simply the sum of :math:`n` independent Bernoulli trials:

.. math::

   X = B_1 + B_2 + \cdots + B_n

where each :math:`B_i` represents the :math:`i`-th trial.

Using the linearity of expectation:

.. math::

   E[X] = E[B_1 + B_2 + \cdots + B_n] = E[B_1] + E[B_2] + \cdots + E[B_n] = np

Since the trials are independent, we can also add their variances:

.. math::

   \text{Var}(X) = \text{Var}(B_1) + \text{Var}(B_2) + \cdots + \text{Var}(B_n) = np(1-p)

Summary of Binomial Distribution Properties
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For :math:`X \sim Bin(n, p)`:

.. math::

   &\mu_X = E[X] = np \\
   &\sigma_X^2 = \text{Var}(X) = np(1-p) \\
   &\sigma_X = \sqrt{np(1-p)}

These formulas align with our intuition:

* If we perform :math:`n` trials with 
  probability :math:`p` of success each time, we expect about :math:`np` successes on 
  average. 
* The variance reflects that we get maximum uncertainty when :math:`p = 0.5` 
  (each trial is equally likely to succeed or fail) and minimum uncertainty when 
  :math:`p` is close to 0 or 1.

Visualizing Binomial Distributions
-------------------------------------

The shape of a binomial distribution depends heavily on its parameters. 
Let's examine how changing :math:`p` affects the distribution when :math:`n = 10`:

**Low Success Probability** (:math:`p = 0.1`)

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/p0.1.png
   :alt: Binomial distribution with p=0.1
   :align: center
   :width: 70%

   :math:`p=0.1`

When success is rare, most probability concentrates near zero. 
We expect 0.1 successes on average, so getting 0 or 1 success is most likely, 
with higher counts becoming increasingly rare.

**Moderate Success Probability** (:math:`p = 0.25`)

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/p0.25.png
   :alt: Binomial distribution with p=0.25
   :align: center
   :width: 70%

   :math:`p=0.25`

As :math:`p` increases, the distribution shifts right but remains skewed. 
We expect 2.5 successes on average, with reasonable probability 
spread across several values.

**Equal Success and Failure** (:math:`p = 0.5`)

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/p0.5.png
   :alt: Binomial distribution with p=0.5
   :align: center
   :width: 70%
   
   :math:`p=0.5`

When success and failure are equally likely, the distribution becomes 
symmetric around its mean of 5. This creates a bell-shaped 
pattern.

**High Success Probability** (:math:`p = 0.9`)

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/p0.9.png
   :alt: Binomial distribution with p=0.9
   :align: center
   :width: 70%

   p=0.9
   
When success is nearly certain, most probability concentrates near the 
maximum. We expect 9.9 successes, so the event of getting 9 or 10 successes dominates 
the distribution.

.. admonition:: Exampleüí°: Evaluating a Medical Trial
   :class: note

   A pharmaceutical company has developed a new medication for a common illness. 
   Historical data shows that **30% of people with this illness recover naturally** 
   without treatment. In a clinical trial, 10 randomly selected patients 
   received the new medication, 9 of which recovered. Is this strong evidence that the medication is effective, 
   or could this result reasonably occur by chance alone?

   **Step 1: Confirm BInS Criteria**

   We confirmed that this is a binomial experiment through Case 4 of
   the first example in this section.

   **Step 2: Is the result realistic in nature?**

   If the number of recovered patients followed the natural recovery rate,
   we would have

   .. math:: X \sim Bin(10, 0.3).

   What is the probability that we see a comparative or better recovery
   rate in nature than the current experiment? In other words, what is 
   :math:`P(X\geq 9)`?

   .. math:: 

      P(X \geq 9) &= P(X=9) + P(X=10) \\
      &= {10 \choose 9}(0.3)^9(0.7)^1 + {10 \choose 10}(0.3)^10(0.7)^0 \\
      &\approx 0.000138 + 0.000006 = 0.000144

   **Step 3: Interpreting the results**

   The probability of seeing 9 or more recoveries by chance alone is only 
   about 0.000144, or roughly 1 in 7,000. This extremely small probability 
   suggests that observing 9 recoveries is highly unlikely if the medication 
   had no effect.

   This analysis provides strong statistical evidence that the medication 
   is effective, though we'd want to see results from larger, more carefully 
   controlled studies before drawing definitive conclusions.

Bringin It All Together
----------------------------------------------


.. admonition:: Key Takeaways üìù
   :class: important

   1. The **binomial distribution** models the number of successes in a set of
      independent and identical trials.
   
   2. Use the **BInS criteria** to identify binomial situations: 
      Binary outcomes, Independent trials, n:math:`n` fixed trials, and constant 
      Success probability.
   
   3. The **binomial PMF** :math:`p_X(x) = \binom{n}{x} p^x (1-p)^{n-x}` 
      combines combinatorics with basic probability principles.
   
   4. Binomial random variables have simple **rxpected value and variance** 
      formulas: :math:`E[X] = np` 
      and :math:`Var(X) = np(1-p)`, derived by viewing binomial variables as 
      sums of Bernoulli trials.
   
   5. Binomial distributions are **symmetric when p = 0.5** and increasingly 
      skewed as p approaches 0 or 1.

..
   Working with Binomial Distributions in R
   -----------------------------------------

   R provides powerful built-in functions for working with binomial distributions, making calculations much easier than computing combinations and powers by hand. Let's explore the key functions and see how they work.

   **The Four Essential R Functions**

   R follows a consistent naming pattern for probability distributions. For binomial distributions:

   - **rbinom()**: Generates random samples from a binomial distribution
   - **dbinom()**: Calculates the probability mass function (exact probabilities)
   - **pbinom()**: Calculates the cumulative distribution function (cumulative probabilities)
   - **qbinom()**: Finds quantiles (the inverse of pbinom)

   **Generating Random Samples with rbinom()**

   The `rbinom()` function simulates drawing random values from a binomial distribution:

   .. code-block:: r

      # Generate 10 random values from Bin(20, 0.3)
      # Each value represents number of successes in 20 trials
      set.seed(123)  # For reproducible results
      random_values <- rbinom(n = 10, size = 20, prob = 0.3)
      print(random_values)
      # Output: 8 4 8 4 2 7 7 6 4 6

      # Generate 1000 samples to see the distribution pattern
      large_sample <- rbinom(n = 1000, size = 20, prob = 0.3)
      
      # Check that sample mean approximates theoretical mean
      sample_mean <- mean(large_sample)
      theoretical_mean <- 20 * 0.3  # np = 6
      print(paste("Sample mean:", round(sample_mean, 2)))
      print(paste("Theoretical mean:", theoretical_mean))

   The `rbinom()` function is particularly useful for simulation studies, where we want to understand how binomial processes behave over many repetitions.

   **Calculating Exact Probabilities with dbinom()**

   The `dbinom()` function computes P(X = x) for specific values:

   .. code-block:: r

      # Calculate P(X = 5) when X ~ Bin(20, 0.3)
      prob_exactly_5 <- dbinom(x = 5, size = 20, prob = 0.3)
      print(paste("P(X = 5) =", round(prob_exactly_5, 4)))

      # Calculate probabilities for multiple values at once
      x_values <- 0:10
      probabilities <- dbinom(x = x_values, size = 20, prob = 0.3)
      
      # Create a probability table
      prob_table <- data.frame(
      x = x_values,
      probability = round(probabilities, 4)
      )
      print(prob_table)

      # Verify probabilities sum to 1 (approximately)
      total_prob <- sum(dbinom(x = 0:20, size = 20, prob = 0.3))
      print(paste("Total probability:", round(total_prob, 6)))

   **Calculating Cumulative Probabilities with pbinom()**

   The `pbinom()` function computes P(X ‚â§ x), which is often more useful than exact probabilities:

   .. code-block:: r

      # Calculate P(X ‚â§ 8) when X ~ Bin(20, 0.3)
      prob_at_most_8 <- pbinom(q = 8, size = 20, prob = 0.3)
      print(paste("P(X ‚â§ 8) =", round(prob_at_most_8, 4)))

      # Calculate P(X ‚â• 9) = 1 - P(X ‚â§ 8)
      prob_at_least_9 <- 1 - pbinom(q = 8, size = 20, prob = 0.3)
      # Alternative: use lower.tail = FALSE
      prob_at_least_9_alt <- pbinom(q = 8, size = 20, prob = 0.3, lower.tail = FALSE)
      print(paste("P(X ‚â• 9) =", round(prob_at_least_9, 4)))

      # Calculate P(5 ‚â§ X ‚â§ 10) = P(X ‚â§ 10) - P(X ‚â§ 4)
      prob_between <- pbinom(q = 10, size = 20, prob = 0.3) - 
                     pbinom(q = 4, size = 20, prob = 0.3)
      print(paste("P(5 ‚â§ X ‚â§ 10) =", round(prob_between, 4)))

   **Calculating the Probability with R**
   
   We want P(X ‚â• 9), the probability of observing 9 or more recoveries by chance alone.

   .. code-block:: r

      # Method 1: Using dbinom() for exact calculation
      prob_exactly_9 <- dbinom(x = 9, size = 10, prob = 0.3)
      prob_exactly_10 <- dbinom(x = 10, size = 10, prob = 0.3)
      prob_at_least_9 <- prob_exactly_9 + prob_exactly_10
      
      print(paste("P(X = 9) =", round(prob_exactly_9, 6)))
      print(paste("P(X = 10) =", round(prob_exactly_10, 6)))
      print(paste("P(X ‚â• 9) =", round(prob_at_least_9, 6)))
      
      # Method 2: Using pbinom() with lower.tail = FALSE
      prob_at_least_9_alt <- pbinom(q = 8, size = 10, prob = 0.3, lower.tail = FALSE)
      print(paste("P(X ‚â• 9) using pbinom =", round(prob_at_least_9_alt, 6)))
      
      # Method 3: Manual calculation to verify
      manual_calc <- choose(10,9) * (0.3^9) * (0.7^1) + choose(10,10) * (0.3^10) * (0.7^0)
      print(paste("Manual calculation =", round(manual_calc, 6)))

   **Understanding the Context with R**
   For comparison, let's examine the expected pattern without treatment:

   .. code-block:: r

      # Calculate expected value and standard deviation
      n <- 10
      p <- 0.3
      expected_recoveries <- n * p
      variance_recoveries <- n * p * (1 - p)
      sd_recoveries <- sqrt(variance_recoveries)
      
      print(paste("Expected recoveries:", expected_recoveries))
      print(paste("Standard deviation:", round(sd_recoveries, 3)))
      print(paste("Variance:", round(variance_recoveries, 3)))
      
      # Calculate how many standard deviations 9 is from the mean
      z_score <- (9 - expected_recoveries) / sd_recoveries
      print(paste("9 recoveries is", round(z_score, 2), "standard deviations above the mean"))
      
      # Generate simulation to show rarity of extreme results
      set.seed(456)
      simulated_results <- rbinom(n = 10000, size = 10, prob = 0.3)
      extreme_results <- sum(simulated_results >= 9)
      print(paste("In 10,000 simulations,", extreme_results, "had 9+ recoveries"))
      print(paste("Simulation probability:", round(extreme_results/10000, 6)))

Exercises
---------

These exercises develop your skills in identifying binomial experiments using the BInS criteria, calculating binomial probabilities, and applying expected value and variance formulas.

.. admonition:: Exercise 1: Identifying Binomial Experiments (BInS Criteria)
   :class: note

   For each scenario, determine whether it represents a binomial experiment. If yes, identify :math:`n` and :math:`p`. If no, explain which BInS criterion is violated.

   a. A software company deploys code to 50 servers. Each server independently has a 2% chance of experiencing a deployment error. Let :math:`X` = number of servers with errors.

   b. A network engineer monitors packets until 10 packets are lost. Let :math:`X` = number of packets transmitted before the 10th loss.

   c. A data scientist runs 20 independent A/B tests. Each test has an 80% chance of detecting a true effect when one exists. Let :math:`X` = number of tests that detect the effect.

   d. Cards are drawn one at a time from a standard 52-card deck **without replacement** until 5 cards are drawn. Let :math:`X` = number of hearts drawn.

   e. A quality engineer inspects 30 circuit boards from a production batch. Each board independently has a 3% defect rate. Let :math:`X` = number of defective boards.

   f. A basketball player takes free throws until she misses. Her free-throw percentage is 85%. Let :math:`X` = number of shots taken.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Server deployment ‚Äî YES, Binomial**

      - **B**\inary: Each server either has an error or doesn't ‚úì
      - **I**\ndependent: Servers fail independently ‚úì
      - **n** fixed: 50 servers ‚úì
      - **S**\ame p: Each server has 2% error rate ‚úì

      :math:`X \sim \text{Bin}(n=50, p=0.02)`

      **Part (b): Packets until 10 losses ‚Äî NO, not Binomial**

      - **n** fixed: ‚ùå The number of trials is **not fixed** ‚Äî we continue until we observe 10 losses.

      This is actually a **negative binomial** experiment (covered in more advanced courses).

      **Part (c): A/B tests ‚Äî YES, Binomial**

      - **B**\inary: Each test either detects the effect or doesn't ‚úì
      - **I**\ndependent: Tests are run independently ‚úì
      - **n** fixed: 20 tests ‚úì
      - **S**\ame p: Each test has 80% detection rate ‚úì

      :math:`X \sim \text{Bin}(n=20, p=0.80)`

      **Part (d): Drawing cards without replacement ‚Äî NO, not Binomial**

      - **I**ndependent: ‚ùå Drawing without replacement means trials are **dependent**. After drawing a heart, the probability of drawing another heart changes.

      With replacement, this would be binomial. Without replacement, the exact distribution is **hypergeometric**.

      **Part (e): Circuit board inspection ‚Äî YES, Binomial**

      - **B**\inary: Each board is either defective or not ‚úì
      - **I**\ndependent: Board defects are independent ‚úì
      - **n** fixed: 30 boards ‚úì
      - **S**\ame p: Each board has 3% defect rate ‚úì

      :math:`X \sim \text{Bin}(n=30, p=0.03)`

      **Part (f): Free throws until miss ‚Äî NO, not Binomial**

      - **n** fixed: ‚ùå The number of trials is **not fixed** ‚Äî we continue until a miss occurs.

      This is a **geometric** experiment (Chapter 5.7 topic).

----

.. admonition:: Exercise 2: Basic Binomial Probability Calculations
   :class: note

   A cybersecurity firm finds that 15% of phishing emails successfully trick users into clicking malicious links. In a simulated attack, 12 phishing emails are sent to employees.

   Let :math:`X` = number of employees who click the malicious link.

   a. Verify that :math:`X` follows a binomial distribution. State the parameters.

   b. Calculate :math:`P(X = 2)`, the probability that exactly 2 employees click.

   c. Calculate :math:`P(X \leq 1)`, the probability that at most 1 employee clicks.

   d. Calculate :math:`P(X \geq 3)`, the probability that 3 or more employees click.

   e. Find :math:`E[X]` and :math:`\sigma_X`.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Verify binomial and state parameters**

      - **B**\inary: Each employee either clicks or doesn't ‚úì
      - **I**\ndependent: Employee decisions are independent ‚úì
      - **n** fixed: 12 emails sent ‚úì
      - **S**\ame p: Each has 15% click rate ‚úì

      :math:`X \sim \text{Bin}(n=12, p=0.15)`

      **Part (b): P(X = 2)**

      .. math::

         P(X = 2) = \binom{12}{2}(0.15)^2(0.85)^{10}

      .. math::

         = \frac{12!}{2! \cdot 10!} \times 0.0225 \times 0.1969

      .. math::

         = 66 \times 0.0225 \times 0.1969 = 0.2924

      **Part (c): P(X ‚â§ 1)**

      .. math::

         P(X \leq 1) = P(X = 0) + P(X = 1)

      .. math::

         P(X = 0) = \binom{12}{0}(0.15)^0(0.85)^{12} = 1 \times 1 \times 0.1422 = 0.1422

      .. math::

         P(X = 1) = \binom{12}{1}(0.15)^1(0.85)^{11} = 12 \times 0.15 \times 0.1673 = 0.3012

      .. math::

         P(X \leq 1) = 0.1422 + 0.3012 = 0.4434

      **Part (d): P(X ‚â• 3)**

      Using the complement:

      .. math::

         P(X \geq 3) = 1 - P(X \leq 2) = 1 - [P(X=0) + P(X=1) + P(X=2)]

      .. math::

         = 1 - [0.1422 + 0.3012 + 0.2924] = 1 - 0.7358 = 0.2642

      **Part (e): E[X] and œÉ_X**

      .. math::

         E[X] = np = 12 \times 0.15 = 1.8 \text{ employees}

      .. math::

         \text{Var}(X) = np(1-p) = 12 \times 0.15 \times 0.85 = 1.53

      .. math::

         \sigma_X = \sqrt{1.53} \approx 1.237

----

.. admonition:: Exercise 3: Bernoulli to Binomial Connection
   :class: note

   A sensor system consists of 5 independent sensors. Each sensor has a 90% probability of correctly detecting an event.

   Let :math:`B_i` be the Bernoulli random variable for sensor :math:`i`:

   .. math::

      B_i = \begin{cases} 1 & \text{if sensor } i \text{ detects the event} \\ 0 & \text{otherwise} \end{cases}

   Let :math:`X = B_1 + B_2 + B_3 + B_4 + B_5` = total number of sensors that detect the event.

   a. Find :math:`E[B_i]` and :math:`\text{Var}(B_i)` for a single sensor.

   b. Use the properties of sums to find :math:`E[X]`.

   c. Use the independence of sensors to find :math:`\text{Var}(X)`.

   d. Verify your answers using the binomial formulas :math:`E[X] = np` and :math:`\text{Var}(X) = np(1-p)`.

   e. The system triggers an alarm if **at least 3** sensors detect the event. Find :math:`P(X \geq 3)`.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Single sensor (Bernoulli)**

      .. math::

         E[B_i] = 1 \cdot p + 0 \cdot (1-p) = p = 0.90

      .. math::

         E[B_i^2] = 1^2 \cdot p + 0^2 \cdot (1-p) = p = 0.90

      .. math::

         \text{Var}(B_i) = E[B_i^2] - (E[B_i])^2 = 0.90 - 0.81 = 0.09

      Alternatively: :math:`\text{Var}(B_i) = p(1-p) = 0.90 \times 0.10 = 0.09`

      **Part (b): E[X] using linearity**

      .. math::

         E[X] = E[B_1 + B_2 + B_3 + B_4 + B_5] = E[B_1] + E[B_2] + \cdots + E[B_5]

      .. math::

         = 5 \times 0.90 = 4.5

      **Part (c): Var(X) using independence**

      Since the sensors are independent:

      .. math::

         \text{Var}(X) = \text{Var}(B_1) + \text{Var}(B_2) + \cdots + \text{Var}(B_5)

      .. math::

         = 5 \times 0.09 = 0.45

      **Part (d): Verify with binomial formulas**

      :math:`X \sim \text{Bin}(n=5, p=0.90)`

      .. math::

         E[X] = np = 5 \times 0.90 = 4.5 \quad \checkmark

      .. math::

         \text{Var}(X) = np(1-p) = 5 \times 0.90 \times 0.10 = 0.45 \quad \checkmark

      **Part (e): P(X ‚â• 3)**

      .. math::

         P(X \geq 3) = P(X=3) + P(X=4) + P(X=5)

      .. math::

         P(X=3) = \binom{5}{3}(0.90)^3(0.10)^2 = 10 \times 0.729 \times 0.01 = 0.0729

      .. math::

         P(X=4) = \binom{5}{4}(0.90)^4(0.10)^1 = 5 \times 0.6561 \times 0.10 = 0.3281

      .. math::

         P(X=5) = \binom{5}{5}(0.90)^5(0.10)^0 = 1 \times 0.5905 \times 1 = 0.5905

      .. math::

         P(X \geq 3) = 0.0729 + 0.3281 + 0.5905 = 0.9915

      There is a **99.15%** probability the alarm will trigger.

----

.. admonition:: Exercise 4: Quality Control Application
   :class: note

   A semiconductor manufacturer produces microchips with a 4% defect rate. A batch of 20 chips is randomly selected for testing.

   a. What is the probability that exactly 1 chip is defective?

   b. What is the probability that no chips are defective?

   c. What is the probability that more than 2 chips are defective?

   d. The batch is rejected if 3 or more chips are defective. What is the probability of rejection?

   e. If the defect rate improved to 2%, how would the rejection probability change?

   .. dropdown:: Solution
      :class-container: sd-border-success

      Let :math:`X \sim \text{Bin}(n=20, p=0.04)`.

      **Part (a): P(X = 1)**

      .. math::

         P(X = 1) = \binom{20}{1}(0.04)^1(0.96)^{19} = 20 \times 0.04 \times 0.4604 = 0.3683

      **Part (b): P(X = 0)**

      .. math::

         P(X = 0) = \binom{20}{0}(0.04)^0(0.96)^{20} = 1 \times 1 \times 0.4420 = 0.4420

      **Part (c): P(X > 2)**

      .. math::

         P(X > 2) = 1 - P(X \leq 2) = 1 - [P(X=0) + P(X=1) + P(X=2)]

      .. math::

         P(X = 2) = \binom{20}{2}(0.04)^2(0.96)^{18} = 190 \times 0.0016 \times 0.4796 = 0.1458

      .. math::

         P(X > 2) = 1 - [0.4420 + 0.3683 + 0.1458] = 1 - 0.9561 = 0.0439

      **Part (d): P(X ‚â• 3) ‚Äî rejection probability**

      .. math::

         P(X \geq 3) = P(X > 2) = 0.0439

      There is about a **4.4%** chance the batch will be rejected.

      **Part (e): Improved defect rate (p = 0.02)**

      Let :math:`Y \sim \text{Bin}(n=20, p=0.02)`.

      .. math::

         P(Y = 0) = (0.98)^{20} = 0.6676

      .. math::

         P(Y = 1) = \binom{20}{1}(0.02)^1(0.98)^{19} = 20 \times 0.02 \times 0.6812 = 0.2725

      .. math::

         P(Y = 2) = \binom{20}{2}(0.02)^2(0.98)^{18} = 190 \times 0.0004 \times 0.6951 = 0.0528

      .. math::

         P(Y \geq 3) = 1 - [0.6676 + 0.2725 + 0.0528] = 1 - 0.9929 = 0.0071

      The rejection probability drops from **4.4%** to **0.71%** ‚Äî about 6 times lower!

----

.. admonition:: Exercise 5: Software Testing Application
   :class: note

   A software team runs automated tests on each code commit. Historically, 8% of commits introduce a bug that the tests detect.

   Over one week, there are 25 commits.

   a. What is the expected number of commits that introduce detected bugs?

   b. What is the standard deviation?

   c. What is the probability that exactly 2 commits introduce bugs?

   d. The team lead gets concerned if 5 or more commits introduce bugs in a week. What is the probability of this happening?

   e. In R, you would calculate part (d) as: `1 - pbinom(4, size = 25, prob = 0.08)`. Explain why we use 4 instead of 5.

   .. dropdown:: Solution
      :class-container: sd-border-success

      Let :math:`X \sim \text{Bin}(n=25, p=0.08)`.

      **Part (a): Expected value**

      .. math::

         E[X] = np = 25 \times 0.08 = 2 \text{ commits}

      **Part (b): Standard deviation**

      .. math::

         \text{Var}(X) = np(1-p) = 25 \times 0.08 \times 0.92 = 1.84

      .. math::

         \sigma_X = \sqrt{1.84} \approx 1.356

      **Part (c): P(X = 2)**

      .. math::

         P(X = 2) = \binom{25}{2}(0.08)^2(0.92)^{23}

      .. math::

         = 300 \times 0.0064 \times 0.1470 = 0.2821

      **Part (d): P(X ‚â• 5)**

      Using complement: :math:`P(X \geq 5) = 1 - P(X \leq 4)`

      .. math::

         P(X = 0) = (0.92)^{25} = 0.1244

      .. math::

         P(X = 1) = \binom{25}{1}(0.08)^1(0.92)^{24} = 25 \times 0.08 \times 0.1352 = 0.2704

      .. math::

         P(X = 2) = 0.2821

      .. math::

         P(X = 3) = \binom{25}{3}(0.08)^3(0.92)^{22} = 0.1881

      .. math::

         P(X = 4) = \binom{25}{4}(0.08)^4(0.92)^{21} = 0.0899

      .. math::

         P(X \leq 4) = 0.1244 + 0.2704 + 0.2821 + 0.1881 + 0.0899 = 0.9549

      .. math::

         P(X \geq 5) = 1 - 0.9549 = 0.0451

      There is about a **4.5%** chance of 5 or more buggy commits.

      **Part (e): Why use 4 in R?**

      The R function `pbinom(q, size, prob)` calculates :math:`P(X \leq q)`.

      To get :math:`P(X \geq 5)`, we need :math:`1 - P(X \leq 4)`.

      - `pbinom(4, 25, 0.08)` gives :math:`P(X \leq 4)`
      - `1 - pbinom(4, 25, 0.08)` gives :math:`P(X \geq 5)`

      Using `pbinom(5, ...)` would give :math:`P(X \leq 5)`, which includes :math:`P(X = 5)` ‚Äî not what we want!

      Alternatively: `pbinom(4, 25, 0.08, lower.tail = FALSE)` directly gives :math:`P(X > 4) = P(X \geq 5)`.

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. If :math:`X \sim \text{Bin}(n, p)`, then :math:`E[X] = np(1-p)`.

   ‚ìâ or ‚íª

2. A binomial random variable counts the number of trials until the first success.

   ‚ìâ or ‚íª

3. For a binomial distribution, the trials must be independent.

   ‚ìâ or ‚íª

4. If :math:`X \sim \text{Bin}(10, 0.5)`, then the distribution of X is symmetric.

   ‚ìâ or ‚íª

5. Sampling without replacement from a large population can be approximated as binomial.

   ‚ìâ or ‚íª

6. :math:`\text{Var}(X)` is maximized when :math:`p = 0.5` for a fixed :math:`n`.

   ‚ìâ or ‚íª

**Multiple Choice Questions** (2 points each)

7. If :math:`X \sim \text{Bin}(15, 0.20)`, what is :math:`E[X]`?

   ‚í∂ 0.20
   
   ‚í∑ 2.4
   
   ‚í∏ 3.0
   
   ‚íπ 12.0

8. For :math:`X \sim \text{Bin}(20, 0.3)`, what is :math:`\text{Var}(X)`?

   ‚í∂ 4.2
   
   ‚í∑ 6.0
   
   ‚í∏ 14.0
   
   ‚íπ 18.0

9. Which of the following is NOT required for a binomial experiment?

   ‚í∂ Fixed number of trials
   
   ‚í∑ Independent trials
   
   ‚í∏ Normally distributed outcomes
   
   ‚íπ Constant probability of success

10. A fair coin is flipped 6 times. What is :math:`P(X = 3)` where X is the number of heads?

    ‚í∂ 0.1563
    
    ‚í∑ 0.3125
    
    ‚í∏ 0.5000
    
    ‚íπ 0.6563

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **False** ‚Äî :math:`E[X] = np`. The formula :math:`np(1-p)` is for variance, not expected value.

   2. **False** ‚Äî Binomial counts the number of **successes in n trials**. Counting trials until first success is the **geometric** distribution.

   3. **True** ‚Äî Independence is the "I" in the BInS criteria and is essential for binomial.

   4. **True** ‚Äî When :math:`p = 0.5`, the binomial distribution is symmetric around its mean :math:`np = 5`.

   5. **True** ‚Äî When the population is much larger than the sample (rule of thumb: population > 10√ó sample), the dependence from sampling without replacement is negligible.

   6. **True** ‚Äî :math:`\text{Var}(X) = np(1-p)`. For fixed n, the product :math:`p(1-p)` is maximized at :math:`p = 0.5` where :math:`p(1-p) = 0.25`.

   **Multiple Choice Answers:**

   7. **‚í∏** ‚Äî :math:`E[X] = np = 15 \times 0.20 = 3.0`.

   8. **‚í∂** ‚Äî :math:`\text{Var}(X) = np(1-p) = 20 \times 0.3 \times 0.7 = 4.2`.

   9. **‚í∏** ‚Äî Binomial outcomes are discrete (0, 1, 2, ..., n), not normally distributed. The other three are the BInS criteria (B is implied by having a success probability).

   10. **‚í∑** ‚Äî :math:`P(X=3) = \binom{6}{3}(0.5)^3(0.5)^3 = 20 \times 0.125 \times 0.125 = 20 \times 0.015625 = 0.3125`.