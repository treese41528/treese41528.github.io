.. _5-6-binomial-distribution:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch5-6">
     <iframe
       id="video-ch5-6"
       title="STAT 350 ‚Äì Chapter 5.6 The Binomial Distribution Video"
       src="https://www.youtube.com/embed/1WON80Ut7lc?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
       allowfullscreen>
     </iframe>
   </div>

The Binomial Distribution
=========================================

Certain patterns in probability occur so frequently that they deserve special attention. 
When we flip coins repeatedly, test multiple products for defects, or survey people about 
their preferences, we encounter the same underlying structure: a fixed number of independent 
trials, each with two possible outcomes. The binomial distribution captures this common scenario, 
providing us with ready-made formulas that eliminate the need to build probability models 
from scratch every time.

.. admonition:: Road Map üß≠
   :class: important

   ‚Ä¢ Identify when to use the **binomial distribution** through the BInS criteria.
   ‚Ä¢ Master the **binomial probability mass function** and its parameters.
   ‚Ä¢ Calculate **expected value and variance** using distribution properties.
   ‚Ä¢ Apply binomial models to **real-world counting problems**.
   ‚Ä¢ Connect binomial random variables to **sums of independent Bernoulli trials**.

Binomial Experiments
---------------------------------------------

The binomial distribution emerges naturally when we perform the same basic 
experiment multiple times under controlled conditions. Think about flipping a 
coin ten times and counting heads, or testing twenty manufactured items for defects. 
These scenarios share a common structure that statisticians have studied extensively.

The BInS Criteria
~~~~~~~~~~~~~~~~~~

A binomial experiment must satisfy four key properties, which we can remember 
using the acronym **BInS**:

.. flat-table:: 
   :header-rows: 2
   :widths: 8 18 60
   :align: center

   * - :cspan:`2` The BInS checklist

   * - Letter
     - Word
     - Meaning
   * - **B**
     - **Binary**
     - Each trial has exactly two possible outcomes (success & failure).
   * - **I**
     - **Independent**
     - Outcomes of different trials don‚Äôt influence one another.

   * - **n**
     - **Fixed \(n\) trials**
     - The number of trials is set in advance.
   * - **S**
     - **Same \(p\)**
     - The probability of success is identical for every trial.


.. admonition:: Examplesüí°: Identifying a Binomial Experiment throuhg BInS
   :class: note 

   For each given scenario, determine whether the experiment fits the BInS
   criteria.

   **Example 1: Rolling a Die**

   Suppose we roll a fair four-sided die five times and observe whether each outcome is a 1 or not.

   - **Binary**: Each roll either shows a 1 (success) or doesn't (failure). ‚úì
   - **Independent**: Each roll doesn't affect subsequent rolls. ‚úì  
   - **n-trials**: We perform exactly 5 rolls. ‚úì
   - **Success**: The probability of rolling a 1 remains 1/4 on every trial. ‚úì

   This satisfies all BInS criteria, so it's a binomial experiment.

   **Example 2: Drug Trial**

   Twenty patients with the same condition receive either a drug or placebo, 
   and we measure whether the treatment is effective.

   - **Binary**: This seems binary (effective/not effective), but there's more complexity
     since patients are given different types of of treatment.
   - **Independent**: Patient outcomes are independent. ‚úì
   - **n-trials**: We have 20 patients ‚úì
   - **Success**: The success probabilities may ore may not be the same for
     different patients, depending on the effectiveness of the drug. ‚ùå

   The experiment is not a binomial experiment.

   **Example 3: Quality Control**

   We randomly sample 15 products from an assembly line using different 
   machines and classify each as acceptable or not acceptable.

   - **Binary**: Each product is either acceptable or not. ‚úì
   - **Independent**: Random sampling makes outcomes independent. ‚úì
   - **n-trials**: We test exactly 15 products. ‚úì
   - **Success**: Different machines may have different success rates ‚ùå

   This is not a binomial experiment.

   **Example 4: A different drug trial**

   A pharmaceutical company has developed a new medication for a common illness. 
   In a clinical trial, 10 randomly selected patients receive the new medication
   to see if they recover within a fixed amount of time.

   - **Binary**: Each patient either recovers or doesn't. ‚úì
   - **Independent**: One patient's recovery doesn't affect another's. ‚úì  
   - **n-trials**: We observe exactly 10 patients. ‚úì
   - **Success**: Recovery probability remains 0.3 for each patient. ‚úì

   This is a binomial experiment.

The Binomial Distribution
--------------------------------

When an experiment satisfies the BInS criteria, we can model the number 
of successes using a binomial distribution.

Definition
~~~~~~~~~~~~~~

A binomial random variable :math:`X` maps each outcome in a binomial experiment 
to the number of successes in :math:`n` trials. We write this as:

.. math::

   X \sim \text{Bin}(n, p)

where

* :math:`n` a positive integer representing the number of trials and
* :math:`p \in [0,1]` is the probability of success on each trial.

.. admonition:: Parameters
   :class: important

   The set of quantities that specify a complete PMF in a known family
   of distributions are called **parameters**. :math:`n` and :math:`p`
   are the parameters of binomial distribution. 

Probability Mass Function
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The binomial PMF gives us the probability of exactly :math:`x` successes in :math:`n` trials:

.. math::

   p_X(x) = \binom{n}{x} p^x (1-p)^{n-x}

for :math:`x \in \text{supp}(X) = \{0, 1, 2, ..., n\}`.

This formula has three components:

1. **Combinations**: 
   
   :math:`\binom{n}{x} = \frac{n!}{x!(n-x)!}` counts the number of ways to arrange 
   :math:`x` successes among :math:`n` trials.

2. **Success probability**: 
   
   :math:`p^x` accounts for the probability of getting exactly :math:`x` successes.

3. **Failure probability**: 

   :math:`(1-p)^{n-x}` accounts for the probability of getting :math:`(n-x)` failures.

The beauty of this formula lies in its generality. Once we know :math:`n` and :math:`p`, 
we can calculate the probability for any number of successes without listing all possible outcomes.

.. admonition:: Validating the binomial PMF
   :class: important

   It is clear that for each :math:`x \in \text{supp}(X)`,
   :math:`p_X(x)` gives a non-negative value, since
   the three components which make up its formula are non-negative. 
   By verifying these terms sum to 1,
   we will also be able to argue that each :math:`p_X(x)` is at most 1.

   We compute the sum of all :math:`p_X(x)` terms using the binomial theorem:

   .. math::

      \sum_{x=0}^{n} \binom{n}{x} p^x (1-p)^{n-x} = (p + (1-p))^n = 1^n = 1.
   
   So, the binomial PMF satsifies both conditions for validity.

Expected Value and Variance: Building from Bernoulli Trials
-------------------------------------------------------------

Rather than deriving the expected value and variance from the PMF directly, 
we can use a clever approach that connects the binomial distribution to 
simpler building blocks.

Bernoulli Random Variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A Bernoulli random variable :math:`B` represents the **outcome of 
a single trial** in a binomial experiment:

.. math::

   B = \begin{cases}
   1 & \text{with probability } p \\
   0 & \text{with probability } 1-p
   \end{cases}

For this simple random variable:

.. math::

   E[B] &= 1 \cdot p + 0 \cdot (1-p) = p, \\
   E[B^2] &= 1^2 \cdot p + 0^2 \cdot (1-p) = p, \\
   \text{Var}(B) &= E[B^2] - (E[B])^2 = p - p^2 = p(1-p).

Connecting Bernoulli to Binomial
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A binomial random variable is simply the sum of :math:`n` independent Bernoulli trials:

.. math::

   X = B_1 + B_2 + \cdots + B_n

where each :math:`B_i` represents the :math:`i`-th trial.

Using the linearity of expectation:

.. math::

   E[X] = E[B_1 + B_2 + \cdots + B_n] = E[B_1] + E[B_2] + \cdots + E[B_n] = np

Since the trials are independent, we can add their variances:

.. math::

   \text{Var}(X) = \text{Var}(B_1) + \text{Var}(B_2) + \cdots + \text{Var}(B_n) = np(1-p)

Summary of Binomial Distribution Properties
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For :math:`X \sim Bin(n, p)`:

.. math::

   &\mu_X = E[X] = np \\
   &\sigma_X^2 = \text{Var}(X) = np(1-p) \\
   &\sigma_X = \sqrt{np(1-p)}

These formulas make intuitive sense. 

* If we perform :math:`n` trials with 
  probability :math:`p` of success each time, we expect about :math:`np` successes on 
  average. 
* The variance reflects that we get maximum uncertainty when :math:`p = 0.5` 
  (each trial is equally likely to succeed or fail) and minimum uncertainty when 
  :math:`p` is close to 0 or 1.

Visualizing Binomial Distributions
-------------------------------------

The shape of a binomial distribution depends heavily on its parameters. 
Let's examine how changing :math:`p` affects the distribution when :math:`n = 10`:

**Low Success Probability** (:math:`p = 0.1`)

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/p0.1.png
   :alt: Binomial distribution with p=0.1
   :align: center
   :width: 70%

   p=0.1

When success is rare, most probability concentrates near zero. 
We expect 0.1 successes on average, so getting 0 or 1 success is most likely, 
with higher counts becoming increasingly rare.

**Moderate Success Probability*** (:math:`p = 0.25`)

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/p0.25.png
   :alt: Binomial distribution with p=0.25
   :align: center
   :width: 70%

   p=0.25

As p increases, the distribution shifts right but remains skewed. 
We expect 2.5 successes on average, with reasonable probability 
spread across several values.

**Equal Success and Failure** (:math:`p = 0.5`)

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/p0.5.png
   :alt: Binomial distribution with p=0.5
   :align: center
   :width: 70%
   
   p=0.5

When success and failure are equally likely, the distribution becomes 
symmetric around its mean of 5. This creates the classic bell-shaped 
pattern associated with binomial distributions.

**High Success Probability** (:math:`p = 0.9`)

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter5/p0.9.png
   :alt: Binomial distribution with p=0.9
   :align: center
   :width: 70%

   p=0.9
   
When success is nearly certain, most probability concentrates near the 
maximum. We expect 9.9 successes, so getting 9 or 10 successes dominates 
the distribution.

.. admonition:: Exampleüí°: Evaluating a Medical Trial
   :class: note

   A pharmaceutical company has developed a new medication for a common illness. 
   Historical data shows that **30% of people with this illness recover naturally** 
   without treatment. In a clinical trial, 10 randomly selected patients 
   received the new medication, 9 of which recovered. Is this strong evidence that the medication is effective, 
   or could this result reasonably occur by chance alone?

   **Step 1: Confirm BInS Criteria**

   We confirmed that this is a binomial experiment through Case 4 of
   the first example in this section.

   **Step 2: Is the result realistic in nature?**

   If the number of recovered patients followed the natural recovery rate,
   we would have

   .. math:: X \sim Bin(10, 0.3).

   What is the probability that we see a comparative or better recovery
   rate in nature than the current experiment? In other words, what is 
   :math:`P(X\geq 9)`?

   .. math:: 

      P(X \geq 9) &= P(X=9) + P(X=10) \\
      &= {10 \choose 9}(0.3)^9(0.7)^1 + {10 \choose 10}(0.3)^10(0.7)^0 \\
      &\approx 0.000138 + 0.000006 = 0.000144

   **Step 3: Interpreting the results**

   The probability of seeing 9 or more recoveries by chance alone is only 
   about 0.000144, or roughly 1 in 7,000. This extremely small probability 
   suggests that observing 9 recoveries is highly unlikely if the medication 
   had no effect.

   This analysis provides strong statistical evidence that the medication 
   is effective, though we'd want to see results from larger, more carefully 
   controlled studies before drawing definitive conclusions.

Bringin It All Together
----------------------------------------------


.. admonition:: Key Takeaways üìù
   :class: important

   1. The **binomial distribution** models the number of successes in a set of
      independent and identical trials.
   
   2. Use the **BInS criteria** to identify binomial situations: 
      Binary outcomes, Independent trials, n:math:`n` fixed trials, and constant 
      Success probability.
   
   3. The **binomial PMF** :math:`p_X(x) = \binom{n}{x} p^x (1-p)^{n-x}` 
      combines combinatorics with basic probability principles.
   
   4. Binomial random variables have simple **rxpected value and variance** 
      formulas: :math:`E[X] = np` 
      and :math:`Var(X) = np(1-p)`, derived by viewing binomial variables as 
      sums of Bernoulli trials.
   
   5. Binomial distributions are **symmetric when p = 0.5** and increasingly 
      skewed as p approaches 0 or 1.

Exercises
~~~~~~~~~~~~~~

1. **Identifying Binomial Experiments**: For each scenario, determine whether it represents a binomial experiment. 
   If not, explain which criterion is violated.
   
   a) A basketball player shoots 20 free throws. Each shot is independent, and her historical free-throw percentage is 85%.
   b) Cards are drawn one by one from a standard deck without replacement until an ace is drawn.
   c) A survey asks 100 randomly selected voters whether they support a particular candidate.
   d) A quality inspector examines products coming off an assembly line until finding the first defective item.

2. **Basic Probability Calculations**: A fair coin is flipped 8 times. Let X be the number of heads.
   
   a) Identify the parameters n and p.
   b) Calculate P(X = 3).
   c) Find P(X ‚â§ 2).
   d) What are the mean and standard deviation of X?

3. **Quality Control Application**: A manufacturing process produces items that are 5% defective. 
   If 25 items are randomly selected for inspection:
   
   a) What is the probability that exactly 2 items are defective?
   b) What is the probability that at most 1 item is defective?
   c) What is the expected number of defective items? What is the standard deviation?
   d) Would finding 5 defective items be considered unusual? Explain using probability.

4. **Medical Testing**: A diagnostic test correctly identifies a disease 90% of the time when 
   the disease is present. If the test is performed on 12 patients who have the disease:
   
   a) What is the probability that all 12 tests are positive?
   b) What is the probability that at least 10 tests are positive?
   c) How many positive tests would you expect to see on average?

5. **R Programming Practice**: Use R to analyze the following scenarios:
   
   a) Simulate 1000 samples from Bin(15, 0.4) and create a histogram. How does the sample 
      distribution compare to the theoretical distribution?
   b) For X ~ Bin(25, 0.2), find the value k such that P(X ‚â§ k) ‚âà 0.95.
   c) Compare P(X = 10) for three binomial distributions: Bin(20, 0.5), Bin(30, 0.33), 
      and Bin(40, 0.25). All have approximately the same mean‚Äîexplain any differences in the probabilities.

..
   Working with Binomial Distributions in R
   -----------------------------------------

   R provides powerful built-in functions for working with binomial distributions, making calculations much easier than computing combinations and powers by hand. Let's explore the key functions and see how they work.

   **The Four Essential R Functions**

   R follows a consistent naming pattern for probability distributions. For binomial distributions:

   - **rbinom()**: Generates random samples from a binomial distribution
   - **dbinom()**: Calculates the probability mass function (exact probabilities)
   - **pbinom()**: Calculates the cumulative distribution function (cumulative probabilities)
   - **qbinom()**: Finds quantiles (the inverse of pbinom)

   **Generating Random Samples with rbinom()**

   The `rbinom()` function simulates drawing random values from a binomial distribution:

   .. code-block:: r

      # Generate 10 random values from Bin(20, 0.3)
      # Each value represents number of successes in 20 trials
      set.seed(123)  # For reproducible results
      random_values <- rbinom(n = 10, size = 20, prob = 0.3)
      print(random_values)
      # Output: 8 4 8 4 2 7 7 6 4 6

      # Generate 1000 samples to see the distribution pattern
      large_sample <- rbinom(n = 1000, size = 20, prob = 0.3)
      
      # Check that sample mean approximates theoretical mean
      sample_mean <- mean(large_sample)
      theoretical_mean <- 20 * 0.3  # np = 6
      print(paste("Sample mean:", round(sample_mean, 2)))
      print(paste("Theoretical mean:", theoretical_mean))

   The `rbinom()` function is particularly useful for simulation studies, where we want to understand how binomial processes behave over many repetitions.

   **Calculating Exact Probabilities with dbinom()**

   The `dbinom()` function computes P(X = x) for specific values:

   .. code-block:: r

      # Calculate P(X = 5) when X ~ Bin(20, 0.3)
      prob_exactly_5 <- dbinom(x = 5, size = 20, prob = 0.3)
      print(paste("P(X = 5) =", round(prob_exactly_5, 4)))

      # Calculate probabilities for multiple values at once
      x_values <- 0:10
      probabilities <- dbinom(x = x_values, size = 20, prob = 0.3)
      
      # Create a probability table
      prob_table <- data.frame(
      x = x_values,
      probability = round(probabilities, 4)
      )
      print(prob_table)

      # Verify probabilities sum to 1 (approximately)
      total_prob <- sum(dbinom(x = 0:20, size = 20, prob = 0.3))
      print(paste("Total probability:", round(total_prob, 6)))

   **Calculating Cumulative Probabilities with pbinom()**

   The `pbinom()` function computes P(X ‚â§ x), which is often more useful than exact probabilities:

   .. code-block:: r

      # Calculate P(X ‚â§ 8) when X ~ Bin(20, 0.3)
      prob_at_most_8 <- pbinom(q = 8, size = 20, prob = 0.3)
      print(paste("P(X ‚â§ 8) =", round(prob_at_most_8, 4)))

      # Calculate P(X ‚â• 9) = 1 - P(X ‚â§ 8)
      prob_at_least_9 <- 1 - pbinom(q = 8, size = 20, prob = 0.3)
      # Alternative: use lower.tail = FALSE
      prob_at_least_9_alt <- pbinom(q = 8, size = 20, prob = 0.3, lower.tail = FALSE)
      print(paste("P(X ‚â• 9) =", round(prob_at_least_9, 4)))

      # Calculate P(5 ‚â§ X ‚â§ 10) = P(X ‚â§ 10) - P(X ‚â§ 4)
      prob_between <- pbinom(q = 10, size = 20, prob = 0.3) - 
                     pbinom(q = 4, size = 20, prob = 0.3)
      print(paste("P(5 ‚â§ X ‚â§ 10) =", round(prob_between, 4)))

   **Calculating the Probability with R**
   
   We want P(X ‚â• 9), the probability of observing 9 or more recoveries by chance alone.

   .. code-block:: r

      # Method 1: Using dbinom() for exact calculation
      prob_exactly_9 <- dbinom(x = 9, size = 10, prob = 0.3)
      prob_exactly_10 <- dbinom(x = 10, size = 10, prob = 0.3)
      prob_at_least_9 <- prob_exactly_9 + prob_exactly_10
      
      print(paste("P(X = 9) =", round(prob_exactly_9, 6)))
      print(paste("P(X = 10) =", round(prob_exactly_10, 6)))
      print(paste("P(X ‚â• 9) =", round(prob_at_least_9, 6)))
      
      # Method 2: Using pbinom() with lower.tail = FALSE
      prob_at_least_9_alt <- pbinom(q = 8, size = 10, prob = 0.3, lower.tail = FALSE)
      print(paste("P(X ‚â• 9) using pbinom =", round(prob_at_least_9_alt, 6)))
      
      # Method 3: Manual calculation to verify
      manual_calc <- choose(10,9) * (0.3^9) * (0.7^1) + choose(10,10) * (0.3^10) * (0.7^0)
      print(paste("Manual calculation =", round(manual_calc, 6)))

   **Understanding the Context with R**
   For comparison, let's examine the expected pattern without treatment:

   .. code-block:: r

      # Calculate expected value and standard deviation
      n <- 10
      p <- 0.3
      expected_recoveries <- n * p
      variance_recoveries <- n * p * (1 - p)
      sd_recoveries <- sqrt(variance_recoveries)
      
      print(paste("Expected recoveries:", expected_recoveries))
      print(paste("Standard deviation:", round(sd_recoveries, 3)))
      print(paste("Variance:", round(variance_recoveries, 3)))
      
      # Calculate how many standard deviations 9 is from the mean
      z_score <- (9 - expected_recoveries) / sd_recoveries
      print(paste("9 recoveries is", round(z_score, 2), "standard deviations above the mean"))
      
      # Generate simulation to show rarity of extreme results
      set.seed(456)
      simulated_results <- rbinom(n = 10000, size = 10, prob = 0.3)
      extreme_results <- sum(simulated_results >= 9)
      print(paste("In 10,000 simulations,", extreme_results, "had 9+ recoveries"))
      print(paste("Simulation probability:", round(extreme_results/10000, 6)))

