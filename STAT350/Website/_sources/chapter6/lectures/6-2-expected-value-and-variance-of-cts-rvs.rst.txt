.. _6-2-expected-value-and-variance-of-cts-rvs:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch6-2">
     <iframe
       id="video-ch6-2"
       title="STAT 350 â€“ Chapter 6.2 Expected Value and Variance Video"
       src="https://www.youtube.com/embed/_5PodnOjT5o?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
       allowfullscreen>
     </iframe>
   </div>

.. admonition:: Slides
   :class: tip

   `Download Chapter 6 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/Chapter%206%20Continuous%20Distributions/L12-14--ContinuousRandomVariablesProbabilityDensityCurves%28Chapter%206%29_AC.pptx>`_



Expected Value and Variance of Continuous Random Variables
==============================================================

Now that we understand how probability density functions work for continuous random variables, 
we need to extend our concepts of expected value and variance from the discrete world. 
The core ideas remain the sameâ€”we still want to measure the **center** and **spread** of a distributionâ€”
but the mathematical machinery shifts **from summation to integration**. 
This transition reveals the beautiful parallel structure between discrete and 
continuous probability theory.

.. admonition:: Road Map ðŸ§­
   :class: important

   â€¢ Extend **expected value** from discrete sums to continuous integrals.
   â€¢ Apply the **Law of the Unconscious Statistician (LOTUS)** for functions of continuous random variables.
   â€¢ Understand that **linearity and additivity properties** of expected values remain unchanged.
   â€¢ Define **variance** using integration and master the computational shortcut.
   â€¢ Explore **properties of variance** for linear transformations and sums of independent variables.

From Discrete Sums to Continuous Integrals
---------------------------------------------

The expected value of a discrete random variable involved summing each possible value, 
weighted by its probability. For continuous random variables, we replace this discrete 
sum with a continuous integral, **weighing each possible value by its probability density**.

Definition
~~~~~~~~~~~~~

The expected value of a continuous random variable :math:`X`, 
denoted :math:`E[X]` or :math:`\mu_X`, is the continuously weighted average 
of all values in its support:

.. math::

   \mu_X = E[X] = \int_{-\infty}^{\infty} x \cdot f_X(x) \, dx

This integral represents the "balance point" or center of mass of the 
probability distribution. Just as in the discrete case, values with higher 
probability density contribute more to the overall average.

Comparison with the Discrete Case
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. flat-table::
   :header-rows: 1
   :align: center
   :width: 70%

   * - Discrete :math:`E[X]`
     - Continuous :math:`E[X]`

   * - :math:`\sum_{x \in \text{supp}(X)} x \cdot p_X(x)`
     - :math:`\int_{-\infty}^{\infty} x \cdot f_X(x) \, dx = \int_{\text{supp}(X)}x \cdot f_X(x) \, dx`

The summation becomes an integration, and the probability mass function :math:`p_X(x)`
is replaced by the probability density function :math:`f_X(x)`. Although the integral is
formally taken over the entire real line :math:`(-\infty, \infty)` in the general definition
of continuous expectation, only values of :math:`x` within the support
contribute meaningfully to the computation, since :math:`f_X(x) = 0` outside :math:`\text{supp}(X)`.
Thus, the integral is effectively taken over the supportâ€”just as the summation is in the discrete case.


.. admonition:: Remark: The Absolute Integrability Condition
   :class: important

   For the expected value of :math:`X` to be well-defined and finite, 
   :math:`X` must satisfy 

   .. math::

      \int_{-\infty}^{\infty} |x| \cdot f_X(x) \, dx < \infty.

   All continuous distributions we encounter in this course
   satisfy this condition.

The Law of the Unconscious Statistician (LOTUS) for Continuous Random Variables
---------------------------------------------------------------------------------

Just as in the discrete case, we often want to find the **expected value of some 
function of a random variable**, like :math:`E[X^2]` or :math:`E[e^X]`. The Law of the Unconscious 
Statistician (LOTUS) extends naturally to continuous random variables.

Theorem: LOTUS
~~~~~~~~~~~~~~~~~~~~~~~~

If :math:`X` is a continuous random variable with PDF :math:`f_X(x)`, and :math:`g(x)` is a function, then:

.. math::

   E[g(X)] = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \, dx

The Power of LOTUS
~~~~~~~~~~~~~~~~~~~~~~~~

This theorem is remarkably powerful because it allows us to compute :math:`E[g(X)]` 
directly without having to find the PDF of the new random variable :math:`Y = g(X)`
Instead, we simply plug :math:`g(x)` into our expectation integral and use 
the original PDF :math:`f_X(x)`.

.. admonition:: ExampleðŸ’¡: Expected value of functions of :math:`X`
   :class: note

   Consider a continuous random variable :math:`X` with PDF 

   .. math::
      f_X(x) = 
      \begin{cases}
      &2x,  &0 \leq x \leq 1\\
      &0, & \text{ elsewhere }
      \end{cases}.

   Find :math:`E[X], E[X^2]`, and :math:`E[\sqrt{X}]`.

   * Find :math:`E[X]` using the definition

     .. math::
        E[X] = \int_0^1 x \cdot (2x) \, dx 
        = \int_0^1 2x^2 \, dx = 2 \cdot \frac{x^3}{3}\Bigg\rvert_0^1 
        = \frac{2}{3}

   * Apply LOTUS for :math:`E[X^2]` and :math:`E[\sqrt{X}]`

   .. math::
      E[X^2] = \int_0^1 x^2 \cdot (2x) \, dx = \int_0^1 2x^3 \, dx 
      = 2 \cdot \frac{x^4}{4}\Bigg\rvert_0^1 = \frac{1}{2}

   .. math::
      E[\sqrt{X}] = \int_0^1 x^{1/2} \cdot 2x \, dx 
      = \int_0^1 2x^{3/2} \, dx
      = 2\cdot \frac{2}{5}x^{5/2}\Bigg\rvert_{0}^1 = \frac{4}{5}


Properties of Expected Value: Unchanged by Continuity
--------------------------------------------------------

The fundamental properties of expected value that we learned for discrete random 
variables apply unchanged to continuous random variables. The only difference is 
that we use integration instead of summation to compute the expected values.

Linearity of Expectation
~~~~~~~~~~~~~~~~~~~~~~~~~~~

For any continuous random variable :math:`X` and constants :math:`a` and :math:`b`:

.. math::

   E[aX + b] = aE[X] + b

**Proof of linearity of expectation**

   .. math::

      \begin{align}
      E[aX + b] &= \int_{-\infty}^{\infty} (ax + b) \cdot f_X(x) \, dx \\
      &= a\int_{-\infty}^{\infty} x \cdot f_X(x) \, dx + b\int_{-\infty}^{\infty} f_X(x) \, dx \\
      &= aE[X] + b \cdot 1 \\
      &= aE[X] + b
      \end{align}

Additivity of Expectation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
For any set of continuous random variables :math:`X_1, X_2, \cdots, X_n`,

.. math::

   E[X_1 + X_2 + \cdots + X_n] = E[X_1] + E[X_2] + \cdots + E[X_n]

The remarkable fact is that additivity holds regardless of any dependence between 
the variables. 

Variance for Continuous Random Variables
-----------------------------------------

The concept of variance also translates 
directly from discrete to continuous random variables. 

Definition
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The variance of a continuous random variable :math:`X` is the expected value of 
the squared deviation from the mean:

.. math::

   \sigma_X^2 = \text{Var}(X) = E[(X - \mu_X)^2]
    = \int_{-\infty}^{\infty} (x - \mu_X)^2 \cdot f_X(x) \, dx

Computational Shortcut for Variance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Just as in the discrete case, we have the much more convenient computational formula:

.. math::

   \sigma_X^2 = E[X^2] - (E[X])^2

Standard Deviation
~~~~~~~~~~~~~~~~~~~~~~~

The standard deviation is the square root of the variance:

.. math::

   \sigma_X = \sqrt{\text{Var}(X)}

.. admonition:: ExampleðŸ’¡: Computing Variance
   :class: note 

   For the random variable :math:`X` with PDF 

   .. math::
      f_X(x) = 
      \begin{cases}
      &2x,  &0 \leq x \leq 1\\
      &0, &\text{ elsewhere }
      \end{cases},

   compute :math:`\text{Var}(X)` and :math:`\sigma_X`.

   Using :math:`E[X]` and :math:`E[X^2]` obtained in the previous example,
   apply the computational shortcut: 

   .. math::

      \text{Var}(X) = E[X^2] - (E[X])^2 
      = \frac{1}{2} - \left(\frac{2}{3}\right)^2 
      = \frac{1}{2} - \frac{4}{9} 
      = \frac{9-8}{18} = \frac{1}{18}

   Therefore, :math:`\sigma_X = \sqrt{1/18} = 1/(3\sqrt{2}) \approx 0.236`.

Properties of Variance for Continuous Random Variables
--------------------------------------------------------

The variance properties we learned for discrete random variables apply 
without modification to continuous random variables.

Variance of Linear Transformations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For any continuous random variable :math:`X` and constants :math:`a` and :math:`b`:

.. math::

   \text{Var}(aX + b) = a^2 \text{Var}(X)

Recall that:

- **Adding a constant** (:math:`b`) doesn't change how spread out a distribution isâ€”it 
  just shifts its location.
- **Multiplying by a constant** (:math:`a`) scales the variance by :math:`a^2`.

Variance of Sums for Independent Variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When :math:`X` and :math:`Y` are **independent** continuous random variables:

.. math::

   \text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y).

This extends to any number of mutually independent variables:

.. math::

   \text{Var}(X_1 + X_2 + \cdots + X_n) = \text{Var}(X_1) + \text{Var}(X_2) + 
   \cdots + \text{Var}(X_n).


.. admonition:: Be Cautious ðŸ›‘
   :class: danger

   The additivity of variances only applies when the random variables are **independent**. 
   This means that the mutual independence of all terms involved **must be provided or
   shown before the rule is applied**. 

   For dependent variables, we need to account for covariance terms.

Covariance and Correlation: A Brief Introduction
-------------------------------------------------

When dealing with multiple continuous random variables that may be dependent, 
we need measures of how they vary together. The concepts of covariance and 
correlation also extend to continuous random variables.

Covariance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The covariance between continuous random variables :math:`X` and :math:`Y` is:

.. math::

   \text{Cov}(X,Y) = E[(X - \mu_X)(Y - \mu_Y)] = E[XY] - \mu_X\mu_Y

Correlation
~~~~~~~~~~~~~~~~

The correlation coefficient is:

.. math::
   \rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}.

As before, correlation is unitless and bounded between -1 and +1.

.. admonition:: Note
   :class: important

   Working with joint distributions of continuous random variables involves multivariable 
   calculus and is beyond the scope of this course. We'll focus on single continuous random 
   variables in the remainder of this chapter.

Bringing It All Together
------------------------

.. admonition:: Key Takeaways ðŸ“
   :class: important

   1. **Expected value** for continuous random variables uses integration instead of 
      summation, but represents the same concept: a weighted average using probability 
      densities as weights.
   
   2. **All expectation properties** (LOTUS, linearity, additivity) remain unchangedâ€”only the 
      computational method (integration vs. summation) differs.
   
   3. **Variance** maintains the same conceptual meaning and computational shortcut.
   
   4. **Variance properties** for linear transformations and sums of independent variables 
      apply identically to continuous random variables.

The transition from discrete to continuous random variables preserves all the fundamental 
relationships we've learned, while replacing the discrete machinery of summation with the 
continuous machinery of integration. This parallel structure demonstrates the elegant 
unity underlying probability theory, whether we're counting discrete outcomes or measuring 
continuous quantities.

Exercises
~~~~~~~~~~~

1. **Basic Expected Value**: For the PDF

   .. math::
      f_X(x) = 
      \begin{cases}
      &3x^2, &0 â‰¤ x â‰¤ 1\\
      &0 &\text{ elsewhere}
      \end{cases},
   
   a) Find :math:`E[X]`.
   b) Find :math:`E[X^2]`.
   c) Find :math:`Var(X)`.
   d) Find :math:`E[2X + 3]`.
   e) Find :math:`Var(3X - 1)`.

2. **Piecewise PDF**: Consider the PDF:
   
   .. math::
      f_X(x) = 
      \begin{cases}
      &x/2, &0 â‰¤ x â‰¤ 2\\
      &(4-x)/2, &2 < x â‰¤ 4\\
      &0, &\text{ elsewhere}
      \end{cases}
   
   a) Verify this is a legitimate PDF.
   b) Find :math:`E[X]`.
   c) Find :math:`Var(X)`.

3. **Linear Transformations**: For :math:`X` with :math:`E[X] = 5` and :math:`Var(X) = 4`, find:
   
   a) :math:`E[3X - 7]`.
   b) :math:`\sigma_{3X - 7}`.