.. _6-2-expected-value-and-variance-of-cts-rvs:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch6-2">
     <iframe
       id="video-ch6-2"
       title="STAT 350 ‚Äì Chapter 6.2 Expected Value and Variance Video"
       src="https://www.youtube.com/embed/_5PodnOjT5o?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
       allowfullscreen>
     </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip
   
   `Download Chapter 6 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/slides/Chapter%
   206%20Continuous%20Distributions/L12-14--ContinuousRandomVariablesProbabilityDensityCurves%28Chapter%206%29_AC.pptx>`_
   
Expected Value and Variance of Continuous Random Variables
==============================================================

Now that we understand how probability density functions work for continuous random variables, 
we need to extend our concepts of expected value and variance from the discrete world. 
The core ideas remain the same‚Äîwe still want to measure the **center** and **spread** of a distribution‚Äîbut
the mathematical machinery shifts **from summation to integration**. 
This transition reveals the beautiful parallel structure between discrete and 
continuous probability theory.

.. admonition:: Road Map üß≠
   :class: important

   ‚Ä¢ Extend **expected value** from discrete sums to continuous integrals.
   ‚Ä¢ Apply the **Law of the Unconscious Statistician (LOTUS)** for functions of continuous random variables.
   ‚Ä¢ Understand that the **linearity and additive properties** of expected values remain unchanged.
   ‚Ä¢ Define **variance** using integration and master the computational shortcut.
   ‚Ä¢ Explore **properties of variance** for linear transformations and sums of independent variables.

From Discrete Sums to Continuous Integrals
---------------------------------------------

The expected value of a discrete random variable involved summing each possible value, 
weighted by its probability. For continuous random variables, we replace this discrete 
sum with a continuous integral, **weighing each possible value by its probability density**.

Definition
~~~~~~~~~~~~~

The expected value of a continuous random variable :math:`X`, 
denoted :math:`E[X]` or :math:`\mu_X`, is the continuously weighted average 
of all values in its support:

.. math::

   \mu_X = E[X] = \int_{-\infty}^{\infty} x \cdot f_X(x) \, dx

This integral represents the "balance point" or center of mass of the 
probability distribution. Just as in the discrete case, values with higher 
probability density contribute more to the overall average.

Comparison with the Discrete Case
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. flat-table::
   :header-rows: 1
   :align: center
   :width: 70%

   * - Discrete :math:`E[X]`
     - Continuous :math:`E[X]`

   * - .. math::
         \sum_{x \in \text{supp}(X)} x \cdot p_X(x)
     - .. math::
         \int_{-\infty}^{\infty} x \cdot f_X(x) \, dx = \int_{\text{supp}(X)}x \cdot f_X(x) \, dx

The summation becomes an integration, and the probability mass function :math:`p_X(x)`
is replaced by the probability density function :math:`f_X(x)`. Although the integral is
formally taken over the entire real line :math:`(-\infty, \infty)` in the general definition
of continuous expectation, only values of :math:`x` within the support
contribute meaningfully to the computation, since :math:`f_X(x) = 0` outside :math:`\text{supp}(X)`.
Thus, the integral is effectively taken over the support‚Äîjust as the summation is in the discrete case.


.. admonition:: Remark: The Absolute Integrability Condition
   :class: important

   For the expected value of :math:`X` to be well-defined and finite, 
   :math:`X` must satisfy 

   .. math::

      \int_{-\infty}^{\infty} |x| \cdot f_X(x) \, dx < \infty.

   All continuous distributions we encounter in this course
   satisfy this condition.

The Law of the Unconscious Statistician (LOTUS) for Continuous Random Variables
---------------------------------------------------------------------------------

Just as in the discrete case, we often want to find the **expected value of some 
function of a random variable**, like :math:`E[X^2]` or :math:`E[e^X]`. The Law of the Unconscious 
Statistician (LOTUS) extends naturally to continuous random variables.

Theorem: LOTUS
~~~~~~~~~~~~~~~~~~~~~~~~

If :math:`X` is a continuous random variable with PDF :math:`f_X(x)`, and :math:`g(x)` is a function, then:

.. math::

   E[g(X)] = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \, dx

The Power of LOTUS
~~~~~~~~~~~~~~~~~~~~~~~~

This theorem is powerful because it allows us to compute :math:`E[g(X)]` 
directly without having to find the PDF of the new random variable :math:`Y = g(X)`.
Instead, we simply plug :math:`g(x)` into our expectation integral and use 
the original PDF :math:`f_X(x)`.

.. admonition:: Exampleüí°: Expected value of functions of :math:`X`
   :class: note

   Consider a continuous random variable :math:`X` with PDF 

   .. math::
      f_X(x) = 
      \begin{cases}
      &2x,  &0 \leq x \leq 1\\
      &0, & \text{ elsewhere }
      \end{cases}.

   Find :math:`E[X], E[X^2]`, and :math:`E[\sqrt{X}]`.

   * Find :math:`E[X]` using the definition

     .. math::
        E[X] = \int_0^1 x \cdot (2x) \, dx 
        = \int_0^1 2x^2 \, dx = 2 \cdot \frac{x^3}{3}\Bigg\rvert_0^1 
        = \frac{2}{3}

   * Apply LOTUS for :math:`E[X^2]` and :math:`E[\sqrt{X}]`

   .. math::
      E[X^2] = \int_0^1 x^2 \cdot (2x) \, dx = \int_0^1 2x^3 \, dx 
      = 2 \cdot \frac{x^4}{4}\Bigg\rvert_0^1 = \frac{1}{2}

   .. math::
      E[\sqrt{X}] = \int_0^1 x^{1/2} \cdot 2x \, dx 
      = \int_0^1 2x^{3/2} \, dx
      = 2\cdot \frac{2}{5}x^{5/2}\Bigg\rvert_{0}^1 = \frac{4}{5}


Properties of Expected Value: Unchanged by Continuity
--------------------------------------------------------

The fundamental properties of expected value that we learned for discrete random 
variables apply unchanged to continuous random variables.

Linearity of Expectation
~~~~~~~~~~~~~~~~~~~~~~~~~~~

For any continuous random variable :math:`X` and constants :math:`a` and :math:`b`:

.. math::

   E[aX + b] = aE[X] + b

**Proof of linearity of expectation**

   .. math::

      \begin{aligned}
      E[aX + b] &= \int_{-\infty}^{\infty} (ax + b) \cdot f_X(x) \, dx \\
      &= a\int_{-\infty}^{\infty} x \cdot f_X(x) \, dx + b\int_{-\infty}^{\infty} f_X(x) \, dx \\
      &= aE[X] + b \cdot 1 \\
      &= aE[X] + b
      \end{aligned}

Additivity of Expectation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
For any set of continuous random variables :math:`X_1, X_2, \cdots, X_n`,

.. math::

   E[X_1 + X_2 + \cdots + X_n] = E[X_1] + E[X_2] + \cdots + E[X_n]

Variance for Continuous Random Variables
-----------------------------------------

Definition
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The variance of a continuous random variable :math:`X` is the expected value of 
the squared deviation from the mean:

.. math::

   \sigma_X^2 = \text{Var}(X) = E[(X - \mu_X)^2]
    = \int_{-\infty}^{\infty} (x - \mu_X)^2 \cdot f_X(x) \, dx

Computational Shortcut for Variance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Just as in the discrete case, we have the much more convenient computational formula:

.. math::

   \sigma_X^2 = E[X^2] - (E[X])^2

Standard Deviation
~~~~~~~~~~~~~~~~~~~~~~~

The standard deviation is the square root of the variance:

.. math::

   \sigma_X = \sqrt{\text{Var}(X)}

.. admonition:: Exampleüí°: Computing Variance
   :class: note 

   For the random variable :math:`X` with PDF 

   .. math::
      f_X(x) = 
      \begin{cases}
      &2x,  &0 \leq x \leq 1\\
      &0, &\text{ elsewhere }
      \end{cases},

   compute :math:`\text{Var}(X)` and :math:`\sigma_X`.

   Using :math:`E[X]` and :math:`E[X^2]` obtained in the previous example,
   apply the computational shortcut: 

   .. math::

      \text{Var}(X) = E[X^2] - (E[X])^2 
      = \frac{1}{2} - \left(\frac{2}{3}\right)^2 
      = \frac{1}{2} - \frac{4}{9} 
      = \frac{9-8}{18} = \frac{1}{18}

   Therefore, :math:`\sigma_X = \sqrt{1/18} = 1/(3\sqrt{2}) \approx 0.236`.

Properties of Variance for Continuous Random Variables
--------------------------------------------------------

The variance properties we learned for discrete random variables apply 
without modification to continuous random variables.

Variance of Linear Transformations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For any continuous random variable :math:`X` and constants :math:`a` and :math:`b`:

.. math::

   \text{Var}(aX + b) = a^2 \text{Var}(X)

Recall that:

- **Adding a constant** (:math:`b`) doesn't change how spread out a distribution is‚Äîit 
  just shifts its location.
- **Multiplying by a constant** (:math:`a`) scales the variance by :math:`a^2`.

Variance of Sums of Independent  Random Variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When :math:`X` and :math:`Y` are **independent** continuous random variables:

.. math::

   \text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y).

This extends to any number of mutually independent variables:

.. math::

   \text{Var}(X_1 + X_2 + \cdots + X_n) = \text{Var}(X_1) + \text{Var}(X_2) + 
   \cdots + \text{Var}(X_n).


.. admonition:: Be Cautious üõë
   :class: danger

   The additivity of variances only applies when the random variables are **independent**. 
   This means that the mutual independence of all terms involved **must be provided or
   mathematically shown before the rule is applied**. 

   For dependent variables, we need to account for covariance terms.

Covariance and Correlation: A Brief Introduction
-------------------------------------------------

When dealing with multiple continuous random variables that may be dependent, 
we need measures of how they vary together. The concepts of covariance and 
correlation also extend to continuous random variables.

Covariance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The covariance between continuous random variables :math:`X` and :math:`Y` is:

.. math::

   \text{Cov}(X,Y) = E[(X - \mu_X)(Y - \mu_Y)] = E[XY] - \mu_X\mu_Y

Correlation
~~~~~~~~~~~~~~~~

The correlation coefficient is:

.. math::
   \rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y}.

As before, correlation is unitless and bounded between -1 and +1.

.. admonition:: Note
   :class: important

   Working with continuous joint distributions involves multivariable 
   calculus and is beyond the scope of this course. We'll focus on single continuous random 
   variables in the remainder of this chapter.

Bringing It All Together
------------------------

.. admonition:: Key Takeaways üìù
   :class: important

   1. The **expected value** of a continuous random variable uses integration instead of 
      summation, but represents the same concept: a weighted average using probability 
      densities as weights.
   
   2. **All properties of expectation** (LOTUS, linearity, additivity) remain unchanged‚Äîonly the 
      computational method (integration vs. summation) differs.
   
   3. **Variance** maintains the same conceptual meaning and computational shortcut.
   
   4. **Variance properties** for linear transformations and sums of independent variables 
      apply identically to continuous random variables.


Exercises
---------

These exercises develop your skills in computing expected values and variances for continuous random variables using integration, applying LOTUS for functions of random variables, and using the properties of expectation and variance.

----

.. admonition:: Exercise 1: Basic Expected Value and Variance Computation
   :class: note

   A biomedical engineer models the concentration :math:`X` (in mg/L) of a drug in a patient's bloodstream with the following PDF:

   .. math::

      f_X(x) = 
      \begin{cases}
      3x^2, & 0 \leq x \leq 1\\
      0, & \text{elsewhere}
      \end{cases}

   a. Verify this is a valid PDF.

   b. Find :math:`E[X]`, the expected drug concentration.

   c. Find :math:`E[X^2]` using LOTUS.

   d. Calculate :math:`\text{Var}(X)` using the computational shortcut.

   e. Find the standard deviation :math:`\sigma_X`.

   f. If the therapeutic range requires concentrations within one standard deviation of the mean, what is this range?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Verify PDF validity**

      **Non-negativity**: :math:`3x^2 \geq 0` for all :math:`x \in [0, 1]`. ‚úì

      **Total area = 1**:

      .. math::

         \int_0^1 3x^2 \, dx = 3 \cdot \frac{x^3}{3} \Bigg|_0^1 = x^3 \Bigg|_0^1 = 1 - 0 = 1 \text{ ‚úì}

      **Part (b): E[X]**

      .. math::

         E[X] = \int_0^1 x \cdot 3x^2 \, dx = 3\int_0^1 x^3 \, dx = 3 \cdot \frac{x^4}{4} \Bigg|_0^1 = \frac{3}{4} = 0.75 \text{ mg/L}

      **Part (c): E[X¬≤] using LOTUS**

      .. math::

         E[X^2] = \int_0^1 x^2 \cdot 3x^2 \, dx = 3\int_0^1 x^4 \, dx = 3 \cdot \frac{x^5}{5} \Bigg|_0^1 = \frac{3}{5} = 0.6 \text{ (mg/L)}^2

      **Part (d): Var(X) using computational shortcut**

      .. math::

         \text{Var}(X) = E[X^2] - (E[X])^2 = \frac{3}{5} - \left(\frac{3}{4}\right)^2 = \frac{3}{5} - \frac{9}{16}

      Finding common denominator (80):

      .. math::

         \text{Var}(X) = \frac{48}{80} - \frac{45}{80} = \frac{3}{80} = 0.0375 \text{ (mg/L)}^2

      **Part (e): Standard deviation**

      .. math::

         \sigma_X = \sqrt{\frac{3}{80}} = \frac{\sqrt{3}}{\sqrt{80}} = \frac{\sqrt{3}}{4\sqrt{5}} = \frac{\sqrt{15}}{20} \approx 0.194 \text{ mg/L}

      **Part (f): Therapeutic range**

      The range within one standard deviation of the mean is:

      .. math::

         (\mu - \sigma, \mu + \sigma) = (0.75 - 0.194, 0.75 + 0.194) = (0.556, 0.944) \text{ mg/L}

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig1_cubic_pdf_expected_value.png
         :alt: Cubic PDF with E[X] = 3/4 and therapeutic range shaded
         :align: center
         :width: 80%

         The cubic PDF :math:`f_X(x) = 3x^2` with E[X] = 0.75 mg/L marked and the therapeutic range (Œº ¬± œÉ) shaded.

----

.. admonition:: Exercise 2: Decreasing Linear PDF
   :class: note

   A reliability engineer models the failure time :math:`X` (in years) of a sensor component with PDF:

   .. math::

      f_X(x) = 
      \begin{cases}
      2(1 - x), & 0 \leq x \leq 1\\
      0, & \text{elsewhere}
      \end{cases}

   a. Sketch the PDF. Is this distribution skewed? If so, in which direction?

   b. Find :math:`E[X]`.

   c. Find :math:`E[X^2]` and :math:`\text{Var}(X)`.

   d. Based on your answers, does this component tend to fail early or late in its first year?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Sketch and skewness**

      The PDF starts at :math:`f_X(0) = 2` and decreases linearly to :math:`f_X(1) = 0`. This forms a right triangle with the base on the x-axis.

      Since the PDF is higher for smaller values of :math:`x`, the distribution is **right-skewed** (positively skewed). Most of the probability mass is concentrated near 0, with a long tail toward 1.

      **Part (b): E[X]**

      .. math::

         E[X] = \int_0^1 x \cdot 2(1-x) \, dx = 2\int_0^1 (x - x^2) \, dx = 2\left[\frac{x^2}{2} - \frac{x^3}{3}\right]_0^1

      .. math::

         = 2\left(\frac{1}{2} - \frac{1}{3}\right) = 2 \cdot \frac{1}{6} = \frac{1}{3} \approx 0.333 \text{ years}

      **Part (c): E[X¬≤] and Var(X)**

      .. math::

         E[X^2] = \int_0^1 x^2 \cdot 2(1-x) \, dx = 2\int_0^1 (x^2 - x^3) \, dx = 2\left[\frac{x^3}{3} - \frac{x^4}{4}\right]_0^1

      .. math::

         = 2\left(\frac{1}{3} - \frac{1}{4}\right) = 2 \cdot \frac{1}{12} = \frac{1}{6}

      .. math::

         \text{Var}(X) = E[X^2] - (E[X])^2 = \frac{1}{6} - \left(\frac{1}{3}\right)^2 = \frac{1}{6} - \frac{1}{9} = \frac{3 - 2}{18} = \frac{1}{18}

      .. math::

         \sigma_X = \sqrt{\frac{1}{18}} = \frac{1}{3\sqrt{2}} \approx 0.236 \text{ years}

      **Part (d): Interpretation**

      The expected failure time is only :math:`\frac{1}{3}` year (4 months), which is well before the midpoint of the first year. Combined with the right-skewed distribution, this indicates the component **tends to fail early**. The decreasing PDF shows that failures become progressively less likely as time passes‚Äîcomponents that survive the initial period are less likely to fail later.

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig2_decreasing_pdf_skewness.png
         :alt: Decreasing linear PDF showing right-skewness
         :align: center
         :width: 80%

         The decreasing PDF :math:`f_X(x) = 2(1-x)` is right-skewed with most mass concentrated near 0.

----

.. admonition:: Exercise 3: LOTUS with Multiple Functions
   :class: note

   An aerospace engineer models aerodynamic drag coefficient :math:`X` with PDF:

   .. math::

      f_X(x) = 
      \begin{cases}
      \frac{3}{8}x^2, & 0 \leq x \leq 2\\
      0, & \text{elsewhere}
      \end{cases}

   a. Verify this is a valid PDF.

   b. Find :math:`E[X]`.

   c. Find :math:`E[X^2]`.

   d. Find :math:`E[X^3]`.

   e. The power required to overcome drag is proportional to :math:`X^3`. If :math:`P = 100X^3` watts, find :math:`E[P]`.

   f. A naive calculation substitutes :math:`E[X]` into the power formula, computing :math:`g(E[X]) = 100 \cdot (E[X])^3` instead of :math:`E[P]`. What value would this give? Which is correct and why do they differ?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Verify PDF validity**

      **Non-negativity**: :math:`\frac{3}{8}x^2 \geq 0` for all :math:`x`. ‚úì

      **Total area = 1**:

      .. math::

         \int_0^2 \frac{3}{8}x^2 \, dx = \frac{3}{8} \cdot \frac{x^3}{3} \Bigg|_0^2 = \frac{1}{8} \cdot 8 = 1 \text{ ‚úì}

      **Part (b): E[X]**

      .. math::

         E[X] = \int_0^2 x \cdot \frac{3}{8}x^2 \, dx = \frac{3}{8}\int_0^2 x^3 \, dx = \frac{3}{8} \cdot \frac{x^4}{4} \Bigg|_0^2 = \frac{3}{32} \cdot 16 = \frac{3}{2} = 1.5

      **Part (c): E[X¬≤]**

      .. math::

         E[X^2] = \int_0^2 x^2 \cdot \frac{3}{8}x^2 \, dx = \frac{3}{8}\int_0^2 x^4 \, dx = \frac{3}{8} \cdot \frac{x^5}{5} \Bigg|_0^2 = \frac{3}{40} \cdot 32 = \frac{96}{40} = \frac{12}{5} = 2.4

      **Part (d): E[X¬≥]**

      .. math::

         E[X^3] = \int_0^2 x^3 \cdot \frac{3}{8}x^2 \, dx = \frac{3}{8}\int_0^2 x^5 \, dx = \frac{3}{8} \cdot \frac{x^6}{6} \Bigg|_0^2 = \frac{1}{16} \cdot 64 = 4

      **Part (e): E[P] using LOTUS**

      Since :math:`P = 100X^3`:

      .. math::

         E[P] = E[100X^3] = 100 \cdot E[X^3] = 100 \times 4 = 400 \text{ watts}

      **Part (f): Naive calculation and comparison**

      The naive calculation substitutes :math:`E[X]` directly into the power formula:

      .. math::

         g(E[X]) = 100 \cdot (E[X])^3 = 100 \cdot (1.5)^3 = 100 \times 3.375 = 337.5 \text{ watts}

      **The correct expected power is 400 watts** (from LOTUS).

      **Why they differ**: In general, :math:`E[g(X)] \neq g(E[X])` unless :math:`g` is a linear function. The power function :math:`g(x) = 100x^3` is **convex** (curves upward for :math:`x > 0`), which means :math:`E[g(X)] > g(E[X])`. Because :math:`g(x)` is convex, variability in :math:`X` increases the expected value of :math:`g(X)`.

      The naive approach underestimates average power because it ignores variability. Higher-than-average drag coefficients contribute disproportionately to power consumption due to the cubic relationship.

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig3_jensen_convex.png
         :alt: Convex function showing E[g(X)] greater than g(E[X])
         :align: center
         :width: 80%

         For the convex function :math:`g(x) = 100x^3`, we have :math:`E[g(X)] = 400 > g(E[X]) = 337.5`.

----

.. admonition:: Exercise 4: Symmetric Distribution and Expected Value
   :class: note

   A quality control engineer models measurement error :math:`X` (in mm) with PDF:

   .. math::

      f_X(x) = 
      \begin{cases}
      \frac{3}{4}(1 - x^2), & -1 \leq x \leq 1\\
      0, & \text{elsewhere}
      \end{cases}

   a. Verify this is a valid PDF.

   b. This PDF is symmetric about :math:`x = 0`. Use this fact to determine :math:`E[X]` without integration.

   c. Find :math:`E[X^2]`.

   d. Calculate :math:`\text{Var}(X)`.

   e. Find :math:`E[X^4]`. (Hint: You'll need this for problems involving variance of :math:`X^2`.)

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Verify PDF validity**

      **Non-negativity**: For :math:`-1 \leq x \leq 1`, we have :math:`x^2 \leq 1`, so :math:`1 - x^2 \geq 0`. Thus :math:`f_X(x) = \frac{3}{4}(1-x^2) \geq 0`. ‚úì

      **Total area = 1**:

      .. math::

         \int_{-1}^{1} \frac{3}{4}(1 - x^2) \, dx = \frac{3}{4}\left[x - \frac{x^3}{3}\right]_{-1}^{1}

      .. math::

         = \frac{3}{4}\left[\left(1 - \frac{1}{3}\right) - \left(-1 + \frac{1}{3}\right)\right] = \frac{3}{4}\left[\frac{2}{3} + \frac{2}{3}\right] = \frac{3}{4} \cdot \frac{4}{3} = 1 \text{ ‚úì}

      **Part (b): E[X] by symmetry**

      The PDF :math:`f_X(x) = \frac{3}{4}(1-x^2)` is an **even function** (symmetric about :math:`x = 0`):

      .. math::

         f_X(-x) = \frac{3}{4}(1 - (-x)^2) = \frac{3}{4}(1 - x^2) = f_X(x)

      When a PDF is symmetric about :math:`x = c`, the expected value equals :math:`c`.

      Therefore, :math:`E[X] = 0`.

      **Part (c): E[X¬≤]**

      .. math::

         E[X^2] = \int_{-1}^{1} x^2 \cdot \frac{3}{4}(1 - x^2) \, dx = \frac{3}{4}\int_{-1}^{1} (x^2 - x^4) \, dx

      Since :math:`x^2 - x^4` is an even function, we can use:

      .. math::

         = \frac{3}{4} \cdot 2\int_{0}^{1} (x^2 - x^4) \, dx = \frac{3}{2}\left[\frac{x^3}{3} - \frac{x^5}{5}\right]_0^1 = \frac{3}{2}\left(\frac{1}{3} - \frac{1}{5}\right)

      .. math::

         = \frac{3}{2} \cdot \frac{2}{15} = \frac{1}{5} = 0.2

      **Part (d): Var(X)**

      .. math::

         \text{Var}(X) = E[X^2] - (E[X])^2 = \frac{1}{5} - 0^2 = \frac{1}{5} = 0.2

      .. math::

         \sigma_X = \sqrt{0.2} = \frac{1}{\sqrt{5}} \approx 0.447 \text{ mm}

      **Part (e): E[X‚Å¥]**

      .. math::

         E[X^4] = \int_{-1}^{1} x^4 \cdot \frac{3}{4}(1 - x^2) \, dx = \frac{3}{4}\int_{-1}^{1} (x^4 - x^6) \, dx

      Using symmetry:

      .. math::

         = \frac{3}{4} \cdot 2\int_{0}^{1} (x^4 - x^6) \, dx = \frac{3}{2}\left[\frac{x^5}{5} - \frac{x^7}{7}\right]_0^1 = \frac{3}{2}\left(\frac{1}{5} - \frac{1}{7}\right)

      .. math::

         = \frac{3}{2} \cdot \frac{2}{35} = \frac{3}{35}

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig4_symmetric_pdf.png
         :alt: Symmetric parabolic PDF with E[X] = 0
         :align: center
         :width: 80%

         The symmetric PDF :math:`f_X(x) = \frac{3}{4}(1-x^2)` has :math:`E[X] = 0` by symmetry‚Äîno integration needed!

----

.. admonition:: Exercise 5: Linear Transformations
   :class: note

   A chemical engineer measures temperature :math:`X` in Celsius with :math:`E[X] = 25¬∞C` and :math:`\sigma_X = 3¬∞C`.

   a. Convert to Fahrenheit using :math:`F = \frac{9}{5}X + 32`. Find :math:`E[F]` and :math:`\sigma_F`.

   b. Convert to Kelvin using :math:`K = X + 273.15`. Find :math:`E[K]` and :math:`\sigma_K`.

   c. A control system triggers an alarm when temperature deviates more than 2 standard deviations from the mean. Express this range in all three temperature scales.

   d. Why does adding a constant (like 273.15 for Kelvin) not change the standard deviation?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Fahrenheit conversion**

      Using :math:`F = \frac{9}{5}X + 32`:

      **Expected value** (linearity):

      .. math::

         E[F] = E\left[\frac{9}{5}X + 32\right] = \frac{9}{5}E[X] + 32 = \frac{9}{5}(25) + 32 = 45 + 32 = 77¬∞F

      **Standard deviation** (variance of linear transformation):

      .. math::

         \text{Var}(F) = \text{Var}\left(\frac{9}{5}X + 32\right) = \left(\frac{9}{5}\right)^2 \text{Var}(X) = \frac{81}{25} \cdot 9 = \frac{729}{25}

      .. math::

         \sigma_F = \sqrt{\frac{729}{25}} = \frac{27}{5} = 5.4¬∞F

      Alternatively: :math:`\sigma_F = \left|\frac{9}{5}\right| \sigma_X = \frac{9}{5} \times 3 = 5.4¬∞F`

      **Part (b): Kelvin conversion**

      Using :math:`K = X + 273.15`:

      **Expected value**:

      .. math::

         E[K] = E[X + 273.15] = E[X] + 273.15 = 25 + 273.15 = 298.15 \text{ K}

      **Standard deviation**:

      .. math::

         \text{Var}(K) = \text{Var}(X + 273.15) = 1^2 \cdot \text{Var}(X) = 9

      .. math::

         \sigma_K = 3 \text{ K}

      The standard deviation is unchanged because adding a constant only shifts the distribution, not its spread.

      **Part (c): Alarm range (¬±2œÉ from mean)**

      **Celsius**: :math:`25 \pm 2(3) = (19, 31)¬∞C`

      **Fahrenheit**: :math:`77 \pm 2(5.4) = (66.2, 87.8)¬∞F`

      **Kelvin**: :math:`298.15 \pm 2(3) = (292.15, 304.15)` K

      **Part (d): Why adding a constant doesn't change œÉ**

      Variance measures **spread**‚Äîhow far values deviate from the mean. When we add a constant :math:`b` to every value:

      - Every observation shifts by the same amount
      - The mean also shifts by that same amount
      - The **deviations from the mean** remain unchanged: :math:`(X + b) - (\mu + b) = X - \mu`

      Since variance is based on squared deviations, and those deviations don't change, variance (and therefore standard deviation) remains the same.

      Mathematically: :math:`\text{Var}(X + b) = E[(X + b - E[X + b])^2] = E[(X - \mu_X)^2] = \text{Var}(X)`

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig5_linear_transformation.png
         :alt: Effect of linear transformations on distributions
         :align: center
         :width: 80%

         Linear transformations: Adding a constant shifts the mean but preserves spread; multiplying scales both mean and spread.

----

.. admonition:: Exercise 6: Sum of Independent Random Variables
   :class: note

   A data center has three independent server racks. The power consumption :math:`X_i` (in kW) of each rack has :math:`E[X_i] = 15` kW and :math:`\text{Var}(X_i) = 4` kW¬≤.

   a. Find the expected total power consumption :math:`E[X_1 + X_2 + X_3]`.

   b. Find :math:`\text{Var}(X_1 + X_2 + X_3)` and the standard deviation of total power.

   c. The facility has a 50 kW power budget. How many standard deviations above the expected total is this budget?

   d. If a fourth identical rack is added, find the new expected total and standard deviation.

   e. By what factor does the standard deviation increase when going from 3 to 4 racks? Is this more or less than the factor increase in expected value?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Expected total power**

      By additivity of expectation:

      .. math::

         E[X_1 + X_2 + X_3] = E[X_1] + E[X_2] + E[X_3] = 15 + 15 + 15 = 45 \text{ kW}

      **Part (b): Variance and SD of total power**

      Since the racks are **independent**, variances add:

      .. math::

         \text{Var}(X_1 + X_2 + X_3) = \text{Var}(X_1) + \text{Var}(X_2) + \text{Var}(X_3) = 4 + 4 + 4 = 12 \text{ kW}^2

      .. math::

         \sigma_{total} = \sqrt{12} = 2\sqrt{3} \approx 3.46 \text{ kW}

      **Part (c): Budget margin in standard deviations**

      .. math::

         z = \frac{50 - 45}{2\sqrt{3}} = \frac{5}{2\sqrt{3}} = \frac{5\sqrt{3}}{6} \approx 1.44 \text{ standard deviations}

      The 50 kW budget is about 1.44 standard deviations above the expected consumption.

      **Part (d): Four racks**

      **Expected total**:

      .. math::

         E[X_1 + X_2 + X_3 + X_4] = 4 \times 15 = 60 \text{ kW}

      **Variance**:

      .. math::

         \text{Var}(X_1 + X_2 + X_3 + X_4) = 4 \times 4 = 16 \text{ kW}^2

      **Standard deviation**:

      .. math::

         \sigma_{\text{total}} = \sqrt{16} = 4 \text{ kW}

      **Part (e): Factor comparison**

      **Standard deviation factor**: :math:`\frac{4}{2\sqrt{3}} = \frac{4}{3.46} \approx 1.155` (or exactly :math:`\frac{2}{\sqrt{3}} = \sqrt{\frac{4}{3}}`)

      **Expected value factor**: :math:`\frac{60}{45} = \frac{4}{3} \approx 1.333`

      The standard deviation increases by a **smaller factor** than the expected value.

      **Key insight**: For :math:`n` independent, identically distributed random variables:

      - Expected value of sum = :math:`n \cdot \mu` (scales linearly with :math:`n`)
      - Standard deviation of sum = :math:`\sqrt{n} \cdot \sigma` (scales with :math:`\sqrt{n}`)

      This "square root law" means that relative variability decreases as we add more independent components‚Äîan important principle in risk diversification.

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig6a_sum_independent_rvs.png
         :alt: Sum of independent random variables - expected value and standard deviation
         :align: center
         :width: 80%

         For sums of independent RVs: :math:`E[\text{Sum}]` grows linearly with :math:`n`, while :math:`\sigma_{\text{Sum}}` grows as :math:`\sqrt{n}`.

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig6b_sqrt_n_law.png
         :alt: The square root n law - coefficient of variation decreases
         :align: center
         :width: 80%

         The ‚àön law: Relative variability (CV = œÉ/Œº) decreases as :math:`1/\sqrt{n}`‚Äîthe basis for diversification benefits.

----

.. admonition:: Exercise 7: Piecewise PDF with Expected Value
   :class: note

   A network engineer models packet sizes :math:`X` (in KB) with PDF:

   .. math::

      f_X(x) = 
      \begin{cases}
      \frac{x}{4}, & 0 \leq x \leq 2\\
      \frac{4-x}{4}, & 2 < x \leq 4\\
      0, & \text{elsewhere}
      \end{cases}

   a. Verify this is a valid PDF. (Hint: Compute each piece separately.)

   b. This is a **triangular distribution**. Identify its mode (peak).

   c. Use symmetry to find :math:`E[X]`.

   d. Find :math:`E[X^2]` by computing the integral over both pieces.

   e. Calculate :math:`\text{Var}(X)`.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Verify PDF validity**

      **Non-negativity**: 

      - For :math:`0 \leq x \leq 2`: :math:`\frac{x}{4} \geq 0` ‚úì
      - For :math:`2 < x \leq 4`: :math:`4 - x \geq 0`, so :math:`\frac{4-x}{4} \geq 0` ‚úì

      **Total area = 1**:

      .. math::

         \int_0^2 \frac{x}{4} \, dx + \int_2^4 \frac{4-x}{4} \, dx

      First integral:

      .. math::

         \int_0^2 \frac{x}{4} \, dx = \frac{1}{4} \cdot \frac{x^2}{2} \Bigg|_0^2 = \frac{1}{8} \cdot 4 = \frac{1}{2}

      Second integral:

      .. math::

         \int_2^4 \frac{4-x}{4} \, dx = \frac{1}{4}\left[4x - \frac{x^2}{2}\right]_2^4 = \frac{1}{4}\left[(16 - 8) - (8 - 2)\right] = \frac{1}{4}(8 - 6) = \frac{1}{2}

      Total: :math:`\frac{1}{2} + \frac{1}{2} = 1` ‚úì

      **Part (b): Mode**

      The mode is where the PDF reaches its maximum. Both pieces meet at :math:`x = 2` with :math:`f_X(2) = \frac{2}{4} = \frac{1}{2}`.

      **Mode = 2 KB**

      **Part (c): E[X] by symmetry**

      The triangular PDF is symmetric about :math:`x = 2` (the peak).

      By symmetry: :math:`E[X] = 2` KB

      **Part (d): E[X¬≤]**

      .. math::

         E[X^2] = \int_0^2 x^2 \cdot \frac{x}{4} \, dx + \int_2^4 x^2 \cdot \frac{4-x}{4} \, dx

      First integral:

      .. math::

         \frac{1}{4}\int_0^2 x^3 \, dx = \frac{1}{4} \cdot \frac{x^4}{4} \Bigg|_0^2 = \frac{1}{16} \cdot 16 = 1

      Second integral:

      .. math::

         \frac{1}{4}\int_2^4 (4x^2 - x^3) \, dx = \frac{1}{4}\left[\frac{4x^3}{3} - \frac{x^4}{4}\right]_2^4

      .. math::

         = \frac{1}{4}\left[\left(\frac{256}{3} - 64\right) - \left(\frac{32}{3} - 4\right)\right]

      .. math::

         = \frac{1}{4}\left[\frac{256 - 192}{3} - \frac{32 - 12}{3}\right] = \frac{1}{4}\left[\frac{64}{3} - \frac{20}{3}\right] = \frac{1}{4} \cdot \frac{44}{3} = \frac{11}{3}

      .. math::

         E[X^2] = 1 + \frac{11}{3} = \frac{14}{3} \approx 4.67

      **Part (e): Var(X)**

      .. math::

         \text{Var}(X) = E[X^2] - (E[X])^2 = \frac{14}{3} - 4 = \frac{14 - 12}{3} = \frac{2}{3}

      .. math::

         \sigma_X = \sqrt{\frac{2}{3}} = \frac{\sqrt{2}}{\sqrt{3}} = \frac{\sqrt{6}}{3} \approx 0.816 \text{ KB}

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig7_triangular_pdf.png
         :alt: Triangular PDF symmetric about x = 2
         :align: center
         :width: 80%

         The triangular PDF is symmetric about :math:`x = 2`, so :math:`E[X] = 2` by symmetry.

----

.. admonition:: Exercise 8: LOTUS with Non-Polynomial Functions
   :class: note

   A materials scientist models crack length :math:`X` (in mm) with the uniform PDF:

   .. math::

      f_X(x) = 
      \begin{cases}
      1, & 0 \leq x \leq 1\\
      0, & \text{elsewhere}
      \end{cases}

   a. Find :math:`E[X]` and :math:`\text{Var}(X)`.

   b. Find :math:`E[\sqrt{X}]` using LOTUS.

   c. Find :math:`E[e^X]` using LOTUS.

   d. Compare :math:`E[\sqrt{X}]` with :math:`\sqrt{E[X]}`. Which is larger and why?

   e. Compare :math:`E[e^X]` with :math:`e^{E[X]}`. Which is larger and why?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): E[X] and Var(X)**

      .. math::

         E[X] = \int_0^1 x \cdot 1 \, dx = \frac{x^2}{2} \Bigg|_0^1 = \frac{1}{2}

      .. math::

         E[X^2] = \int_0^1 x^2 \cdot 1 \, dx = \frac{x^3}{3} \Bigg|_0^1 = \frac{1}{3}

      .. math::

         \text{Var}(X) = E[X^2] - (E[X])^2 = \frac{1}{3} - \frac{1}{4} = \frac{1}{12}

      **Part (b): E[‚àöX] using LOTUS**

      .. math::

         E[\sqrt{X}] = \int_0^1 \sqrt{x} \cdot 1 \, dx = \int_0^1 x^{1/2} \, dx = \frac{x^{3/2}}{3/2} \Bigg|_0^1 = \frac{2}{3}

      **Part (c): E[eÀ£] using LOTUS**

      .. math::

         E[e^X] = \int_0^1 e^x \cdot 1 \, dx = e^x \Bigg|_0^1 = e - 1 \approx 1.718

      **Part (d): Compare E[‚àöX] with ‚àöE[X]**

      - :math:`E[\sqrt{X}] = \frac{2}{3} \approx 0.667`
      - :math:`\sqrt{E[X]} = \sqrt{\frac{1}{2}} = \frac{1}{\sqrt{2}} \approx 0.707`

      :math:`E[\sqrt{X}] < \sqrt{E[X]}`

      **Explanation**: The square root function :math:`g(x) = \sqrt{x}` is **concave** (curves downward). For concave functions, we have:

      .. math::

         E[g(X)] \leq g(E[X])

      Intuitively: the square root "compresses" larger values more than smaller values, so averaging first (then taking the root) gives a higher result than taking roots first (then averaging).

      **Part (e): Compare E[eÀ£] with e^{E[X]}**

      - :math:`E[e^X] = e - 1 \approx 1.718`
      - :math:`e^{E[X]} = e^{1/2} = \sqrt{e} \approx 1.649`

      :math:`E[e^X] > e^{E[X]}`

      **Explanation**: The exponential function :math:`g(x) = e^x` is **convex** (curves upward). For convex functions, we have:

      .. math::

         E[g(X)] \geq g(E[X])

      Intuitively: the exponential "amplifies" larger values more than smaller values, so the average of exponentials exceeds the exponential of the average.

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig8a_jensen_concave.png
         :alt: Concave square root function showing E[g(X)] less than g(E[X])
         :align: center
         :width: 70%

         **Concave function** :math:`g(x) = \sqrt{x}`: :math:`E[g(X)] \leq g(E[X])`.

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig8b_jensen_convex.png
         :alt: Convex exponential function showing E[g(X)] greater than g(E[X])
         :align: center
         :width: 70%

         **Convex function** :math:`g(x) = e^x`: :math:`E[g(X)] \geq g(E[X])`.

----

.. admonition:: Exercise 9: Variance of a Transformed Variable
   :class: note

   A computer scientist models algorithm runtime :math:`X` (in seconds) with PDF:

   .. math::

      f_X(x) = 
      \begin{cases}
      2x, & 0 \leq x \leq 1\\
      0, & \text{elsewhere}
      \end{cases}

   a. Find :math:`E[X]`, :math:`E[X^2]`, and :math:`\text{Var}(X)`.

   b. The cost function is :math:`C = 5X + 10` dollars. Find :math:`E[C]` and :math:`\text{Var}(C)`.

   c. A quadratic cost model uses :math:`Q = 3X^2`. Find :math:`E[Q]`.

   d. For the quadratic cost :math:`Q = 3X^2`, find :math:`\text{Var}(Q)`. (Hint: You need :math:`E[X^4]`.)

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Basic moments**

      .. math::

         E[X] = \int_0^1 x \cdot 2x \, dx = 2\int_0^1 x^2 \, dx = 2 \cdot \frac{1}{3} = \frac{2}{3}

      .. math::

         E[X^2] = \int_0^1 x^2 \cdot 2x \, dx = 2\int_0^1 x^3 \, dx = 2 \cdot \frac{1}{4} = \frac{1}{2}

      .. math::

         \text{Var}(X) = E[X^2] - (E[X])^2 = \frac{1}{2} - \frac{4}{9} = \frac{9 - 8}{18} = \frac{1}{18}

      **Part (b): Linear cost C = 5X + 10**

      Using linearity of expectation:

      .. math::

         E[C] = E[5X + 10] = 5E[X] + 10 = 5 \cdot \frac{2}{3} + 10 = \frac{10}{3} + 10 = \frac{40}{3} \approx \$13.33

      Using variance of linear transformation:

      .. math::

         \text{Var}(C) = \text{Var}(5X + 10) = 5^2 \cdot \text{Var}(X) = 25 \cdot \frac{1}{18} = \frac{25}{18} \approx 1.39

      .. math::

         \sigma_C = \sqrt{\frac{25}{18}} = \frac{5}{3\sqrt{2}} \approx \$1.18

      **Part (c): Quadratic cost Q = 3X¬≤**

      Note: :math:`Q` is defined as a cost in dollars, where the coefficient 3 carries units of dollars/second¬≤ to ensure dimensional consistency.

      Using LOTUS:

      .. math::

         E[Q] = E[3X^2] = 3 \cdot E[X^2] = 3 \cdot \frac{1}{2} = \frac{3}{2} = \$1.50

      **Part (d): Var(Q) for Q = 3X¬≤**

      We need :math:`E[Q^2] = E[9X^4] = 9E[X^4]`.

      First, find :math:`E[X^4]`:

      .. math::

         E[X^4] = \int_0^1 x^4 \cdot 2x \, dx = 2\int_0^1 x^5 \, dx = 2 \cdot \frac{1}{6} = \frac{1}{3}

      Now:

      .. math::

         E[Q^2] = 9 \cdot E[X^4] = 9 \cdot \frac{1}{3} = 3

      .. math::

         \text{Var}(Q) = E[Q^2] - (E[Q])^2 = 3 - \left(\frac{3}{2}\right)^2 = 3 - \frac{9}{4} = \frac{3}{4}

      .. math::

         \sigma_Q = \sqrt{\frac{3}{4}} = \frac{\sqrt{3}}{2} \approx \$0.87

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch6-2/fig9_variance_shortcut.png
         :alt: Variance computational shortcut illustration
         :align: center
         :width: 80%

         The variance shortcut: :math:`\text{Var}(X) = E[X^2] - (E[X])^2`. Note that :math:`E[X^2] > (E[X])^2` always (unless Var = 0).

----

Additional Practice Problems
----------------------------

**True/False Questions** (1 point each)

1. For a continuous random variable, :math:`E[X^2] = (E[X])^2`.

   ‚ìâ or ‚íª

2. If :math:`X` has PDF symmetric about :math:`x = 5`, then :math:`E[X] = 5`.

   ‚ìâ or ‚íª

3. For any random variable :math:`X` and constant :math:`c`, :math:`\text{Var}(X + c) = \text{Var}(X)`.

   ‚ìâ or ‚íª

4. If :math:`X` and :math:`Y` are independent, then :math:`\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)`.

   ‚ìâ or ‚íª

5. :math:`E[3X + 2] = 3E[X] + 2` is an example of the linearity of expectation.

   ‚ìâ or ‚íª

6. For any function :math:`g(x)`, :math:`E[g(X)] = g(E[X])`.

   ‚ìâ or ‚íª

7. Variance can never be negative.

   ‚ìâ or ‚íª

8. If :math:`\text{Var}(X) = 9`, then :math:`\text{Var}(2X) = 18`.

   ‚ìâ or ‚íª

**Multiple Choice Questions** (2 points each)

9. For :math:`f_X(x) = 2x` on :math:`[0, 1]`, what is :math:`E[X]`?

   ‚í∂ 1/3
   
   ‚í∑ 1/2
   
   ‚í∏ 2/3
   
   ‚íπ 3/4

10. If :math:`E[X] = 4` and :math:`E[X^2] = 20`, what is :math:`\text{Var}(X)`?

    ‚í∂ 4
    
    ‚í∑ 16
    
    ‚í∏ 20
    
    ‚íπ 36

11. If :math:`\text{Var}(X) = 5`, what is :math:`\text{Var}(3X - 7)`?

    ‚í∂ 5
    
    ‚í∑ 8
    
    ‚í∏ 15
    
    ‚íπ 45

12. If :math:`X` and :math:`Y` are independent with :math:`\text{Var}(X) = 3` and :math:`\text{Var}(Y) = 5`, what is :math:`\text{Var}(X + Y)`?

    ‚í∂ 2
    
    ‚í∑ 8
    
    ‚í∏ 15
    
    ‚íπ 64

13. For :math:`f_X(x) = 3x^2` on :math:`[0, 1]`, what is :math:`E[X^2]`?

    ‚í∂ 1/2
    
    ‚í∑ 3/5
    
    ‚í∏ 3/4
    
    ‚íπ 4/5

14. Which property allows us to compute :math:`E[X^2]` directly from :math:`f_X(x)` without finding the PDF of :math:`X^2`?

    ‚í∂ Linearity of expectation
    
    ‚í∑ Additivity of variance
    
    ‚í∏ Law of the Unconscious Statistician (LOTUS)
    
    ‚íπ Variance shortcut formula

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **True/False Answers:**

   1. **False** ‚Äî In general, :math:`E[X^2] \geq (E[X])^2`. Equality holds only when :math:`\text{Var}(X) = 0` (i.e., :math:`X` is a constant).

   2. **True** ‚Äî Symmetry about :math:`x = c` implies :math:`E[X] = c` (the balance point).

   3. **True** ‚Äî Adding a constant shifts all values but doesn't change the spread. :math:`\text{Var}(X + c) = \text{Var}(X)`.

   4. **True** ‚Äî For independent random variables, variances add: :math:`\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)`.

   5. **True** ‚Äî This is exactly the linearity property: :math:`E[aX + b] = aE[X] + b`.

   6. **False** ‚Äî In general, :math:`E[g(X)] \neq g(E[X])` unless :math:`g` is linear. This is why LOTUS is needed.

   7. **True** ‚Äî Variance is :math:`E[(X - \mu)^2]`, an expected value of squared terms, which cannot be negative.

   8. **False** ‚Äî :math:`\text{Var}(2X) = 2^2 \cdot \text{Var}(X) = 4 \times 9 = 36`, not 18.

   **Multiple Choice Answers:**

   9. **‚í∏** ‚Äî :math:`E[X] = \int_0^1 x \cdot 2x \, dx = 2 \cdot \frac{1}{3} = \frac{2}{3}`.

   10. **‚í∂** ‚Äî :math:`\text{Var}(X) = E[X^2] - (E[X])^2 = 20 - 16 = 4`.

   11. **‚íπ** ‚Äî :math:`\text{Var}(3X - 7) = 3^2 \cdot \text{Var}(X) = 9 \times 5 = 45`.

   12. **‚í∑** ‚Äî For independent RVs: :math:`\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) = 3 + 5 = 8`.

   13. **‚í∑** ‚Äî :math:`E[X^2] = \int_0^1 x^2 \cdot 3x^2 \, dx = 3 \cdot \frac{1}{5} = \frac{3}{5}`.

   14. **‚í∏** ‚Äî LOTUS (Law of the Unconscious Statistician) allows :math:`E[g(X)] = \int g(x) f_X(x) \, dx` directly.