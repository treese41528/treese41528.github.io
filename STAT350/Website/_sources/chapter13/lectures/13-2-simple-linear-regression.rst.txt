.. _13-2-simple-linear-regression:

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch13-3">
      <iframe
         id="video-ch13-3"
         title="STAT 350 â€“ Chapter 13.3 Simple Linear Regression Model Video"
         src="https://www.youtube.com/embed/a9skiqjau8I?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides ðŸ“Š
   :class: tip

   `Download Chapter 13 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%2013%20Linear%20Regression/SimpleLinearRegression_AC.pptx>`_
   
Simple Linear Regression
=======================================================

After identifying a linear association between two quantitative variables via a scatter plot,
we proceed to mathematical modeling of the association. This involves formalizing the population model, developing methods 
to estimate the model parameters, and assessing the model's fit to the data.

.. admonition:: Road Map ðŸ§­
   :class: important

   * Understand the implications of linear regression **model assumptions**. 
   * Derive the least squares **estimates of the slope and intercept parameters**, and present the result as a
     fitted regression line.
   * Use the **ANOVA table** to decompose the sources of variability in the repsonse into
     error and model components.
   * Use the **coefficient of determination** and the **sample correlation** as assisting numerical summaries.

.. _assumptions_LR:

The Simple Linear Regression Model
---------------------------------------------------

In this course, we assume that the :math:`X` values are **given (non-random)** as :math:`x_1, x_2, \cdots, x_n`. 
For each explanatory value, the corresponding response :math:`Y_i` is generated from the population model:

.. math::
   :label: regression-model

   Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i,

where :math:`\varepsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)`, and 
:math:`\beta_0` and :math:`\beta_1` are the intercept and slope of the *true* linear association.

Important Implications of the Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. The Mean Response
^^^^^^^^^^^^^^^^^^^^^^^
:math:`y=\beta_0 + \beta_1 x` represents the **mean response line**, or the *true* relationship between the explanatory and response variables. That is,

.. math::
   E[Y|X=x] &= E[\beta_0 + \beta_1 x + \varepsilon] \\
   &= \beta_0 + \beta_1 x + E[\varepsilon]\\
   &= \beta_0 + \beta_1 x,

for any given :math:`x` values in an appropriate range.
:math:`\beta_0`, :math:`\beta_1`, and :math:`x` pass through the expectation operator 
unchanged since they are constants. The error term has expected value zero, so it disappears. 

2. The Error Term
^^^^^^^^^^^^^^^^^^^^^^

The error term :math:`\varepsilon` represents the **random variation** in :math:`Y` that is not explained by the 
mean response line. With non-random explanatory values, this is the only source of randomness in :math:`Y`. Namely,

.. math::
   \text{Var}(Y|X=x) = \text{Var}(\beta_0 + \beta_1 x + \varepsilon) = \text{Var}(\varepsilon) = \sigma^2.

Note that the result does not depend on the index :math:`i` or the value of :math:`x_i`. In our model, we assume
that the true spread of :math:`Y` values around the regression line **remains constant** regardless of the :math:`X` value.

3. The Conditional distribution of :math:`Y_i`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

With the explanatory value fixed as :math:`x_i`, each :math:`Y_i` is a linear function of a normal random variable :math:`\varepsilon_i`.
This implies that :math:`Y_i|X=x_i` is also normally distributed. Combining this result with the previous two observations,
we can write the exact conditional distribution of the response as:

.. math::
   Y_i|X=x_i \sim N(\beta_0 + \beta_1 x_i, \quad \sigma^2)



.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/y-conditional-dist.png
   :width: 50%
   :align: center 
   :alt: Visualization of conditionals distributions of :math:`Y_i`
   
   Visualization of conditional distributions of :math:`Y_i` given :math:`x_i`


Summary of the Essential Assumptions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following four assumptions are equivalent to the model assumptions discussed above and are 
required to ensure validity of subsequent analysis procedures.

* **Assumption 1: Independence**

  For each given value :math:`x_i`, :math:`Y_i` is a size-1 simple random sample from the distribution
  of :math:`Y|X=x_i`. Further, the pairs :math:`(x_i, Y_i)` are independent from all other pairs 
  :math:`(x_j, Y_j)`, :math:`j\neq i`.

* **Assumption 2: Linearity**

  The association between the explanatory variable and the response is linear on average.

* **Assumption 3: Normality**

  The errors are normally distributed:

  .. math::
      \varepsilon_i \stackrel{iid}{\sim} N(0, \sigma^2) \quad \text{for } i = 1, 2, \ldots, n

* **Assumption 4: Equal Variance**

  The error terms have constant variance :math:`\sigma^2` across all values of :math:`X`. 

Point Estimates of Slope and Intercept
-------------------------------------------

We now develop the point estimates for the unknown intercept and slope parameters :math:`\beta_0` and :math:`\beta_1`
using the **least squares** method. The estimators are chosen so that the resulting trend line yields the smallest possible 
overall squared distance from the observed response values.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/least-squares.png
   :width: 50%
   :align: center 
   :alt: Distances between observed reposnes and the fitted line
   
   Distances between observed reponses and the fitted line

The goal is mathematically equivalent to finding the arguments that minimize:

.. math::
   :label: ls-objective

   g(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2

To find their explicit forms, we take the partial derivative of Eq. :eq:`ls-objective` with respect to each parameter and set it 
equal to zero. Then we solve the resulting system of equations for :math:`\hat{\beta}_0` and :math:`\hat{\beta}_1`.

Derivation of the Intercept Estimate
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Take the partial derivative of :math:`g(\beta_0, \beta_1)` with respect to :math:`\beta_0`:

.. math::

   \frac{\partial g}{\partial \beta_0} &= \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2\\
   &= -2 \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)\\
   &= \sum_{i=1}^n y_i - n\beta_0 - \beta_1 \sum_{i=1}^n x_i

By setting this equal to zero and solving for :math:`\beta_0`, we obtain:

.. math::
   :label: b0

   \hat{\beta}_0 = \frac{1}{n}\sum_{i=1}^n y_i - \beta_1 \frac{1}{n}\sum_{i=1}^n x_i = \bar{y} - \beta_1 \bar{x}.

Derivation of the Slope Estimate
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Likewise, we take the partial derivative of :math:`g(\beta_0, \beta_1)` with respect to :math:`\beta_1`:

.. math::

   \frac{\partial g}{\partial \beta_1} &= -2 \sum_{i=1}^n x_i(y_i - \beta_0 - \beta_1 x_i)\\
   &= -2 \left(\sum_{i=1}^n x_i y_i - \beta_0 \sum_{i=1}^n x_i - \beta_1 \sum_{i=1}^n x_i^2 \right)

Substitute :math:`\hat{\beta}_0` (Eq. :eq:`b0`) for :math:`\beta_0` and set equal to zero:

.. math::

   0 &= \sum_{i=1}^n x_i y_i - (\bar{y}-\beta_1 \bar{x})\sum_{i=1}^n x_i - \beta_1 \sum_{i=1}^n x_i^2\\
   &= \sum_{i=1}^n x_i y_i - (\bar{y}-\beta_1 \bar{x})n\bar{x} - \beta_1 \sum_{i=1}^n x_i^2\\
   &= \sum_{i=1}^n x_i y_i - n\bar{x}\bar{y} - \beta_1 (\sum_{i=1}^n x_i^2 - n \bar{x}^2)

Isolate :math:`\beta_1` to obtain:


.. math::
   :label: slope-estimate

   \hat{\beta}_1 = \frac{\sum_{i=1}^n x_i y_i - n\bar{x}\bar{y}}{\sum_{i=1}^n x_i^2 - n\bar{x}^2}

Summary
~~~~~~~~~~~

**Slope estimate**:

.. math::

   \hat{\beta}_1 = \frac{\sum_{i=1}^n x_i y_i - n\bar{x}\bar{y}}{\sum_{i=1}^n x_i^2 - n\bar{x}^2}
  
**Intercept estimate**:

.. math::
   \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}

.. admonition:: Alternative Notation
   :class: important

   The symbols :math:`b_0` and :math:`b_1` are used interchangeably with :math:`\hat{\beta}_0` and :math:`\hat{\beta}_1`,
   respectively.

Alternative Expressions of the Slope Estimate
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Several alternative expressions exist for the slope estimate :math:`\hat{\beta}_1`, each offering 
a different perspective on its relation to various components of regression analysis. 
Being able to transition freely between these forms is key to deepening the intuition for linear regression and
enabling efficient computation.

Define the sum of cross products and the sum of sqaures of :math:`X` as:

- :math:`S_{XY} = \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})`
- :math:`S_{XX} = \sum_{i=1}^n (x_i - \bar{x})^2`

In addition, the sample covariance of :math:`X` and :math:`Y` is denoted :math:`s_{XY}` (lower case :math:`s`) and defined
as:

.. math::

   s_{XY} = \frac{ \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{n-1}

Also recall that :math:`s_X^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}` denotes 
the sample variance of :math:`x_1, \cdots, x_n`.

Then :math:`\hat{\beta}_1` :eq:`slope-estimate` can also be written as:

.. math::
   :label: slope-estimate-alternative

   \hat{\beta}_1 = \frac{S_{XY}}{S_{XX}} = \frac{s_{XY}}{s^2_X} 
   = \frac{\sum_{i=1}^n (x_i -\bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}

.. admonition:: Exercise: Prove equality between the alternative expressions
   :class: important

   **Hint:**  Showing the second and third equality of :eq:`slope-estimate-alternative` is relatively simple. To show
   the first equality, begin with the numerators:

   .. math:: 
      \sum_{i=1}^n (x_i -\bar{x})(y_i - \bar{y}) &= \sum_{i=1}^n x_i y_i -\bar{x}\sum_{i=1}^n y_i -\bar{y}\sum_{i=1}^n x_i + n\bar{x}\bar{y}\\
      &= \sum_{i=1}^n x_i y_i -\bar{x}(n\bar{y}) -\bar{y}(n\bar{x}) + n\bar{x}\bar{y}\\
      &= \sum_{i=1}^n x_i y_i -n\bar{x}\bar{y} \quad âœ…

   Repeat a similar procedure for the denominators.



The Fitted Regression Line
-----------------------------------------------

Once the slope and intercept estimates are obtained, we can use them to construct the **fitted regression line**:

.. math::

   \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x.

By plugging in specific :math:`x` values into the equation, we obtain point estimates of the corresponding responses.
The **hat notation is essential**, as it distinguishes these estimates from the observed responses, :math:`y`.

Properties of the Fitted Regression Line
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**1. Interpretation of the Slope**

The slope :math:`\hat{\beta}_1` represents the estimated average change in the response for every one-unit change in the 
explanatory variable. The sign of :math:`\hat{\beta}_1` indicates the direction of the association.

**2. Interpretation of the Intercept**

The intercept :math:`\hat{\beta}_0` represents the estimated average value of the response when the explanatory variable 
equals zero. However, this may not have significance if :math:`X = 0` is outside the range of the data or 
not practically meaningful.

**3. The Line Passes Through** :math:`(\bar{x}, \bar{y})`

If we substitute :math:`x = \bar{x}` into the fitted regression equation:

.. math::

   \hat{y} = \hat{\beta}_0 + \hat{\beta}_1\bar{x} = (\bar{y} - \hat{\beta}_1\bar{x}) + \hat{\beta}_1\bar{x} = \bar{y}.

That is, a fitted regression line always passes through the point :math:`(\bar{x}, \bar{y})`.

**4. Non-exchangeability**

If we swap the explanatory and response variables and refit the model, the resulting fitted line will not be the same as the
original, nor will the new slope be the algebraic inverse of the original. 

.. admonition:: Example ðŸ’¡: Blood Pressure Study
   :class: note

   A new treatment for high blood pressure is being assessed for feasibility. In an early trial, 11 subjects had 
   their blood pressure measured before and after treatment. Researchers want to determine if there is a linear association 
   between patient age and the change in systolic blood pressure after 24 hours.
   Using the data and the summary statistics below, compute and interpret the fitted regression line.
   
   **Variable Definitions**:

   - **Explanatory variable (X)**: Age of patient (years)
   - **Response variable (Y)**: Change in blood pressure = (After treatment) - (Before treatment)

   **Data**:

   .. flat-table::
      :header-rows: 1
      :align: center
      :width: 70%

      * - :math:`i`
        - Age (:math:`x_i`)
        - Î”BP (:math:`y_i`)
      * - 1
        - 70
        - -28
      * - 2
        - 51
        - -10
      * - 3
        - 65
        - -8
      * - 4
        - 70
        - -15
      * - 5
        - 48
        - -8
      * - 6
        - 70
        - -10
      * - 7
        - 45
        - -12
      * - 8
        - 48
        - 3
      * - 9
        - 35
        - 1
      * - 10
        - 48
        - -5
      * - 11
        - 30
        - 8

   **Scatter Plot**:

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/age-bp.png
      :width: 85%
      :align: center 
      :alt: Scatter plot of age vs blood pressure data

      Scatter plot of age vs blood pressure data

   **Summary Statistics**:

   - :math:`n=11`
   - :math:`\bar{x} = 52.7273` years
   - :math:`\bar{y} = -7.6364` mm Hg
   - :math:`\sum x_i y_i = -5485`
   - :math:`\sum x_i^2 = 32588`

   **Slope Calculation**:

   .. math::

      S_{XY} &= \sum x_i y_i - n\bar{x}\bar{y} \\
      &= -5485 - 11(52.7273)(-7.6364) \\
      &= -1055.8857\\
      &\quad\\
      S_{XX} &= \sum x_i^2 - n\bar{x}^2\\
      &= 32588 - 11(52.7273)^2 \\
      &= 2006.1502

   .. math::

      b_1 = \frac{S_{xy}}{S_{xx}} = \frac{-1055.8857}{2006.1502} = -0.5263

   **Intercept Calculation**:

   .. math::

      b_0 = \bar{y} - b_1\bar{x} = -7.6364 - (-0.5263)(52.7273) = 20.114

   **Fitted Regression Line**:

   .. math::

      \hat{y} = 20.114 - 0.526x

   **Interpretation**:

   - For each additional year of age, the change in blood pressure decreases by an average of 0.526 mm Hg.
   - The intercept estimate has no practical meaning since we do not study newborns for blood pressure treatment.
   - The negative slope suggests that older patients benefit more from the treatment.
 
------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch13-4">
      <iframe
         id="video-ch13-4"
         title="STAT 350 â€“ Chapter 13.4 Simple Linear Regression ANOVA Table and Coefficient of Determination Video"
         src="https://www.youtube.com/embed/nD9hKHIaUIQ?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

The ANOVA Table for Regression
-----------------------------------

Just as in ANOVA, we can decompose the total variability in the response variable into 
two components, one arising from the model structure and the other from random error. 

Total Variability: SST
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/SST.png
   :width: 70%
   :alt: Components of SST
   :align: center

   SST is the sum of the squared lengths of all the red dotted lines. They measure the distances of
   the response values from an overall mean.

.. math::
   :label: SST

   \text{SST} = \sum_{i=1}^n (y_i - \bar{y})^2

SST measures how much the response values deviate from their overall mean, ignoring the explanatory variable.
SST can also be denoted as :math:`S_{YY}`.
The degrees of freedom associated with :math:`\text{SST}` is :math:`df_T = n-1`.

.. admonition:: Computational Shortcut for SST
   :class: important 

   While Eq. :eq:`SST` conveys its intuitive meaning, it is often more convenient to use the following equivalent
   formula for computation:

   .. math::

      \text{SST} = \sum_{i=1}^n y_i^2 - n\bar{y}^2

Model Variability: SSR and MSR
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/SSR.png
   :width: 70%
   :alt: Components of SSR
   :align: center

   SSR is the sum of the squared lengths of all the red dotted lines. They measure the distances of
   the predicted :math:`\hat{y}` values from an overall mean.

The variability in :math:`Y` arising from its linear association with :math:`X` is measured with
the regression sum of squares, or SSR:

.. math::

   \text{SSR} = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2.

SSR expresses how much the fitted values deviate from the overall mean. Its associated degrees of freedom is
denoted :math:`df_R`, and it is equal to the number of explanatory variables. In simple linear regression,
:math:`df_R` is always equal to 1. Therefore, 

.. math::

   \text{MSR} = \frac{\text{SSR}}{df_R} = \text{SSR}.

.. admonition:: Computational Shortcut for SSR
   :class: important 

   Using the expanded formula of :math:`\hat{y}_i` and :math:`\hat{\beta}_0`,

   .. math::

      \text{SSR} &= \sum_{i=1}^n (\hat{\beta}_0 + \hat{\beta}_1x_i - \bar{y})^2\\
      &= \sum_{i=1}^n ((\bar{y}-\hat{\beta}_1\bar{x}) + \hat{\beta}_1x_i - \bar{y})^2\\
      &= \hat{\beta}_1^2\sum_{i=1}^n (x_i -\bar{x})^2\\
      &= \hat{\beta}_1\frac{S_{XY}}{S_{XX}} S_{XX}
      = \hat{\beta}_1 S_{XY}

   The final equality uses the definition of :math:`S_{XX}` and the fact that :math:`\hat{\beta}_1 = \frac{S_{XY}}{S_{XX}}`.
   The resulting equation 

   .. math::
      \text{SSR} = \hat{\beta}_1 S_{XY}
      
   is convenient for computaion if :math:`\hat{\beta}_1` and :math:`S_{XY}` are available.

Variability Due to Random Error: SSE and MSE
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/SSE.png
   :width: 70%
   :alt: Components of SSE
   :align: center

   SSE is the sum of the squared lengths of all the red dotted lines. They measure the distances between
   the observed and predicted response values.

Finally, the variability in :math:`Y` arising from random error is measured with
the error sum of squares, or SSE:

.. math::

   \text{SSE} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n e_i^2

**The Residuals**

Each difference :math:`e_i = y_i - \hat{y}_i` is called the :math:`i`-th **residual**. Residuals serve as proxies for
the unobserved true error terms :math:`\varepsilon_i`.

**Degrees of Freedom and MSE**

The degrees of freedom associated with SSE is :math:`df_E = n-2` because its formula involves two 
estimated parameters (:math:`b_0` and :math:`b_1`).
Readjusting the scale by the degrees of freedom,

.. math::
   \text{MSE} = \frac{\text{SSE}}{n-2}.

**MSE as Variance Estimator**

:math:`\text{MSE}` is a mean of squared distances of :math:`y_i` from their estimated means, :math:`\hat{y_i}`.
Therefore, we use it as an estimator of :math:`\sigma^2`:

.. math::
   \hat{\sigma}^2 = MSE

The Fundamental Identity
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Just like in ANOVA, the total sum of squares and the total degrees freedom deompose 
into their respective model and error components:

.. math::

   \text{SST} = \text{SSR} + \text{SSE}  \quad \text{ and } \quad df_T = df_R + df_E.

This can be proven algebraically by adding and subtracting :math:`\hat{y}_i` in the expression for 
SST and using properties of least squares. You are encouraged to show this as an independent exercise.

Partial ANOVA Table for Simple Linear Regression
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. list-table:: 
   :header-rows: 1
   :widths: 15 15 25 15 10 10

   * - Source
     - df
     - Sum of Squares
     - Mean Square
     - F-statistic
     - p-value
   * - Regression
     - 1
     - :math:`\sum_{i=1}^n (\hat{y}_i - \bar{y})^2`
     - :math:`\frac{\text{SSR}}{1}`
     - ?
     - ?
   * - Error
     - :math:`n-2`
     - :math:`\sum_{i=1}^n (y_i - \hat{y}_i)^2`
     - :math:`\frac{\text{SSE}}{n-2}`
     - 
     - 
   * - Total
     - :math:`n-1`
     - :math:`\sum_{i=1}^n (y_i - \bar{y})^2`
     - 
     - 
     - 

We will discuss how to compute and use the :math:`F`-statistic and :math:`p`-value in the upcoming lesson.

.. admonition:: Example ðŸ’¡: Blood Pressure Study, Continued
   :class: note 

   The summary statistics and the model parameter estimates computed in the previous example 
   for the blood pressure study are listed below:

   - :math:`n=11`
   - :math:`\bar{x} = 52.7273` years
   - :math:`\bar{y} = -7.6364` mm Hg
   - :math:`\sum x_i y_i = -5485`
   - :math:`\sum x_i^2 = 32588`
   - :math:`\sum y_i^2 = 1580`
   - :math:`S_{XY} = -1055.8857`
   - :math:`S_{XX}=2006.1502`
   - :math:`b_0 = 20.114`
   - :math:`b_1 = -0.5263`


   **Q1**: Predict the change in blood pressure for
   a patient who is 65 years old. Compute the corresponding residual.

   The predicted change is

   .. math:: 
      \hat{y} = 20.11 - 0.526(65) = -14.0 \text{ mm Hg}

   The observed response value for :math:`x=65` is :math:`y=-8` from the data table. Therefore, the residual is

   .. math::
      e = -8 - (-14) = 6 \text{ mm Hg}

   **Q2**: Complete the ANOVA table for the linear regression model between age and change in blood pressure.

   For each sum of squares, we will use the computational shortcut rather than the definition:

   .. math::

      &\text{SSR} = b_1 S_{xy} = (-0.5263)(-1055.8857) = 555.7126\\
      &\text{SST} = \sum y_i^2 - n\bar{y}^2 = 1580 - 11(-7.6364)^2 = 938.5393

   Finally, using the decomposition identity of the sums of squares,

   .. math::

      \text{SSE} = \text{SST} - \text{SSR} = 938.5393 - 555.7126 = 382.8267

   The mean squares are:

   .. math::
      & \text{MSR} = \frac{\text{SSR}}{1} = 555.7126\\
      & \text{MSE} = \frac{\text{SSE}}{9} = 42.5363
   
   The results are organized into a table below: 
   
   .. list-table:: 
      :header-rows: 1
      :widths: 10 5 10 10 10 10

      *  - Source
         - df
         - Sum of Squares
         - Mean Square
         - F-statistic
         - p-value
      *  - Regression
         - 1
         - :math:`555.7126`
         - :math:`555.7126`
         - ?
         - ?
      *  - Error
         - :math:`9`
         - :math:`382.8267`
         - :math:`42.5363`
         - 
         - 
      *  - Total
         - :math:`10`
         - :math:`938.5393`
         - 
         - 
         - 

The Coefficient of Determination (:math:`R^2`)
--------------------------------------------------

The coefficient of determination, denoted :math:`R^2`, provides a single numerical measure of how well our 
regression line fits the data. It is defined as:

.. math::

   R^2 = \frac{\text{SSR}}{\text{SST}}.

Due to the fundamental identity :math:`\text{SST} = \text{SSR} + \text{SSE}` and the fact that
each component is non-negative, :math:`R^2` is always between 0 and 1.

It is interpreted as the **fraction of the response variability 
that is explained by its linear association with the explanatory variable.**

:math:`R^2` approaches 1 when:

- :math:`\text{SSR}` approaches :math:`\text{SST}`.
- The residuals :math:`y_i - \hat{y}_i` have small magnitudes.
- The regression line captures most of the variability in the response.
- Points gather tightly around the fitted line.

:math:`R^2` approaches 0 when:

- :math:`\text{SSE}` approaches :math:`\text{SST}`.
- The residuals have large magnitudes.
- The explanatory variable provides little information about the response.
- Points scatter widely around the fitted line.

.. admonition:: Example ðŸ’¡: Blood Pressure Study, Continued
   :class: note 

   Compute and interpret the coefficient of determination from the blood pressure study.

   .. math::

      R^2 = \frac{\text{SSR}}{\text{SST}} = \frac{555.7126}{938.5393} = 0.5921

   **Interpretation**: Approximately 59.21% of the variation in blood pressure change is explained by the 
   linear relationship with patient age.

Important Limitations of :math:`R^2`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

While :math:`R^2` is a useful summary measure, it has important limitations that require careful interpretation.
These limitations are evident in the scatter plots of the well-known Anscome's quartet (:numref:`anscome's-quartet`). 
The figure presents four bivariate data sets that have distinct patterns yet are **identical in their** :math:`R^2` values:

.. _anscome's-quartet:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/anscome's-quartet.png
   :width: 60%
   :align: center 
   :alt: anscome's-quartet
   
   Anscome's Quartet

Let us point out some important observations from :numref:`anscome's-quartet`: 

1. A high :math:`R^2` value does not guarantee linaer association.
2. :math:`R^2` is not robust to outliers. Outliers can dramatically reduce or inflate :math:`R^2`.
3. A high :math:`R^2` does not automatically guarantee good prediction performance. 

   * Even when the true form is not linear, we might obtain a high :math:`R^2` if the sample size is small
     and the points happen to align well with a linear pattern.
   * Absolute scale of the observed errors also matter.
     Consider a scenario where :math:`\text{SSR} = 9 \times 10^6` and :math:`\text{SST} = 10^7`, 
     giving :math:`R^2 = 0.90`. While 90% of the total variation is explained, :math:`\text{SSE} = 10^6` indicates 
     substantial absolute errors that might make predictions unreliable for practical purposes.


Best Practices for Using the Coefficient of Determination
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Always examine scatter plots to verify model assumptions and check for outliers.
2. Use :math:`R^2` as one component of model assessment, not the sole criterion. 
   Consider factors such as sample size, scale, and practical significance.


----------------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch13-5">
      <iframe
         id="video-ch13-5"
         title="STAT 350 â€“ Chapter 13.5 Sample Pearson Correlation Coefficient Video"
         src="https://www.youtube.com/embed/qSG28mV6fx4?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

Sample Pearson Correlation Coefficient
----------------------------------------

Another numerical measure that proves useful in simple linear regression is the sample Pearson correlation coefficient. 
This provides a **standardized quantification of the direction and strength of a linear association.**
It is defined as: 

.. math::

   r = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2} \sqrt{\sum_{i=1}^n (y_i - \bar{y})^2}}.

As suggested by the structural similarity, :math:`r` is the sample equivalent of the population correlation introduced 
in the probability chapters:

.. math::
   :label: sample-corr-def

   \rho = \frac{E[(X - \mu_X)(Y - \mu_Y)]}{\sigma_X \sigma_Y}.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/correlation-plots.png
   :width: 90%
   :align: center 
   :alt: Scatter plots with different sample correlation values
   
   Scatter plots with different sample correlation values

Sample correlation is unitless and always falls between -1 and +1, with the sign
indicating the direction and the magnitude indicating the strength of linear association.
The bounded range makes it easy to compare correlations across different data sets.

We classify linear associations as strong, moderate or weak using the following rule of thumb:

- Strong association: :math:`0.8 <|r| \leq 1`
- Moderate association: :math:`0.5 <|r| \leq 8`  
- Weak association: :math:`0 \leq |r| \leq 0.5`  

Alternative Expressions of Sample Correlation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**1. Using Sample Covariance and Sample Standard Deviations** 

Using the sample covariance :math:`s_{XY} = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})` and 
the sample standard deviations :math:`s_X` and :math:`s_Y`, we can rewrite the definition of sample 
correlation :eq:`sample-corr-def` as:

.. math::
   :label: alt-sample-corr1

   r = \frac{s_{XY}}{s_X s_Y}.

This definition gives us an insight into what :math:`r` measures. Sample covariance is an *unscaled* measure 
of how :math:`X` and :math:`Y` move together. 

* If the two variables typically take values above or below their respective means
  simultaneously, most terms in the summation will be positive, increasing the chance that
  the mean of these terms, the sample covariance, is also positive.
* If the values tend to lie on opposite sides of their respective means,
  the sample covariance is likely negative.

Sample correlation is obtained by removing variable-specific scales from sample covariance, dividing it
by the sample standard deviations of both :math:`X` and :math:`Y`.

**2. Using Sums of Cross-products and Squares** 

The definition of sample correlation can also be expressed using the sums of cross-products and squares,
:math:`S_{XY}, S_{XX},` and :math:`S_{YY}`:

.. math::
   r = \frac{S_{XY}}{\sqrt{S_{XX}S_{YY}}}.

**3. Cross-products of Standardized Data**

Modifying the equation :eq:`alt-sample-corr1`,

.. math::
   r &=  \frac{\sum_{i=1}^n[(x_i - \bar{x})(y_i - \bar{y})]/(n-1)}{s_X s_Y}\\
   &= \frac{1}{n-1}\sum_{i=1}^n \left(\frac{x_i - \bar{x}}{s_X}\right)\left(\frac{y_i - \bar{y}}{s_Y}\right).

This gives us another perspective into what the correlation coefficient is really doingâ€”it is a mean of 
cross-products of the **standardized values** of the explanatory and response variables.

Connections with Other Components of Linear Regression
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Slope Estimate, :math:`b_1`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Both the correlation coefficient and the slope estimate capture information about a linear relationship. 
How exactly are they connected?

Recall that one way to write the slope estimate is:

.. math::

   b_1 = \frac{s_{XY}}{s_X^2}.

Using the correlation formula :math:`r = \frac{s_{XX}}{s_X s_Y}`, the slope estimate can be written as:

.. math::

   b_1 = \frac{r s_X s_Y}{s_X^2} = r \frac{s_Y}{s_X}

This relationship shows that the slope estimate is simply the **correlation coefficient rescaled by 
the ratio of standard deviations**. 

2. Coefficient of Determination, :math:`R^2`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Recall that:

* :math:`b_1 = \frac{S_{XY}}{S_{XX}}`
* :math:`\text{SSR} = b_1 S_{XY}` 
* :math:`\text{SST} = S_{YY}`

Combining these together,

.. math::

   R^2 = \frac{\text{SSR}}{\text{SST}} = \frac{b_1 S_{XY}}{S_{YY}} 
   = \frac{S_{XY}^2}{S_{XX}S_{YY}} = \left(\frac{S_{XY}}{\sqrt{S_{XX} S_{YY}}}\right)^2 = r^2.

In other words, the coefficient of determination is **equal** to the square of the correlation coefficient.
However, this special relationship **holds only for simple linear regression**, where the model has a single explanatory variable.

Limitations of Correlation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Correlation specifically measures the direction and strength of **linear** association. Therefore,
:math:`r = 0` indicates an absence of **linear** association, not necessarily an absence of association altogether.
Symmetrical non-linear relationships tend to produce sample correlation close to zero, as any linear trend one one side offsets
the trend on the other.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/zero-corr-plots.png
   :width: 90%
   :align: center 
   :alt: Scatter plots with zero sample correlation
   
   Scatter plots with zero sample correlation

Furthermore, extreme observations can significantly impact correlation values.
These limitations mean we need to examine our data visually before trusting correlation values, just as we do with :math:`R^2`. 

Bringing It All Together
-------------------------------

.. admonition:: Key Takeaways ðŸ“
   :class: important

   1. **The simple linear regression model** :math:`Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i` and its associated assumptions
      formalize the linear relationship between quantitative variables.

   2. The **least squares method** provides optimal estimates for the slope and intercept parameters by minimizing the sum of 
      squared differences between the observed data and the summary line.

   3. The variability components of linear regression can also be organized into an **ANOVA table**. 

   4. **Residuals** :math:`e_i = y_i - \hat{y}_i` estimate the unobserved error terms and enable 
      variance estimation through :math:`s^2 = \text{MSE} = \text{SSE}/(n-2)`.

   5. **The coefficient of determination** :math:`R^2 = \text{SSR}/\text{SST}` measures the proportion of total variation 
      in observed :math:`Y` that is explained by the regression model.

   6. **The sample Pearson correlation coefficient** :math:`r` provides the direction and strength of a linear relationship on a 
      standard numerical scale between -1 and 1.

   7. **Model assessment requires multiple tools**: scatter plots, residual analysis, ANOVA table, :math:`R^2`,  and :math:`r` all work 
      together to evaluate model appropriateness.


Exercises
---------

.. admonition:: Exercise 1: Least Squares Calculation
   :class: note

   An aerospace engineer is studying the relationship between altitude (X, in thousands of feet) and air density (Y, in kg/mÂ³) for aircraft performance modeling. The following data were collected:

   .. list-table::
      :header-rows: 1
      :widths: 20 20

      * - Altitude (000 ft)
        - Air Density (kg/mÂ³)
      * - 0
        - 1.225
      * - 5
        - 1.056
      * - 10
        - 0.905
      * - 15
        - 0.771
      * - 20
        - 0.653
      * - 25
        - 0.549

   Given summary statistics:

   - :math:`n = 6`, :math:`\bar{x} = 12.5`, :math:`\bar{y} = 0.8598`
   - :math:`\sum x_i = 75`, :math:`\sum y_i = 5.159`
   - :math:`\sum x_i^2 = 1375`, :math:`\sum x_i y_i = 52.68`

   a. Calculate :math:`S_{XX}` and :math:`S_{XY}`.

   b. Calculate the least squares estimate of the slope :math:`b_1`.

   c. Calculate the least squares estimate of the intercept :math:`b_0`.

   d. Write the fitted regression equation.

   e. Interpret the slope in context.

   f. Interpret the intercept in context. Is it meaningful?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Calculate S_XX and S_XY**

      .. math::

         S_{XX} = \sum x_i^2 - n\bar{x}^2 = 1375 - 6(12.5)^2 = 1375 - 937.5 = 437.5

      .. math::

         S_{XY} = \sum x_i y_i - n\bar{x}\bar{y} = 52.68 - 6(12.5)(0.8598) = 52.68 - 64.485 = -11.805

      **Part (b): Slope estimate**

      .. math::

         b_1 = \frac{S_{XY}}{S_{XX}} = \frac{-11.805}{437.5} = -0.02699

      **Part (c): Intercept estimate**

      .. math::

         b_0 = \bar{y} - b_1\bar{x} = 0.8598 - (-0.02699)(12.5) = 0.8598 + 0.3374 = 1.1972

      **Part (d): Fitted equation**

      :math:`\hat{y} = 1.1972 - 0.02699x`

      Or in context: Air Density = 1.1972 âˆ’ 0.02699(Altitude)

      **Part (e): Slope interpretation**

      For each additional 1,000 feet of altitude, air density decreases by approximately 0.027 kg/mÂ³.

      **Part (f): Intercept interpretation**

      At sea level (altitude = 0), the predicted air density is 1.1972 kg/mÂ³. This is meaningful and very close to the actual sea level air density of 1.225 kg/mÂ³, so the intercept has a valid physical interpretation.

      **R verification:**

      .. code-block:: r

         alt <- c(0, 5, 10, 15, 20, 25)
         density <- c(1.225, 1.056, 0.905, 0.771, 0.653, 0.549)
         fit <- lm(density ~ alt)
         coef(fit)  # Intercept: 1.197, alt: -0.02699

----

.. admonition:: Exercise 2: Properties of Least Squares Line
   :class: note

   Using the air density regression from Exercise 1:

   a. Verify that the fitted line passes through :math:`(\bar{x}, \bar{y})`.

   b. Calculate the predicted air density at 12,000 feet (x = 12).

   c. Calculate the residual for the observation at 10,000 feet (x = 10, y = 0.905).

   d. What is the sum of all residuals for a least squares regression line? Explain why.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Verification**

      At :math:`x = \bar{x} = 12.5`:

      :math:`\hat{y} = 1.1972 - 0.02699(12.5) = 1.1972 - 0.3374 = 0.8598 = \bar{y}` âœ“

      The fitted line passes through the point :math:`(\bar{x}, \bar{y}) = (12.5, 0.8598)`.

      **Part (b): Prediction at x = 12**

      :math:`\hat{y} = 1.1972 - 0.02699(12) = 1.1972 - 0.3239 = 0.8733` kg/mÂ³

      **Part (c): Residual at x = 10**

      First, calculate the fitted value:

      :math:`\hat{y} = 1.1972 - 0.02699(10) = 1.1972 - 0.2699 = 0.9273`

      Then the residual:

      :math:`e = y - \hat{y} = 0.905 - 0.9273 = -0.0223`

      The negative residual indicates the observed value is slightly below the fitted line.

      **Part (d): Sum of residuals**

      The sum of all residuals for a least squares line is always **zero** (or essentially zero due to rounding).

      This is a mathematical property of the least squares method: the normal equations guarantee that :math:`\sum_{i=1}^{n} e_i = 0`. Geometrically, the line passes through :math:`(\bar{x}, \bar{y})`, causing positive and negative residuals to balance exactly.

----

.. admonition:: Exercise 3: ANOVA Table Construction
   :class: note

   A chemical engineer studies the relationship between reaction temperature (Â°C) and product yield (%). With :math:`n = 15` observations, the regression analysis yields:

   - :math:`\bar{y} = 72.4`
   - :math:`b_0 = 35.2`, :math:`b_1 = 0.62`
   - :math:`SST = \sum_{i=1}^{15}(y_i - \bar{y})^2 = 2450`
   - :math:`SSR = \sum_{i=1}^{15}(\hat{y}_i - \bar{y})^2 = 1862`

   a. Calculate SSE using the relationship SST = SSR + SSE.

   b. Complete the ANOVA table with all degrees of freedom, sums of squares, mean squares, and F-statistic.

   c. Calculate the estimate of :math:`\sigma` (the standard deviation of errors).

   d. Calculate :math:`R^2` and interpret its meaning in context.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Calculate SSE**

      :math:`SSE = SST - SSR = 2450 - 1862 = 588`

      **Part (b): Complete ANOVA table**

      .. list-table::
         :header-rows: 1
         :widths: 20 10 15 15 10

         * - Source
           - df
           - SS
           - MS
           - F
         * - Regression
           - 1
           - 1862
           - 1862.00
           - 41.17
         * - Error
           - 13
           - 588
           - 45.23
           - 
         * - Total
           - 14
           - 2450
           - 
           - 

      Calculations:

      - :math:`df_{Reg} = 1` (simple linear regression)
      - :math:`df_{Error} = n - 2 = 15 - 2 = 13`
      - :math:`df_{Total} = n - 1 = 14`
      - :math:`MSR = SSR/df_{Reg} = 1862/1 = 1862`
      - :math:`MSE = SSE/df_{Error} = 588/13 = 45.23`
      - :math:`F = MSR/MSE = 1862/45.23 = 41.17`

      **Part (c): Estimate of Ïƒ**

      :math:`\hat{\sigma} = \sqrt{MSE} = \sqrt{45.23} = 6.725`

      This estimates the standard deviation of the random errors around the regression line.

      **Part (d): RÂ² and interpretation**

      :math:`R^2 = \frac{SSR}{SST} = \frac{1862}{2450} = 0.76 = 76\%`

      **Interpretation:** Approximately 76% of the variation in product yield is explained by the linear relationship with reaction temperature.

----

.. admonition:: Exercise 4: Coefficient of Determination
   :class: note

   For each scenario, interpret what the given :math:`R^2` value means:

   a. A regression of fuel consumption (mpg) on vehicle weight (lbs) yields :math:`R^2 = 0.78`.

   b. A regression of exam score on hours of study yields :math:`R^2 = 0.35`.

   c. A regression of chip manufacturing yield on equipment age yields :math:`R^2 = 0.95`.

   d. Which regression model would you be most confident using for predictions? Least confident? Explain.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Fuel consumption model (RÂ² = 0.78)**

      78% of the variation in fuel consumption is explained by the linear relationship with vehicle weight. This is a reasonably strong modelâ€”weight is an important predictor of fuel efficiency.

      **Part (b): Exam score model (RÂ² = 0.35)**

      Only 35% of the variation in exam scores is explained by hours of study. This suggests that while study time helps, many other factors (prior knowledge, test-taking ability, sleep, etc.) affect scores.

      **Part (c): Chip yield model (RÂ² = 0.95)**

      95% of the variation in manufacturing yield is explained by equipment age. This is a very strong relationship, indicating equipment age is the dominant factor affecting yield.

      **Part (d): Confidence in predictions**

      - **Most confident:** Chip manufacturing yield model (:math:`R^2 = 0.95`) â€” predictions will have the smallest prediction errors since almost all variation is explained.
      
      - **Least confident:** Exam score model (:math:`R^2 = 0.35`) â€” predictions will have large uncertainty since 65% of variation is unexplained.

      Higher :math:`R^2` means less unexplained variation, leading to more precise predictions.

----

.. admonition:: Exercise 5: Relationship Between r and RÂ²
   :class: note

   For a simple linear regression of server response time (Y, ms) on number of concurrent users (X):

   - Sample correlation: :math:`r = -0.82`

   a. Calculate :math:`R^2`.

   b. What percentage of the variation in response time is explained by the number of users?

   c. What percentage is unexplained (due to other factors)?

   d. What is the sign of the slope :math:`b_1`? How do you know?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Calculate RÂ²**

      :math:`R^2 = r^2 = (-0.82)^2 = 0.6724`

      **Part (b): Explained variation**

      67.24% of the variation in response time is explained by the linear relationship with number of concurrent users.

      **Part (c): Unexplained variation**

      :math:`1 - R^2 = 1 - 0.6724 = 0.3276 = 32.76\%` is unexplained.

      This variation is due to other factors like server configuration, network latency, query complexity, etc.

      **Part (d): Sign of slope**

      The slope :math:`b_1` is **negative**.

      We know this because :math:`r = -0.82 < 0`, and the slope and correlation always have the same sign. This makes sense: more concurrent users typically increase (not decrease) response time, so there may be an error in the problem setupâ€”or this represents a scenario where more users triggers server scaling.

      **Note:** If the negative correlation seems counterintuitive, it could represent a scenario where higher traffic triggers auto-scaling or caching improvements.

----

.. admonition:: Exercise 6: Residual Analysis Basics
   :class: note

   A materials scientist models tensile strength (MPa) as a function of carbon content (%) in steel alloys. The regression output gives :math:`\hat{y} = 280 + 450x`.

   For the following observations, calculate the fitted value and residual:

   .. list-table::
      :header-rows: 1
      :widths: 25 25 25 25

      * - Carbon Content (x)
        - Tensile Strength (y)
        - Fitted Value (:math:`\hat{y}`)
        - Residual (:math:`e`)
      * - 0.20
        - 365
        - ?
        - ?
      * - 0.35
        - 445
        - ?
        - ?
      * - 0.50
        - 520
        - ?
        - ?
      * - 0.65
        - 570
        - ?
        - ?

   a. Complete the table above.

   b. What does a positive residual indicate about the observed vs. predicted value?

   c. What does a negative residual indicate?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Completed table**

      .. list-table::
         :header-rows: 1
         :widths: 25 25 25 25

         * - Carbon Content (x)
           - Tensile Strength (y)
           - Fitted Value (:math:`\hat{y}`)
           - Residual (:math:`e`)
         * - 0.20
           - 365
           - 370
           - âˆ’5
         * - 0.35
           - 445
           - 437.5
           - +7.5
         * - 0.50
           - 520
           - 505
           - +15
         * - 0.65
           - 570
           - 572.5
           - âˆ’2.5

      Calculations:

      - :math:`\hat{y}_{0.20} = 280 + 450(0.20) = 280 + 90 = 370`
      - :math:`\hat{y}_{0.35} = 280 + 450(0.35) = 280 + 157.5 = 437.5`
      - :math:`\hat{y}_{0.50} = 280 + 450(0.50) = 280 + 225 = 505`
      - :math:`\hat{y}_{0.65} = 280 + 450(0.65) = 280 + 292.5 = 572.5`

      **Part (b): Positive residual**

      A positive residual (:math:`e > 0`) indicates the observed value is **above** the regression lineâ€”the actual tensile strength is higher than predicted by the model.

      **Part (c): Negative residual**

      A negative residual (:math:`e < 0`) indicates the observed value is **below** the regression lineâ€”the actual tensile strength is lower than predicted by the model.

----

.. admonition:: Exercise 7: MSE and Standard Error
   :class: note

   From an ANOVA table for a regression with :math:`n = 20` observations:

   - SSE = 324
   - SSR = 1296

   a. Calculate the degrees of freedom for error.

   b. Calculate MSE.

   c. Calculate the estimate of :math:`\sigma`.

   d. What does this estimate of :math:`\sigma` represent in the context of the regression model?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Degrees of freedom for error**

      :math:`df_{Error} = n - 2 = 20 - 2 = 18`

      **Part (b): MSE**

      :math:`MSE = \frac{SSE}{df_{Error}} = \frac{324}{18} = 18`

      **Part (c): Estimate of Ïƒ**

      :math:`\hat{\sigma} = \sqrt{MSE} = \sqrt{18} = 4.243`

      **Part (d): Interpretation**

      The estimate :math:`\hat{\sigma} = 4.243` represents the **estimated standard deviation of the random errors** (:math:`\varepsilon_i`) in the regression model.

      In practical terms:

      - It measures the typical vertical distance of data points from the regression line
      - It quantifies the "noise" or unexplained variation in Y after accounting for X
      - Smaller values indicate data points cluster more tightly around the fitted line

----

.. admonition:: Exercise 8: Alternative Slope Formulas
   :class: note

   The slope estimate can be written in multiple equivalent forms:

   .. math::

      b_1 = \frac{S_{XY}}{S_{XX}} = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2} = r \cdot \frac{s_Y}{s_X}

   Given:

   - :math:`r = 0.85`
   - :math:`s_X = 12.4` (standard deviation of X)
   - :math:`s_Y = 28.6` (standard deviation of Y)

   a. Calculate :math:`b_1` using the formula :math:`b_1 = r \cdot \frac{s_Y}{s_X}`.

   b. If the correlation changed to :math:`r = -0.85` (same magnitude, opposite sign), what would :math:`b_1` be?

   c. What does this relationship tell us about the connection between correlation and slope?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Calculate bâ‚**

      .. math::

         b_1 = r \cdot \frac{s_Y}{s_X} = 0.85 \times \frac{28.6}{12.4} = 0.85 \times 2.306 = 1.960

      **Part (b): With negative correlation**

      .. math::

         b_1 = (-0.85) \times \frac{28.6}{12.4} = -0.85 \times 2.306 = -1.960

      **Part (c): Connection between r and bâ‚**

      Key relationships:

      1. **Same sign:** The slope and correlation always have the same sign. Positive r means positive slope; negative r means negative slope.

      2. **Magnitude relationship:** The slope magnitude depends on both |r| and the ratio of standard deviations (sY/sX).

      3. **Scaling:** A steeper slope doesn't necessarily mean stronger correlationâ€”it could just mean Y has larger spread relative to X.

      4. **Interpretation:** Correlation measures strength of linear association on a standardized scale (âˆ’1 to 1), while slope measures the actual rate of change in Y per unit change in X (in original units).

----

.. admonition:: Exercise 9: Model Assumptions Conceptual
   :class: note

   The simple linear regression model is :math:`Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i` where :math:`\varepsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)`.

   For each statement, identify which assumption(s) would be violated:

   a. The relationship between X and Y follows a curved pattern.

   b. The spread of Y values around the regression line increases as X increases.

   c. The data were collected over time and consecutive observations tend to have similar residuals.

   d. Several residuals have extremely large magnitudes compared to others.

   e. Observations were randomly selected and there is no connection between different data points.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Curved pattern**

      Violates **Linearity** â€” The model assumes :math:`E[Y|X] = \beta_0 + \beta_1 X` is a straight line. A curved pattern means this assumption fails.

      **Part (b): Spread increases with X**

      Violates **Equal Variance (Homoscedasticity)** â€” The model assumes :math:`Var(\varepsilon_i) = \sigma^2` is constant for all observations. Increasing spread (funnel shape) indicates heteroscedasticity.

      **Part (c): Consecutive observations have similar residuals**

      Violates **Independence** â€” The "iid" assumption requires errors to be independent. When consecutive observations are correlated (autocorrelation), this assumption fails. Common in time series data.

      **Part (d): Extreme residuals**

      Potentially violates **Normality** â€” While outliers don't necessarily violate assumptions (they could be legitimate extreme values from a normal distribution), multiple extreme residuals suggest the errors may not follow a normal distribution (e.g., heavy-tailed distribution).

      **Part (e): Random selection, no connection**

      This describes a situation where **no assumptions are violated** â€” this is exactly what the independence assumption requires.

----

Additional Practice Problems
----------------------------

**Quick Calculations**

1. Given :math:`\sum x_i^2 = 500`, :math:`n = 10`, :math:`\bar{x} = 6`, calculate :math:`S_{XX}`.

2. If :math:`b_1 = 2.5`, :math:`\bar{x} = 10`, and :math:`\bar{y} = 40`, find :math:`b_0`.

3. SST = 1000, SSR = 750. Calculate SSE and :math:`R^2`.

**True/False**

4. The least squares line minimizes the sum of squared horizontal distances from points to the line.

5. If :math:`R^2 = 1`, then SSE = 0.

6. The residuals from a least squares fit always sum to zero.

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **Quick Calculations:**

   1. :math:`S_{XX} = \sum x_i^2 - n\bar{x}^2 = 500 - 10(6)^2 = 500 - 360 = 140`

   2. :math:`b_0 = \bar{y} - b_1\bar{x} = 40 - 2.5(10) = 40 - 25 = 15`

   3. :math:`SSE = SST - SSR = 1000 - 750 = 250`; :math:`R^2 = SSR/SST = 750/1000 = 0.75`

   **True/False:**

   4. **FALSE** â€” Least squares minimizes the sum of squared *vertical* distances (residuals), not horizontal distances.

   5. **TRUE** â€” If :math:`R^2 = 1`, then :math:`SSR = SST`, so :math:`SSE = SST - SSR = 0`. All points lie exactly on the regression line.

   6. **TRUE** â€” This is a mathematical property of the least squares method. The normal equations guarantee :math:`\sum e_i = 0`.