.. _13-3-diagnostics-inference:

Model Diagnostics and Statistical Inference
==========================================================


Having developed several point estimates related to simple linear regression,
we now face the critical stage of uncertainty quantification using confidence regions and 
hypothesis tests. Before conducting any statistical inference, however, we must 
first verify that our model assumptions are reasonable.

.. admonition:: Road Map üß≠
   :class: important

   * Understand why all assumption-checking procedures involve the residuals.
   * List the appropriate graphcial tools to assess each linear regression assumption, and systematically search for
     signs of assumption violation in each plot.
   * Perform inference on the overall model utility and on the model parameters.

----------------------------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch13-6">
      <iframe
         id="video-ch13-6"
         title="STAT 350 ‚Äì Chapter 13.6 Diagnostics for Model Assumptions Video"
         src="https://www.youtube.com/embed/p8kRL-vUpVo?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides üìä
   :class: tip

   `Download Chapter 13 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%2013%20Linear%20Regression/SimpleLinearRegression_AC.pptx>`_
   
Preliminaries for Model Diagnostics
---------------------------------------

Review of Simple Linear Regression Assumptions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The simple linear regression model 

.. math:: 
   :label: model

   Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
   
requires four key assumptions:

1. For each given :math:`x_i`, :math:`Y_i` is a size-1 **simple random sample** from the distribution of 
   all possible responses, :math:`Y|X=x_i`.
2. The association between the explanatory and response variables is **linear** on average.
3. The error terms are **normally distributed**:
   :math:`\varepsilon_i \stackrel{iid}{\sim} N(0, \sigma^2) \quad \text{for } i = 1, 2, \ldots, n.`
4. The error terms have **constant variance** :math:`\sigma^2` across all values of :math:`X`. 

For detailed discussion on the assumptions, revisit :numref:`assumptions`.

Overview of Residuals and Their Properties
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Note that :math:`\varepsilon_i`'s are the only random component of the current linear regression model, and therefore,
all model assumptions are essentially assumptions about the behavior of the error terms.

Also recall the mathematical definition of residuals: 

.. math:: 
   e_i = y_i - \hat{y}_i, \text{ for each } i = 1,2,\cdots, n.

Residuals can be viewed as the observed counterparts of the true error terms :math:`\varepsilon_i` because:

.. math::
   \varepsilon_i = y_i - (\beta_0 + \beta_1 x_i) \approx y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i) = y_i - \hat{y}_i = e_i.

Since we do not have access to the true realizations of :math:`\varepsilon_i`, the residuals instead 
play a key role in model diagnostics.

Scatter Plot and Residual Plot for Linearity and Constant Variance
----------------------------------------------------------------------------

We have used scatter plots for the preliminary assessment of the association between two quantitative variables.
We introduce the **residual plot** as an additional tool for model diagnosis; it is simply 
a scatter plot of :math:`(x_i, e_i)` for :math:`i=1,\cdots, n`.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/scatter-vs-residual.png
   :width: 90%
   :align: center
   :alt: Comparison of scatter plot and residual plot
   
   Comparison of scatter plot and residual plot
   
A residual plot can be viewed as the original scatter plot rotated so that the regression line becomes horizontal. This
removes the visual bias of the tilted trend line and highlights the deviations 
from the fitted line.

Scatter plots and residual plots are typically used together to assess two assumptions: 
**linearity and constant variance.**

Characteristics of Ideal Scatter and Residual Plots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When the linearity and constant variance assumptions hold, 
the points on both the scatter plot and residual plot form a random scatter around their respective summary lines, 
with a roughly constant spread across the :math:`x`-axis.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/residual-no-violation.png
   :width: 70%
   :align: center
   :alt: A residual plot showing no signs of assumption violation
   
   A residual plot showing no signs of assumption violation

Signs of Assumption Violation on Scatter and Resiual Plots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Violation of the Linearity Assumption
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If residuals systematically fall below the zero line in certain regions of :math:`x` values and
above the line in others, this suggests the true relationship is non-linear. 

.. _linearity-violation:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/residual-linearity.jpg
   :width: 70%
   :align: center
   :alt: Resiual plots with linearity assumption violated
   
   Resiual plots with linearity assumption violated

2. Violation of the Constant Variance Assumption
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the spread of points around the trend line is inconsistent, the data likely violate the constant variance assumption.

.. _variance-violation:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/residual-variance.png
   :width: 90%
   :align: center
   :alt: Resiual plots with constant variance assumption violated
   
   Resiual plots with constant variance assumption violated

* **Hourglass Pattern**:  The spread is larger for extreme values of :math:`X` than in the 
  middle range.

* **Cone Pattern**: As :math:`X` increases, the residual errors become larger.

Other patterns of non-constant variance can also occur. Even without systematic patterns, an inconsistent spread alone is a 
significant sign of assumption violation.

3. Outliers
^^^^^^^^^^^^^^

Recall that scatter plots are also used to identify potential outliers and influential points. 
Residual plots can serve the same purpose.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/residual-outliers.png
   :width: 90%
   :align: center
   :alt: Resiual plots with outliers
   
   Resiual plots with outliers; left plot shows a :math:`y`-outlier, and right shows an :math:`x`-outlier that
   potentially has high influence on the model.

.. admonition:: Violations May or May not Occur Simultaneously
   :class: danger

   While :numref:`linearity-violation` show signs of non-linearity, it does not violate the constant variance assumption since the
   bandwidth of data points around their true summary curve (quadratic association) is reasonably constant. 

   Likewise, :numref:`variance-violation` shows no sign of non-linearity, as the patterns of spread above and below the zero line
   are roughly symmetric.

   Be aware that different types of violations may occur simultaneously or separately, and be prepared to distinguish
   one case from another on the graphs. 
   
   **Exercise:** draw a residual plot that shows violation of lineary and constant variance assumptions simultaneously.

Histogram and Normal Probability Plot of Residuals for Normality
----------------------------------------------------------------------------

To verify normality of the error terms, we construct a histogram and a normal probability plot of
the residuals.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/hist-qq-residuals.png
      :width: 90%
      :align: center
      :alt: Histogram and normal probability plot of residuals

      Histogram and normal probability plot of residuals

These plots are used in the same way as in any prior context of normality assessment.
We check whether the points follow a straight line on the normal probability plot and whether the histogram is bell-shaped.

Summary of Diagnostic Tools and Comprehensive Eaxmples
----------------------------------------------------------

We have introduced four graphical tools to assess the assumptions of linear regression. See the
table below for a summary:

.. flat-table::
   :header-rows: 1
   

   * - Assumption
     - Graphical Tools

   * - SRS
     - None (must be ensured through experimental design)

   * - Linearity
     - Scatter plot and residual plot 

   * - Constant variance
     - Scatter plot and residual plot
   
   * - Normality of errors
     - Histogram and QQ plot of residuals

.. admonition:: Example  üí°: Comprehensive Diagnostic Exercise
   :class: note 

   For each of the following sets of graphical representations, assess whether the data set meets the
   assumptions required for valid linear regression.

   **Example 1**

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/graphical-diag-example1.png
      :width: 90%
      :align: center
      :alt: First example of comprehensive model diagnostics

      Example 1 of graphical diagnosis

   - Scatter plot and residual plot show a systematic **curved pattern**.
   - If the correct summary curve is used (quadratic form), then the size of their vertical spread
     would not vary by the :math:`x`-values. So non-constant variance is not a concern.
   - Histogram and normal probability plots look reasonable.

   This data set violates the linearity assumption.


   **Example 2**

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/graphical-diag-example2.png
      :width: 90%
      :align: center
      :alt: Second example of comprehensive model diagnostics

      Example 2 of graphical diagnosis

   - The dots follow a linear pattern according to both the scatter plot and the residual plot‚Äîthey form mirror 
     images below and above their summary lines.
   - The spread of the points are narrower in the two ends than in the middle region. This raises a concern
     for **non-constant variance**.
   - The histogram and the QQ plot indicate that the error distribution has **heavier tails than the blue curve**. 
     The error distribution may not be normal.

   This data set violates the constant-variance assumption and is suspected to have non-normal error terms.

..
   .. admonition:: Example : Blood Pressure Study, Continued
      :class: note 
         
      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-diagnostics.png
         :width: 80%
         
         Diagnostic analysis of the blood pressure treatment study showing scatter plot with fitted line

      Let's apply our diagnostic procedures to the blood pressure treatment study, where we examined the relationship between patient age and change in blood pressure after 24 hours of treatment.

      **Scatter Plot Assessment**:

      Looking at the scatter plot with the fitted line :math:`\hat{y} = 20.12 - 0.5263x`, we can trace our finger across the plot to assess the spread of points around the line. The linear relationship appears reasonable, though with only 11 observations, it's challenging to definitively assess the constant variance assumption.

      **Residual Plot Analysis**:

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-residuals.png
         :width: 80%
         
         Residual plot for the blood pressure data showing individual residuals labeled by vehicle type

      The residual plot for the car efficiency data shows each vehicle's residual clearly labeled. When we examine the spread across different cylinder volume ranges:

      - Left region (around 1.5L): Limited observations make assessment difficult
      - Middle region (around 1.8-2.0L): Several observations with varied residuals
      - Right region (around 2.5L): Adequate spread above and below zero

      The residual plot suggests potential minor violations of the constant variance assumption, but nothing strong enough to invalidate our analysis given the small sample size.

When Assumptions Are Violated
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If diagnostic procedures reveal serious assumption violations, we should not proceed with linear regression analysis. Instead,
we should take an appropriate measure to mitigate the effect of the violation or use a different analysis method.

**Linearity Violations**:

- Consider transformations of variables (log, square root, etc.)
- Fit non-linear models (beyond course scope)
- Use piecewise or segmented regression for different regions

**Constant Variance Violations**:

- Variable transformations may help stabilize variance
- Weighted least squares methods (beyond course scope)
- Robust regression techniques

**Normality Violations**:

- Often less critical for large sample sizes due to Central Limit Theorem
- Bootstrap methods for inference (beyond course scope)
- Non-parametric alternatives

**Independence Violations**:

- Time series methods for temporally correlated observations
- Mixed effects models for clustered data


----------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch13-7">
      <iframe
         id="video-ch13-7"
         title="STAT 350 ‚Äì Chapter 13.7 Simple Linear Regression Model Inference - F-test Video"
         src="https://www.youtube.com/embed/mTQ3GU9rpys?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

The F-Test for Model Utility
----------------------------------

Once we have verified that our model assumptions are reasonably satisfied, we proceed with statistical inference. 
The first question we typically ask is: "Does the simple linear regression model provide useful information about 
the relationship between the explanatory and response variables?" 

We organize our answer to this question
into the usual four-step hypothesis test. We refer to the ANOVA table constructed in Chapter 13.2.

Step 1: Parameter of Interest
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The focus of this test is on the overall usefulness of the linear relationship rather than on specific parameters. 
Therefore, it is okay to skip explicit parameter definition.

Step 2: Hypotheses
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* :math:`H_0:` There is no linear association between :math:`X` and :math:`Y`.
* :math:`H_0:` There is a linear association between :math:`X` and :math:`Y`.

**Important**: Always state the hypotheses using the experimental context, replacing "X" and "Y" with 
the actual variable names and providing sufficient background information.

Step 3: Test Statistic and P-value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Recall that MSR is a scaled measurement of the variability attributed to the
model structure, while MSE is a scaled measure of random error. Therefore, if MSR is 
substantially greater than MSE, we can consider the model to be statistically significant.

The corresponding test statistic is:

.. math::

   F_{TS} = \frac{\text{MSR}}{\text{MSE}},

with degrees of freedom :math:`df_1 = 1` and :math:`df_2 = n-2`.

A large observed value of :math:`F_{TS}` is in favor of the alternative hypothesis that the model is significant. Therefore, 

.. math::
   \text{p-value} = P(F_{1,  n-2} > f_{TS}).

Step 4: Decision and Conclusion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- If p-value ‚â§ :math:`\alpha`: 

  Reject :math:`H_0`. At the :math:`\alpha` significance level, we have sufficient 
  evidence to conclude that there is a linear association between [explanatory variable] and [response variable] in [context]."

- If p-value > :math:`\alpha`: 

  Fail to reject :math:`H_0`. At the :math:`\alpha` significance level, we do not have sufficient evidence to 
  conclude that there is a linear association between [explanatory variable] and [response variable] in [context]."

.. admonition:: Example üí°: Blood Pressure Study, Continued
   :class: note 

   Assuming that the assumptions of linear regression have been verified, perform the 
   :math:`F`-test for model utility using :math:`\alpha = 0.05.`
   Use the partially filled ANOVA table from the previous lesson:

   .. list-table:: 
      :header-rows: 1
      :widths: 10 5 10 10 10 10

      *  - Source
         - df
         - Sum of Squares
         - Mean Square
         - F-statistic
         - p-value
      *  - Regression
         - 1
         - :math:`555.7126`
         - :math:`555.7126`
         - ?
         - ?
      *  - Error
         - :math:`9`
         - :math:`382.8267`
         - :math:`42.5363`
         - 
         - 
      *  - Total
         - :math:`10`
         - :math:`938.5393`
         - 
         - 
         - 
         
   **Step 1: Parameter Definition**

   This step is skipped for model utility tests.

   **Step 2: Hypotheses**:

   - :math:`H_0`: There is no linear association between patient age and change in blood pressure.
   - :math:`H_a`: There is a linear association between patient age and change in blood pressure.

   **Step 3: Computation**

   .. math::
      f_{TS} = \frac{\text{MSR}}{\text{MSE}} = \frac{555.7126}{42.5363} = 13.06

   The test statistic has two degrees of freedom: :math:`df_1=1` and :math:`df_2=9`. The :math:`p`-value is:

   .. math:: 
      P(F_{1,9} > 13.06) = 0.0055.

   **Step 4: Conclusion**

   At :math:`\alpha = 0.05`, we reject :math:`H_0` and conclude that there is sufficient evidence of a 
   linear association between patient age and change in blood pressure.


------------------------------------------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch13-8">
      <iframe
         id="video-ch13-8"
         title="STAT 350 ‚Äì Chapter 13.8 Simple Linear Regression Model Inference - Slope and Intercept Video"
         src="https://www.youtube.com/embed/_XCCR_oXcL0?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>


Distributional Properties of the Slope and Intercept Estimators
------------------------------------------------------------------

To develop inference procedures for :math:`\beta_0` and :math:`\beta_1`, we need to understand the statistical properties 
of their **estimators**, :math:`b_0` and :math:`b_1`. This draws distinction from our focus on
:math:`b_0` and :math:`b_1` as **estimates** so far, yielding one set of realized values based on a single data. 
We would now like to study their behaviors across many different datasets.

1. Slope and Intecept Estimators Are Linear Combinations of the Responses
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The first step in constructing inference for regression parameters is to recognize that their estimators are
linear combinations of the response variables, :math:`Y_i, i=1,\cdots, n`. Capital :math:`Y` is
used throughout this section to emphasize that we are discussing responses as **random variables**, not their observed values.
The values of the explanatory variable are still considered given and fixed.

We first show that the slope estimator :math:`b_1` is a linear combination of :math:`Y_i, i=1,\cdots, n`, starting with its
definition:

.. math::
   :label: slope-linear-comb

   b_1 &= \frac{\sum_{i=1}^n x_iY_i - n\bar{x}\bar{Y}}{S_{XX}} \\
       &= \frac{\sum_{i=1}^n x_iY_i - \sum_{i=1}^n\bar{x}Y_i}{S_{XX}}\\
       &= \sum_{i=1}^n \left(\frac{x_i - \bar{x}}{S_{XX}}\right)  Y_i

Each coefficient to the responses above only consists of non-random quantities involving the
explanatory variable. 

For the intercept estimator, we borrow the result of Eq. :eq:`slope-linear-comb`.

.. math::

   b_0 &= \bar{Y} -b_1\bar{x} \\
       &= \sum_{i=1}^n \frac{1}{n}Y_i - \bar{x}\sum_{i=1}^n \left(\frac{x_i - \bar{x}}{S_{XX}}\right)  Y_i\\
       &= \sum_{i=1}^n \left(\frac{1}{n} - \frac{\bar{x}}{S_{XX}}(x_i - \bar{x})\right) Y_i\\

Again, the coefficient of each term above consists of non-random quantities involving only the
sample size and the explanatory values.

**‚ÄºÔ∏è Key Observation**

Recall that each :math:`Y_i` is **normally disributed** given an observed :math:`x_i`. Since both
:math:`b_1` and :math:`b_0`
are linear combinations of normal random variables, they must also be normally distributed. We now
proceed to state their expectations and variances. 

2. The Expectation and Variance of the Parameter Estimators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**The Slope Estimator**

.. math::
   &E[b_1] = \beta_1\\
   &\text{Var}(b_1) = \frac{\sigma^2}{S_{XX}}

**The Intercept Estimator**

.. math::
   &E[b_0] = \beta_0\\
   &\text{Var}(b_0) = \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)

**Key Insights**:

- Both :math:`b_1` and :math:`b_0` are **unbiased estimators** of :math:`\beta_1` and :math:`\beta_0`, respectively.
- The slope variance depends only on the error variance and the spread of the explanatory values.
- The intercept variance includes additional uncertainty when :math:`\bar{x} \neq 0`.

.. admonition:: Derive the Results as an Independent Exercise
   :class: important

   The expectations and variances of both estimators can be derived using general properties 
   and some algebraic manipulation. You are encouraged to verify these results as an independent exercise.


3. The Complete Distribution of the Estimators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Provided that all model assumptions hold,

.. math::

   b_1 \sim N\left(\beta_1, \frac{\sigma^2}{S_{XX}}\right)


.. math::

   b_0 \sim N\left(\beta_0, \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)\right)


4. Estimated Standard Errors and t-Distribution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Since :math:`\sigma^2` is unknown, we replace it with its estimate :math:`s^2 = MSE` to obtain the 
estimated standard errors:

.. math::

   \widehat{SE}(b_1) = \sqrt{\frac{MSE}{S_{XX}}}

.. math::

   \widehat{SE}(b_0) = \sqrt{MSE\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)}

Studentization of each estimator then gives us a :math:`t`-distributed random variable, both with :math:`df=n-2`:

.. math::

   \frac{b_1 - \beta_1}{\widehat{SE}(b_1)} \sim t_{n-2}

.. math::

   \frac{b_0 - \beta_0}{\widehat{SE}(b_0)} \sim t_{n-2}

We use these as the foundation for confidence regions and hypothesis tests on the true
values of the regression parameters, :math:`\beta_1` and :math:`\beta_0`.

Confidence Regions for Parameters
---------------------------------------

Recall the general form of a :math:`t`-confidence region:

.. math::
   \text{estimate} \pm t_{crit} \widehat{SE}(\text{estimate})

The table below applies the general form to all possible parameter-side combination:

.. flat-table::
   :header-rows: 1

   * - Side
     - :math:`\beta_0`
     - :math:`\beta_1`
   
   * - CI
     - .. math::
         b_0 \pm t_{\alpha/2, n-2}\sqrt{MSE\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)}
     - .. math::
         b_1 \pm t_{\alpha/2, n-2} \sqrt{\frac{MSE}{S_{XX}}}

   * - UCB
     - .. math::
         b_0 + t_{\alpha, n-2}\sqrt{MSE\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)}
     - .. math::
         b_1 + t_{\alpha, n-2} \sqrt{\frac{MSE}{S_{XX}}}

   * - LCB
     - .. math::
         b_0 - t_{\alpha, n-2}\sqrt{MSE\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)}
     - .. math::
         b_1 - t_{\alpha, n-2} \sqrt{\frac{MSE}{S_{XX}}}

Hypothesis Testing for the Slope Parameter
-------------------------------------------------

**Step 1: Parameter of Interest**

Define :math:`\beta_1` as the slope of the **true regression line** relating [explanatory variable] to [response variable].
Make sure to use experiment-specific variable names and context.

**Step 2: Hypotheses**

It is possible to construct a one-sided or two-sided hypothesis test for the slope parameter against any null value :math:`\beta_{10}`. 

.. math::
   &H_0: \beta_{1} (=, \leq, \geq) \beta_{10}\\
   &H_1: \beta_{1} (\neq, >, <) \beta_{10}

The most common hypothesis is the two-sided variant with the null value :math:`\beta_{10} = 0`. 

.. math::
   &H_0: \beta_1 = 0\\
   &H_a: \beta_1 \neq 0

**Step 3: Test Statistic and P-value**

.. math::

   T_{ts} = \frac{b_1 - \beta_{10}}{\widehat{SE}(b_1)}

has :math:`t`-distribution with :math:`df = n-2`.  Once an observed value :math:`t_{TS}` is obtained,
the :math:`p`-value computation should align with the sidedness of the hypothesis.

* Two-sided: :math:`p`-value :math:`= 2P(T_{n-2} > |t_{TS}|)`
* Upper-tailed: :math:`p`-value :math:`= P(T_{n-2} > t_{TS})` 
* Lower-tailed: :math:`p`-value :math:`= P(T_{n-2} < t_{TS})`

**Step 4: Decision and Conclusion**

Compare p-value to :math:`\alpha` and draw conclusions about the slope parameter **in context**.

.. admonition:: Hypothesis Tests for the Intercept
   :class: important

   Hypothesis tests on :math:`\beta_0` can be constructed in a similar manner. They are not explored
   in detail because:
   
   (1) The procedure only differs by the form of the standard error estimate, and
   (2) The intercept often does not have any practial significance.

..
   Implementation in R
   -------------------

   **Fitting the Model**:

   ```r
   # Fit linear model
   fit <- lm(response ~ explanatory, data = dataset_name)

   # Extract coefficients
   coefficients(fit)  # or fit`coefficients

   # Extract residuals and fitted values  
   residuals(fit)     # or fit`residuals
   fitted(fit)        # or fit`fitted.values
   ```

   **Getting Inference Results**:

   ```r
   # Complete summary with tests and R-squared
   summary(fit)

   # ANOVA table for F-test
   anova(fit)
   # or
   summary(aov(fit))
   ```

   **Manual Calculations** (when given summary statistics rather than raw data):

   ```r
   # Calculate test statistics manually
   F_stat <- MSR / MSE
   t_stat <- (b1 - beta1_null) / SE_b1

   # Calculate p-values
   p_value_F <- pf(F_stat, df1 = 1, df2 = n-2, lower.tail = FALSE)
   p_value_t <- 2 * pt(abs(t_stat), df = n-2, lower.tail = FALSE)

   # Calculate confidence intervals
   margin_error <- qt(alpha/2, df = n-2, lower.tail = FALSE) * SE_b1
   CI_lower <- b1 - margin_error
   CI_upper <- b1 + margin_error
   ```

.. admonition:: Example üí°: Blood Pressure Study, Continued
   :class: note 

   For the blood pressure vs age data set, perform a hypothesis test to determine if the slope parameter is non-zero.
   Then, construct a corresponding confidence region.

   **Step 1: Define the parameter of interest**
   
   We are interested in :math:`\beta_1`, the true change in blood pressure per year increase in patient age.

   **Step 2: State the hypotheses**

   .. math::
      &H_0: \beta_1 = 0\\
      &H_a: \beta_1 \neq 0

   **Step 3: Computation**

   The test statistic is:

   .. math::
      t_{TS} = \frac{b_1-0}{\sqrt{\text{MSE}/S_{xx}}}\frac{-0.526 - 0}{0.146} = -3.61.

   The :math:`p`-value is:

   .. math::
      2P(T_9 > 3.61)  = 0.0055.
   
   **Step 4: Conclusion**

   :math:`p`-value :math:`=0.0055 < \alpha`. The null hypothesis is rejected.
   At :math:`\alpha=0.05`, there is enough evidence to conclude that the slope of the true linear 
   association between age and change in blood pressure is different than :math:`0`.

   **95% Confidence Interval**:

   The confidence region corresponding to a two-sided hypothesis test is a confidence interval.
   Use R to compute the critical value:

   .. code-block:: r

      qt(0.025, df=9, lower.tail=FALSE)
      #2.262

   Then putting the components together,

   .. math::
      -0.526 \pm (2.262)(0.146) = -0.526 \pm 0.330 = (-0.856, -0.196).

   We are 95% confident that each additional year of age is associated with a decrease in blood pressure between 
   0.196 and 0.856 mm Hg after treatment.


..
   Standard Error Formulas and Confidence Intervals
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-interval-slope.png
      :width: 80%
      
      Confidence interval formula for the slope parameter with all components clearly labeled

   Since :math:`\sigma^2` is unknown, we estimate it using :math:`s^2 = \text{MSE}` and use the t-distribution:

   **Standard Error of the Slope**:

   .. math::

      SE(b_1) = \sqrt{\frac{\text{MSE}}{S_{xx}}}

   where :math:`S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2`.

   **Confidence Interval for the Slope**:

   .. math::

      b_1 \pm t_{\alpha/2, n-2} \sqrt{\frac{\text{MSE}}{S_{xx}}}

   This provides a range of plausible values for the true slope :math:`\beta_1` with :math:`(1-\alpha) \times 100\%` confidence.

   **Interpretation**: We are :math:`(1-\alpha) \times 100\%` confident that the true change in the response variable for each one-unit increase in the explanatory variable lies within this interval.

   Complete Hypothesis Testing Framework
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-hypothesis-test-slope.png
      :width: 80%
      
      Complete framework for hypothesis testing about the slope parameter

   **Step 1: Parameter of Interest**

   We are interested in :math:`\beta_1`, the population slope of the mean response line :math:`\mu_{Y|X=x}`.

   **Step 2: Hypotheses**

   The general form allows for various null values :math:`\beta_{10}`:

   .. math::

      H_0: \beta_1 = \beta_{10} \quad \text{vs} \quad H_a: \beta_1 \neq \beta_{10}

   Alternative formulations:

   - One-sided upper: :math:`H_a: \beta_1 > \beta_{10}`
   - One-sided lower: :math:`H_a: \beta_1 < \beta_{10}`

   **Step 3: Test Statistic and P-value**

   .. math:

      t = \frac{b_1 - \beta_{10}}{SE(b_1)} = \frac{b_1 - \beta_{10}}{\sqrt{\text{MSE}/S_{xx}}}

   The test statistic follows a t-distribution with :math:`n-2` degrees of freedom.

   **P-value calculation depends on the alternative**:

   - Two-sided: :math:`\text{p-value} = 2P(t_{n-2} > |t|)`
   - Upper tail: :math:`\text{p-value} = P(t_{n-2} > t)`  
   - Lower tail: :math:`\text{p-value} = P(t_{n-2} < t)`

   **Step 4: Decision and Conclusion**

   Compare p-value to :math:`\alpha` and state conclusions in context of the problem.

Special Case: Equivalence of F-test and t-test
----------------------------------------------------------------------------------

For simple linear regresssion, the hypothesis test on :math:`H_a: \beta_1 \neq 0` is equivalent 
to the :math:`F`-test for model utility. Intuitively, this makes sense because, in order for the model
to meaningfully describe the response variable, the explanatory variable must
have a significant association with it. 

Mathematically, this equivalence is explained through the special equality: 

.. math::

   T_{ts}^2 = F_{TS}

To see why this is true, recall that :math:`b_1 = \frac{S_{XY}}{S_{XX}}` 
and :math:`\text{MSR} = b_1\text{S}_{XY}`. Using these, 

.. math:: 

   T_{ts}^2 = \frac{b_1^2}{\text{MSE}/S_{XX}} = \frac{b_1 \frac{S_{XY}}{S_{XX}} S_{XX}}{MSE} = \frac{MSR}{MSE}

This implies that both tests provide identical p-values and conclusions.

**The Equivalence Only Holds for Simple LR**

The two tests serve different purposes in general linear regression with multiple predictors:

- The :math:`F`-test assesses whether at least one of the predictors provide useful information about the response variable.
- The :math:`t`-test on a slope parameter assesses whether the single corresponding predictor is useful.

.. admonition:: Example üí°: Blood Pressure Study, Continued
   :class: note 
      
   For the blood pressure dataset, verify the equivalence of the model utility :math:`F`-test and the 
   two-tailed :math:`t`-test on the slope parameter against :math:`\beta_{10}=0`.

   From the two previous examples, 

   - :math:`t_{TS}^2 = (-3.61)^2 = 13.03`
   - :math:`f_{TS}=13.06`
   - There is a small difference due to rounding, but both tests give :math:`p`-value :math:`= 0.0055`. ‚úî


..
   Implementation in R: Complete Workflow
   -----------------------------------------

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-r-implementation.png
      :width: 80%
      
      Complete R workflow for fitting models and conducting inference

   **Model Fitting and Basic Output**:

   .. code-block:: r

      # Fit the linear model
      fit <- lm(response_variable ~ explanatory_variable, data = dataFrame)
      
      # Extract key components
      coefficients(fit)     # Get b0 and b1
      residuals(fit)        # Get residuals for diagnostics
      fitted.values(fit)    # Get predicted values
      
      # Complete inference summary
      summary(fit)          # Includes R¬≤, F-test, t-tests, standard errors
      
      # ANOVA table
      anova(fit)           # Or summary(aov(fit))

   **What summary(fit) Provides**:

   - Coefficient estimates (:math:`b_0`, :math:`b_1`) with standard errors
   - t-statistics and p-values for testing each coefficient equals zero
   - R-squared and adjusted R-squared
   - F-statistic and p-value for overall model utility
   - Residual standard error (estimate of :math:`\sigma`)

   **Manual Calculations** (useful for understanding or when given summary statistics):

   .. code-block:: r

      # Calculate standard error of slope manually
      SE_b1 <- sqrt(MSE / Sxx)
      
      # Calculate t-statistic
      t_stat <- (b1 - beta1_null) / SE_b1
      
      # Calculate confidence interval
      t_critical <- qt(alpha/2, df = n-2, lower.tail = FALSE)
      CI_lower <- b1 - t_critical * SE_b1
      CI_upper <- b1 + t_critical * SE_b1
      
      # Calculate p-values
      p_value_two_sided <- 2 * pt(abs(t_stat), df = n-2, lower.tail = FALSE)
      p_value_F <- pf(F_stat, df1 = 1, df2 = n-2, lower.tail = FALSE)


..
   This part is more appropriate for lession 4. And an example going through the whole workflow
   already exists in the following section.
   Summary of Linear Regression Workflow
   -----------------------------------------------

   **1. Exploratory Analysis**

   - Determine which variable should be explanatory vs. response
   - Create scatter plot to assess initial relationship 

   **2. Model Fitting**

   - Fit least squares regression line and calculate basic summary statistics such as
   the slope and parameter estimates, MSE, correlation coefficient, and R-squared.

   **3. Diagnostic Checking** (Critical before inference!)

   - Create residual plots to check linearity and constant variance.
   - Examine histograms and QQ plots of residuals for normality.
   - Identify any influential points or assumption violations.

   **4. Inference** (only if assumptions are reasonable)

   - Perform :math:`F`-test for overall model utility and inference for individual parameters.
   - Interpret results in context of the original problem.

   **5. Predictions** (if the model utility is significant)

   - Make predictions with appropriate uncertainty quantification. Covered in the upcoming lesson.

Bring It All Tgether
---------------------

.. admonition:: Key Takeaways üìù
   :class: important

   1. **Diagnostic plots** are essential for verifying model assumptions 
      before conducting hypothesis tests or constructing confidence intervals.

   2. **Four key diagnostic tools work together**: scatter plots, residual plots, histograms of residuals, and QQ plots 
      provide complementary information about different aspects of model adequacy.

   3. **The model utility F-test** assesses whether the linear relationship explains a significant portion of 
      the variability in the response variable.

   4. Inference on the slope and intercept **adapts familiar principles** used in previous inference procedures
      on single parameters. 

   6. In simple linear regression, the model utility **F-test** is equivalent
      to the :math:`t`-test on :math:`H_a: \beta_1 \neq 0`.

Exercises
---------

.. admonition:: Exercise 1: Diagnostic Plot Interpretation
   :class: note

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch13-3/fig1_residual_patterns.png
      :width: 95%
      :align: center

      Four residual plots showing different patterns

   For each residual plot above (A, B, C, D), identify:

   a. Whether the linearity assumption appears satisfied

   b. Whether the constant variance (homoscedasticity) assumption appears satisfied

   c. Any outliers or influential points

   d. Overall assessment: Is the linear regression model appropriate?

   .. dropdown:: Solution
      :class-container: sd-border-success

      +--------+------------+-----------------+----------+-------------------------+
      | Plot   | Linearity  | Constant Var    | Outliers | Model Appropriate?      |
      +========+============+=================+==========+=========================+
      | A      | ‚úì Satisfied| ‚úì Satisfied     | None     | Yes - proceed           |
      +--------+------------+-----------------+----------+-------------------------+
      | B      | ‚úì Satisfied| ‚úó Funnel pattern| None     | No - variance issue     |
      +--------+------------+-----------------+----------+-------------------------+
      | C      | ‚úó Curved   | ‚úì Satisfied     | None     | No - linearity violated |
      +--------+------------+-----------------+----------+-------------------------+
      | D      | ‚úì Satisfied| ‚úì Satisfied     | Yes (3)  | Caution - investigate   |
      +--------+------------+-----------------+----------+-------------------------+

----

.. admonition:: Exercise 2: Complete Diagnostic Analysis
   :class: note

   A software engineer models the relationship between lines of code (X, in thousands) and execution time (Y, in seconds) for :math:`n = 25` programs.

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/Exercises/ch13-3/fig2_full_diagnostics.png
      :width: 95%
      :align: center

      Diagnostic plots: scatter plot, residual plot, histogram, and QQ-plot

   Based on the four diagnostic plots:

   a. Assess the linearity assumption using the scatter plot and residual plot.

   b. Assess the constant variance assumption using the residual plot.

   c. Assess the normality assumption using the histogram and QQ-plot.

   d. Are there any apparent outliers? If so, describe them.

   e. Would you proceed with inference based on this model? Justify your answer.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Linearity assessment**

      - **Scatter plot:** Shows a clear positive linear trend with points following an approximately straight pattern
      - **Residual plot:** Residuals appear randomly scattered around zero with no systematic curvature

      **Conclusion:** Linearity assumption is satisfied ‚úì

      **Part (b): Constant variance assessment**

      - **Residual plot:** The vertical spread of residuals appears roughly constant across all fitted values
      - No funnel shape or systematic change in spread

      **Conclusion:** Equal variance assumption is satisfied ‚úì

      **Part (c): Normality assessment**

      - **Histogram:** Residuals show an approximately symmetric, bell-shaped distribution
      - The kernel density (red) and normal curve (blue) align reasonably well
      - **QQ-plot:** Points fall close to the reference line with no major systematic departures

      **Conclusion:** Normality assumption is satisfied ‚úì

      **Part (d): Outliers**

      No obvious outliers. All points appear consistent with the general pattern, and no residuals are extremely far from zero.

      **Part (e): Proceed with inference?**

      **Yes**, inference is appropriate. All four assumptions (LINE) appear to be satisfied:

      - **L**\inearity: Yes (scatter and residual plots)
      - **I**\ndependence: Assumed (no time ordering mentioned)
      - **N**\ormality: Yes (histogram and QQ-plot)
      - **E**\qual variance: Yes (residual plot)

      The model is well-suited for hypothesis tests and confidence intervals.

----

.. admonition:: Exercise 3: ANOVA F-Test for Model Utility
   :class: note

   An industrial engineer studies the relationship between conveyor belt speed (ft/min) and defect rate (defects per 1000 units). With :math:`n = 18` production runs:

   - SSR = 245.6
   - SSE = 89.4

   a. State the hypotheses for testing model utility.

   b. Complete the ANOVA table and calculate the F-statistic.

   c. Find the p-value using R: ``pf(F_stat, 1, 16, lower.tail = FALSE)``

   d. At :math:`\alpha = 0.05`, what is your conclusion? State it in context using proper conclusion format.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Hypotheses**

      - :math:`H_0: \beta_1 = 0` (There is no linear relationship between conveyor speed and defect rate)
      - :math:`H_a: \beta_1 \neq 0` (There is a linear relationship between conveyor speed and defect rate)

      **Part (b): ANOVA Table**

      .. list-table::
         :header-rows: 1
         :widths: 20 10 15 15 10

         * - Source
           - df
           - SS
           - MS
           - F
         * - Regression
           - 1
           - 245.6
           - 245.6
           - 43.97
         * - Error
           - 16
           - 89.4
           - 5.5875
           - 
         * - Total
           - 17
           - 335.0
           - 
           - 

      Calculations:

      - :math:`df_{Error} = n - 2 = 18 - 2 = 16`
      - :math:`MSR = 245.6/1 = 245.6`
      - :math:`MSE = 89.4/16 = 5.5875`
      - :math:`F = MSR/MSE = 245.6/5.5875 = 43.97`

      **Part (c): P-value**

      .. code-block:: r

         pf(43.97, 1, 16, lower.tail = FALSE)
         # p-value = 6.86e-06

      **Part (d): Conclusion**

      At :math:`\alpha = 0.05`, since p-value (6.86 √ó 10‚Åª‚Å∂) < 0.05, we **reject** :math:`H_0`.

      There is sufficient evidence to conclude that there is a significant linear relationship between conveyor belt speed and defect rate.

----

.. admonition:: Exercise 4: Inference for the Slope
   :class: note

   From a regression of drug dosage (mg) on patient recovery time (days) with :math:`n = 24` patients:

   - :math:`b_1 = -0.85` (days per mg)
   - :math:`SE_{b_1} = 0.23`

   a. Construct a 95% confidence interval for the true slope :math:`\beta_1`.

   b. Interpret this confidence interval in context.

   c. Test :math:`H_0: \beta_1 = 0` vs :math:`H_a: \beta_1 \neq 0` at :math:`\alpha = 0.05`.

      - State the test statistic formula and calculate :math:`t_{TS}`
      - Find the p-value
      - State your conclusion in context using proper format

   d. Is the result of part (c) consistent with your confidence interval in part (a)? Explain.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): 95% CI for slope**

      Degrees of freedom: :math:`df = n - 2 = 24 - 2 = 22`

      Critical value: :math:`t_{0.025, 22} = 2.074`

      .. math::

         CI = b_1 \pm t^* \cdot SE_{b_1} = -0.85 \pm 2.074(0.23) = -0.85 \pm 0.477

      **95% CI: (‚àí1.33, ‚àí0.37)**

      **Part (b): Interpretation**

      We are 95% confident that the true slope :math:`\beta_1` is between ‚àí1.33 and ‚àí0.37 days per mg.

      In context: For each additional mg of drug dosage, recovery time decreases by between 0.37 and 1.33 days, on average.

      **Part (c): Hypothesis test**

      **Step 1:** Parameter: :math:`\beta_1` = true change in recovery time per mg increase in dosage

      **Step 2:** :math:`H_0: \beta_1 = 0` vs. :math:`H_a: \beta_1 \neq 0`

      **Step 3:** Test statistic:

      .. math::

         t_{TS} = \frac{b_1 - 0}{SE_{b_1}} = \frac{-0.85}{0.23} = -3.70

      P-value (two-sided): ``2 * pt(abs(-3.70), 22, lower.tail = FALSE)`` = 0.0012

      **Step 4:** At :math:`\alpha = 0.05`, since p-value (0.0012) < 0.05, we **reject** :math:`H_0`.

      There is sufficient evidence to conclude that drug dosage has a significant linear effect on recovery time.

      **Part (d): Consistency**

      **Yes**, the results are consistent. The CI (‚àí1.33, ‚àí0.37) does not contain 0, which corresponds to rejecting :math:`H_0: \beta_1 = 0`. This illustrates the duality between confidence intervals and hypothesis tests.

----

.. admonition:: Exercise 5: Inference for the Intercept
   :class: note

   Using the drug dosage regression from Exercise 4:

   - :math:`b_0 = 14.2` (days)
   - :math:`SE_{b_0} = 1.85`
   - :math:`n = 24`

   a. Interpret :math:`b_0` in context. Is this interpretation meaningful?

   b. Construct a 90% confidence interval for :math:`\beta_0`.

   c. Test :math:`H_0: \beta_0 = 12` vs :math:`H_a: \beta_0 \neq 12` at :math:`\alpha = 0.10`.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Interpretation of b‚ÇÄ**

      :math:`b_0 = 14.2` represents the predicted recovery time (in days) when drug dosage is 0 mg.

      **Is it meaningful?** Only if dosage = 0 is within or near the range of observed data. If the study only included positive dosages (e.g., 5-50 mg), then :math:`b_0` is an extrapolation and may not have practical meaning. It would represent the baseline recovery time without the drug.

      **Part (b): 90% CI for Œ≤‚ÇÄ**

      :math:`df = 22`, :math:`t_{0.05, 22} = 1.717`

      .. math::

         CI = 14.2 \pm 1.717(1.85) = 14.2 \pm 3.18

      **90% CI: (11.02, 17.38)**

      **Part (c): Test H‚ÇÄ: Œ≤‚ÇÄ = 12**

      Test statistic:

      .. math::

         t_{TS} = \frac{b_0 - 12}{SE_{b_0}} = \frac{14.2 - 12}{1.85} = \frac{2.2}{1.85} = 1.19

      P-value: ``2 * pt(1.19, 22, lower.tail = FALSE)`` = 0.247

      At :math:`\alpha = 0.10`, since p-value (0.247) > 0.10, we **fail to reject** :math:`H_0`.

      There is not sufficient evidence to conclude that the true intercept differs from 12 days.

----

.. admonition:: Exercise 6: F-test and t-test Relationship
   :class: note

   For a simple linear regression with :math:`n = 30` observations, the following results were obtained:

   - t-statistic for testing :math:`H_0: \beta_1 = 0`: :math:`t_{TS} = 4.28`
   - F-statistic for model utility: :math:`F_{TS} = 18.32`

   a. Verify that :math:`F_{TS} = t_{TS}^2` (allowing for rounding).

   b. What are the degrees of freedom for the t-test? For the F-test?

   c. Why is this relationship only true for simple linear regression (one predictor)?

   d. Using ``pt()`` and ``pf()``, verify that the p-values are the same.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Verify F = t¬≤**

      :math:`t_{TS}^2 = (4.28)^2 = 18.32 = F_{TS}` ‚úì

      The relationship holds exactly.

      **Part (b): Degrees of freedom**

      - **t-test:** :math:`df = n - 2 = 30 - 2 = 28`
      - **F-test:** :math:`df_1 = 1` (numerator), :math:`df_2 = n - 2 = 28` (denominator)

      **Part (c): Why only for simple linear regression?**

      This relationship :math:`F = t^2` holds only when:

      1. There is exactly one predictor (df‚ÇÅ = 1 for the F-test)
      2. The t-test is for :math:`H_0: \beta_1 = 0`

      In multiple regression with :math:`k > 1` predictors, the overall F-test has :math:`df_1 = k`, and there's no single t-statistic that corresponds to it. Each predictor has its own t-test, but these are not equivalent to the overall F-test.

      **Part (d): P-value verification**

      .. code-block:: r

         # t-test p-value (two-sided)
         2 * pt(4.28, 28, lower.tail = FALSE)
         # [1] 0.000196
         
         # F-test p-value
         pf(18.32, 1, 28, lower.tail = FALSE)
         # [1] 0.000196

      The p-values are identical (within rounding), confirming the equivalence of the two tests for simple linear regression.

----

.. admonition:: Exercise 7: Comprehensive Hypothesis Test
   :class: note

   A civil engineer studies the relationship between traffic volume (vehicles per hour) and road surface wear (mm of depth reduction per year). Data from :math:`n = 15` road sections yields:

   - :math:`\hat{y} = 0.12 + 0.0008x`
   - :math:`R^2 = 0.72`
   - :math:`MSE = 0.0045`
   - :math:`S_{XX} = 125000000`

   a. Calculate :math:`SE_{b_1}`.

   b. Test whether there is a significant positive relationship between traffic volume and road wear at :math:`\alpha = 0.01`.

      **Step 1:** Define the parameter of interest.

      **Step 2:** State :math:`H_0` and :math:`H_a` (in symbols and words).

      **Step 3:** Calculate the test statistic and p-value. Report degrees of freedom.

      **Step 4:** State your conclusion in context.

   c. Construct a 99% confidence interval for :math:`\beta_1`.

   d. Construct a 99% lower confidence bound for :math:`\beta_1`.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Calculate SE_b‚ÇÅ**

      .. math::

         SE_{b_1} = \sqrt{\frac{MSE}{S_{XX}}} = \sqrt{\frac{0.0045}{125000000}} = \sqrt{3.6 \times 10^{-11}} = 1.90 \times 10^{-5}

      **Part (b): Four-Step Hypothesis Test**

      **Step 1:** Let :math:`\beta_1` = the true change in road wear (mm/year) per additional vehicle per hour.

      **Step 2:** 
      
      - :math:`H_0: \beta_1 = 0` (no linear relationship between traffic volume and road wear)
      - :math:`H_a: \beta_1 > 0` (positive linear relationship ‚Äî more traffic causes more wear)

      **Step 3:** Test statistic and p-value

      .. math::

         t_{TS} = \frac{b_1 - 0}{SE_{b_1}} = \frac{0.0008}{1.90 \times 10^{-5}} = 42.1

      :math:`df = n - 2 = 13`

      P-value: ``pt(42.1, 13, lower.tail = FALSE)`` ‚âà 0 (essentially zero)

      **Step 4:** At :math:`\alpha = 0.01`, since p-value ‚âà 0 < 0.01, we **reject** :math:`H_0`.

      There is sufficient evidence to conclude that there is a significant positive linear relationship between traffic volume and road surface wear.

      **Part (c): 99% CI for Œ≤‚ÇÅ**

      :math:`t_{0.005, 13} = 3.012`

      .. math::

         CI = 0.0008 \pm 3.012(1.90 \times 10^{-5}) = 0.0008 \pm 0.0000572

      **99% CI: (0.000743, 0.000857)**

      **Part (d): 99% Lower Confidence Bound**

      For a one-sided bound: :math:`t_{0.01, 13} = 2.650`

      .. math::

         LCB = b_1 - t_{0.01, 13} \cdot SE_{b_1} = 0.0008 - 2.650(1.90 \times 10^{-5}) = 0.000750

      **99% LCB: 0.000750**

      We are 99% confident that each additional vehicle per hour increases road wear by at least 0.00075 mm/year.

----

.. admonition:: Exercise 8: Model Utility with R Output
   :class: note

   The following R output is from a regression of patient systolic blood pressure (mmHg) on age (years):

   .. code-block:: text

      Call:
      lm(formula = BP ~ Age, data = patients)
      
      Residuals:
          Min      1Q  Median      3Q     Max 
      -15.234  -5.891  -0.456   4.789  18.234
      
      Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
      (Intercept)  95.4500     5.2340  18.236  < 2e-16 ***
      Age           0.8920     0.1120   7.964 2.34e-09 ***
      
      Residual standard error: 8.456 on 38 degrees of freedom
      Multiple R-squared:  0.6253, Adjusted R-squared:  0.6154 
      F-statistic: 63.43 on 1 and 38 DF,  p-value: 2.341e-09

   a. Write the fitted regression equation.

   b. Interpret the slope in context.

   c. What is the estimate of :math:`\sigma`?

   d. What is :math:`R^2`? Interpret it.

   e. Is the model statistically significant at :math:`\alpha = 0.01`? Cite the relevant test statistic and p-value.

   f. Construct a 95% confidence interval for the slope using the output.

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Fitted equation**

      :math:`\widehat{BP} = 95.45 + 0.892 \times Age`

      Or: Predicted Systolic BP = 95.45 + 0.892(Age)

      **Part (b): Slope interpretation**

      For each additional year of age, systolic blood pressure increases by an estimated 0.892 mmHg, on average.

      **Part (c): Estimate of œÉ**

      :math:`\hat{\sigma} = 8.456` mmHg (residual standard error from output)

      **Part (d): R¬≤ interpretation**

      :math:`R^2 = 0.6253`

      Approximately 62.5% of the variation in systolic blood pressure is explained by the linear relationship with age.

      **Part (e): Model significance**

      **Yes**, the model is statistically significant at :math:`\alpha = 0.01`.

      - F-statistic = 63.43 on 1 and 38 DF
      - p-value = 2.341 √ó 10‚Åª‚Åπ < 0.01

      (Equivalently: t = 7.964 for the slope, p-value = 2.34 √ó 10‚Åª‚Åπ)

      **Part (f): 95% CI for slope**

      :math:`t_{0.025, 38} = 2.024`

      .. math::

         CI = 0.892 \pm 2.024(0.112) = 0.892 \pm 0.227

      **95% CI: (0.665, 1.119)**

      We are 95% confident that for each additional year of age, systolic BP increases by between 0.67 and 1.12 mmHg.

----

.. admonition:: Exercise 9: Assumption Checking in R
   :class: note

   A researcher has fitted a regression model and needs to check assumptions. Complete the following R code to create proper diagnostic plots:

   .. code-block:: r

      # Fit the model
      fit <- lm(y ~ x, data = mydata)
      
      # Create residual plot
      mydata$fitted <- ____________
      mydata$residuals <- ____________
      
      ggplot(mydata, aes(x = fitted, y = residuals)) +
        geom_point() +
        geom_hline(yintercept = ____, color = "blue", linewidth = 1) +
        labs(title = "Residual Plot",
             x = "____________",
             y = "____________") +
        theme_minimal()
      
      # Create QQ-plot of residuals
      ggplot(mydata, aes(sample = ____________)) +
        stat_qq() +
        stat_qq_line(color = "red", linewidth = 1) +
        labs(title = "Normal QQ-Plot of Residuals",
             x = "Theoretical Quantiles",
             y = "Sample Quantiles") +
        theme_minimal()

   a. Fill in the blanks to complete the code.

   b. What pattern in the residual plot would indicate non-constant variance?

   c. What pattern in the QQ-plot would indicate heavy-tailed residuals?

   .. dropdown:: Solution
      :class-container: sd-border-success

      **Part (a): Completed code**

      .. code-block:: r

         # Fit the model
         fit <- lm(y ~ x, data = mydata)
         
         # Create residual plot
         mydata$fitted <- fitted(fit)
         mydata$residuals <- residuals(fit)
         
         ggplot(mydata, aes(x = fitted, y = residuals)) +
           geom_point() +
           geom_hline(yintercept = 0, color = "blue", linewidth = 1) +
           labs(title = "Residual Plot",
                x = "Fitted Values",
                y = "Residuals") +
           theme_minimal()
         
         # Create QQ-plot of residuals
         ggplot(mydata, aes(sample = residuals)) +
           stat_qq() +
           stat_qq_line(color = "red", linewidth = 1) +
           labs(title = "Normal QQ-Plot of Residuals",
                x = "Theoretical Quantiles",
                y = "Sample Quantiles") +
           theme_minimal()

      **Part (b): Non-constant variance pattern**

      A **funnel shape** (or megaphone/cone pattern) in the residual plot indicates non-constant variance:

      - **Opening funnel:** Variance increases with fitted values (common)
      - **Closing funnel:** Variance decreases with fitted values (less common)

      The vertical spread of residuals should be roughly constant across all fitted values for the assumption to be satisfied.

      **Part (c): Heavy-tailed residuals**

      In the QQ-plot, heavy-tailed residuals show an **S-shape**:

      - Points curve **below** the reference line on the left side
      - Points curve **above** the reference line on the right side

      This indicates more extreme values than a normal distribution would produce.

----

Additional Practice Problems
----------------------------

**Quick Calculations**

1. Given :math:`b_1 = 3.2`, :math:`SE_{b_1} = 0.8`, :math:`n = 22`. Calculate the t-statistic for :math:`H_0: \beta_1 = 0`.

2. With :math:`t_{TS} = 2.5` and :math:`df = 20`, is the result significant at :math:`\alpha = 0.05` (two-sided)?

3. If :math:`F = 16` for the model utility test, what is :math:`|t|` for the slope test?

**True/False**

4. If the 95% CI for :math:`\beta_1` contains 0, the F-test will not reject :math:`H_0`.

5. The F-test and t-test for the slope always give identical conclusions in simple linear regression.

6. A significant p-value guarantees the model assumptions are satisfied.

.. dropdown:: Answers to Practice Problems
   :class-container: sd-border-success

   **Quick Calculations:**

   1. :math:`t_{TS} = \frac{b_1 - 0}{SE_{b_1}} = \frac{3.2}{0.8} = 4.0`

   2. Critical value: :math:`t_{0.025, 20} = 2.086`. Since :math:`|2.5| > 2.086`, **yes**, significant at :math:`\alpha = 0.05`.

   3. :math:`|t| = \sqrt{F} = \sqrt{16} = 4`

   **True/False:**

   4. **TRUE** ‚Äî Due to the duality between CIs and hypothesis tests. If 0 is in the CI, the test doesn't reject :math:`H_0: \beta_1 = 0`, which is equivalent to the F-test in SLR.

   5. **TRUE** ‚Äî For simple linear regression, :math:`F = t^2` and the p-values are identical.

   6. **FALSE** ‚Äî A significant p-value only means we reject :math:`H_0`. Model assumptions must be verified separately through diagnostic plots, not hypothesis tests.