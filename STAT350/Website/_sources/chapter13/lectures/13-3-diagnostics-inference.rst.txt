.. _13-3-diagnostics-inference:

Model Diagnostics and Statistical Inference
==========================================================


Having developed several point estimates related to simple linear regression,
we now face the critical stage of uncertainty quantification using confidence regions and 
hypothesis tests. Before conducting any statistical inference, however, we must 
first verify that our model assumptions are reasonable.

.. admonition:: Road Map ðŸ§­
   :class: important

   * Understand why all assumption-checking procedures involve the residuals.
   * List the appropriate graphcial tools to assess each linear regression assumption, and systematically search for
     signs of assumption violation in each plot.
   * Perform inference on the overall model utility and on the model parameters.

----------------------------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch13-6">
      <iframe
         id="video-ch13-6"
         title="STAT 350 â€“ Chapter 13.6 Diagnostics for Model Assumptions Video"
         src="https://www.youtube.com/embed/p8kRL-vUpVo?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

.. admonition:: Slides ðŸ“Š
   :class: tip

   `Download Chapter 13 slides (PPTX) <https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/
   slides/Chapter%2013%20Linear%20Regression/SimpleLinearRegression_AC.pptx>`_
   
Preliminaries for Model Diagnostics
---------------------------------------

Review of Simple Linear Regression Assumptions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The simple linear regression model 

.. math:: 
   :label: model

   Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
   
requires four key assumptions:

1. For each given :math:`x_i`, :math:`Y_i` is a size-1 **simple random sample** from the distribution of 
   all possible responses, :math:`Y|X=x_i`.
2. The association between the explanatory and response variables is **linear** on average.
3. The error terms are **normally distributed**:
   :math:`\varepsilon_i \stackrel{iid}{\sim} N(0, \sigma^2) \quad \text{for } i = 1, 2, \ldots, n.`
4. The error terms have **constant variance** :math:`\sigma^2` across all values of :math:`X`. 

For detailed discussion on the assumptions, revisit :numref:`assumptions`.

Overview of Residuals and Their Properties
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Note that :math:`\varepsilon_i`'s are the only random component of the current linear regression model, and therefore,
all model assumptions are essentially assumptions about the behavior of the error terms.

Also recall the mathematical definition of residuals: 

.. math:: 
   e_i = y_i - \hat{y}_i, \text{ for each } i = 1,2,\cdots, n.

Residuals can be viewed as the observed counterparts of the true error terms :math:`\varepsilon_i` because:

.. math::
   \varepsilon_i = y_i - (\beta_0 + \beta_1 x_i) \approx y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i) = y_i - \hat{y}_i = e_i.

Since we do not have access to the true realizations of :math:`\varepsilon_i`, the residuals instead 
play a key role in model diagnostics.

Scatter Plot and Residual Plot for Linearity and Constant Variance
----------------------------------------------------------------------------

We have used scatter plots for the preliminary assessment of the association between two quantitative variables.
We introduce the **residual plot** as an additional tool for model diagnosis; it is simply 
a scatter plot of :math:`(x_i, e_i)` for :math:`i=1,\cdots, n`.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/scatter-vs-residual.png
   :width: 90%
   :align: center
   :alt: Comparison of scatter plot and residual plot
   
   Comparison of scatter plot and residual plot
   
A residual plot can be viewed as the original scatter plot rotated so that the regression line becomes horizontal. This
removes the visual bias of the tilted trend line and highlights the deviations 
from the fitted line.

Scatter plots and residual plots are typically used together to assess two assumptions: 
**linearity and constant variance.**

Characteristics of Ideal Scatter and Residual Plots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When the linearity and constant variance assumptions hold, 
the points on both the scatter plot and residual plot form a random scatter around their respective summary lines, 
with a roughly constant spread across the :math:`x`-axis.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/residual-no-violation.png
   :width: 70%
   :align: center
   :alt: A residual plot showing no signs of assumption violation
   
   A residual plot showing no signs of assumption violation

Signs of Assumption Violation on Scatter and Resiual Plots
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Violation of the Linearity Assumption
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If residuals systematically fall below the zero line in certain regions of :math:`x` values and
above the line in others, this suggests the true relationship is non-linear. 

.. _linearity-violation:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/residual-linearity.jpg
   :width: 70%
   :align: center
   :alt: Resiual plots with linearity assumption violated
   
   Resiual plots with linearity assumption violated

2. Violation of the Constant Variance Assumption
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If the spread of points around the trend line is inconsistent, the data likely violate the constant variance assumption.

.. _variance-violation:
.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/residual-variance.png
   :width: 90%
   :align: center
   :alt: Resiual plots with constant variance assumption violated
   
   Resiual plots with constant variance assumption violated

* **Hourglass Pattern**:  The spread is larger for extreme values of :math:`X` than in the 
  middle range.

* **Cone Pattern**: As :math:`X` increases, the residual errors become larger.

Other patterns of non-constant variance can also occur. Even without systematic patterns, an inconsistent spread alone is a 
significant sign of assumption violation.

3. Outliers
^^^^^^^^^^^^^^

Recall that scatter plots are also used to identify potential outliers and influential points. 
Residual plots can serve the same purpose.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/residual-outliers.png
   :width: 90%
   :align: center
   :alt: Resiual plots with outliers
   
   Resiual plots with outliers; left plot shows a :math:`y`-outlier, and right shows an :math:`x`-outlier that
   potentially has high influence on the model.

.. admonition:: Violations May or May not Occur Simultaneously
   :class: danger

   While :numref:`linearity-violation` show signs of non-linearity, it does not violate the constant variance assumption since the
   bandwidth of data points around their true summary curve (quadratic association) is reasonably constant. 

   Likewise, :numref:`variance-violation` shows no sign of non-linearity, as the patterns of spread above and below the zero line
   are roughly symmetric.

   Be aware that different types of violations may occur simultaneously or separately, and be prepared to distinguish
   one case from another on the graphs. 
   
   **Exercise:** draw a residual plot that shows violation of lineary and constant variance assumptions simultaneously.

Histogram and Normal Probability Plot of Residuals for Normality
----------------------------------------------------------------------------

To verify normality of the error terms, we construct a histogram and a normal probability plot of
the residuals.

.. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/hist-qq-residuals.png
      :width: 90%
      :align: center
      :alt: Histogram and normal probability plot of residuals

      Histogram and normal probability plot of residuals

These plots are used in the same way as in any prior context of normality assessment.
We check whether the points follow a straight line on the normal probability plot and whether the histogram is bell-shaped.

Summary of Diagnostic Tools and Comprehensive Eaxmples
----------------------------------------------------------

We have introduced four graphical tools to assess the assumptions of linear regression. See the
table below for a summary:

.. flat-table::
   :header-rows: 1
   

   * - Assumption
     - Graphical Tools

   * - SRS
     - None (must be ensured through experimental design)

   * - Linearity
     - Scatter plot and residual plot 

   * - Constant variance
     - Scatter plot and residual plot
   
   * - Normality of errors
     - Histogram and QQ plot of residuals

.. admonition:: Example  ðŸ’¡: Comprehensive Diagnostic Exercise
   :class: note 

   For each of the following sets of graphical representations, assess whether the data set meets the
   assumptions required for valid linear regression.

   **Example 1**

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/graphical-diag-example1.png
      :width: 90%
      :align: center
      :alt: First example of comprehensive model diagnostics

      Example 1 of graphical diagnosis

   - Scatter plot and residual plot show a systematic **curved pattern**.
   - If the correct summary curve is used (quadratic form), then the size of their vertical spread
     would not vary by the :math:`x`-values. So non-constant variance is not a concern.
   - Histogram and normal probability plots look reasonable.

   This data set violates the linearity assumption.


   **Example 2**

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/graphical-diag-example2.png
      :width: 90%
      :align: center
      :alt: Second example of comprehensive model diagnostics

      Example 2 of graphical diagnosis

   - The dots follow a linear pattern according to both the scatter plot and the residual plotâ€”they form mirror 
     images below and above their summary lines.
   - The spread of the points are narrower in the two ends than in the middle region. This raises a concern
     for **non-constant variance**.
   - The histogram and the QQ plot indicate that the error distribution has **heavier tails than the blue curve**. 
     The error distribution may not be normal.

   This data set violates the constant-variance assumption and is suspected to have non-normal error terms.

..
   .. admonition:: Example : Blood Pressure Study, Continued
      :class: note 
         
      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-diagnostics.png
         :width: 80%
         
         Diagnostic analysis of the blood pressure treatment study showing scatter plot with fitted line

      Let's apply our diagnostic procedures to the blood pressure treatment study, where we examined the relationship between patient age and change in blood pressure after 24 hours of treatment.

      **Scatter Plot Assessment**:

      Looking at the scatter plot with the fitted line :math:`\hat{y} = 20.12 - 0.5263x`, we can trace our finger across the plot to assess the spread of points around the line. The linear relationship appears reasonable, though with only 11 observations, it's challenging to definitively assess the constant variance assumption.

      **Residual Plot Analysis**:

      .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-residuals.png
         :width: 80%
         
         Residual plot for the blood pressure data showing individual residuals labeled by vehicle type

      The residual plot for the car efficiency data shows each vehicle's residual clearly labeled. When we examine the spread across different cylinder volume ranges:

      - Left region (around 1.5L): Limited observations make assessment difficult
      - Middle region (around 1.8-2.0L): Several observations with varied residuals
      - Right region (around 2.5L): Adequate spread above and below zero

      The residual plot suggests potential minor violations of the constant variance assumption, but nothing strong enough to invalidate our analysis given the small sample size.

When Assumptions Are Violated
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If diagnostic procedures reveal serious assumption violations, we should not proceed with linear regression analysis. Instead,
we should take an appropriate measure to mitigate the effect of the violation or use a different analysis method.

**Linearity Violations**:

- Consider transformations of variables (log, square root, etc.)
- Fit non-linear models (beyond course scope)
- Use piecewise or segmented regression for different regions

**Constant Variance Violations**:

- Variable transformations may help stabilize variance
- Weighted least squares methods (beyond course scope)
- Robust regression techniques

**Normality Violations**:

- Often less critical for large sample sizes due to Central Limit Theorem
- Bootstrap methods for inference (beyond course scope)
- Non-parametric alternatives

**Independence Violations**:

- Time series methods for temporally correlated observations
- Mixed effects models for clustered data


----------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch13-7">
      <iframe
         id="video-ch13-7"
         title="STAT 350 â€“ Chapter 13.7 Simple Linear Regression Model Inference - F-test Video"
         src="https://www.youtube.com/embed/mTQ3GU9rpys?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>

The F-Test for Model Utility
----------------------------------

Once we have verified that our model assumptions are reasonably satisfied, we proceed with statistical inference. 
The first question we typically ask is: "Does the simple linear regression model provide useful information about 
the relationship between the explanatory and response variables?" 

We organize our answer to this question
into the usual four-step hypothesis test. We refer to the ANOVA table constructed in Chapter 13.2.

Step 1: Parameter of Interest
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The focus of this test is on the overall usefulness of the linear relationship rather than on specific parameters. 
Therefore, it is okay to skip explicit parameter definition.

Step 2: Hypotheses
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* :math:`H_0:` There is no linear association between :math:`X` and :math:`Y`.
* :math:`H_0:` There is a linear association between :math:`X` and :math:`Y`.

**Important**: Always state the hypotheses using the experimental context, replacing "X" and "Y" with 
the actual variable names and providing sufficient background information.

Step 3: Test Statistic and P-value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Recall that MSR is a scaled measurement of the variability attributed to the
model structure, while MSE is a scaled measure of random error. Therefore, if MSR is 
substantially greater than MSE, we can consider the model to be statistically significant.

The corresponding test statistic is:

.. math::

   F_{TS} = \frac{\text{MSR}}{\text{MSE}},

with degrees of freedom :math:`df_1 = 1` and :math:`df_2 = n-2`.

A large observed value of :math:`F_{TS}` is in favor of the alternative hypothesis that the model is significant. Therefore, 

.. math::
   \text{p-value} = P(F_{1,  n-2} > f_{TS}).

Step 4: Decision and Conclusion
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- If p-value â‰¤ :math:`\alpha`: 

  Reject :math:`H_0`. At the :math:`\alpha` significance level, we have sufficient 
  evidence to conclude that there is a linear association between [explanatory variable] and [response variable] in [context]."

- If p-value > :math:`\alpha`: 

  Fail to reject :math:`H_0`. At the :math:`\alpha` significance level, we do not have sufficient evidence to 
  conclude that there is a linear association between [explanatory variable] and [response variable] in [context]."

.. admonition:: Example ðŸ’¡: Blood Pressure Study, Continued
   :class: note 

   Assuming that the assumptions of linear regression have been verified, perform the 
   :math:`F`-test for model utility using :math:`\alpha = 0.05.`
   Use the partially filled ANOVA table from the previous lesson:

   .. list-table:: 
      :header-rows: 1
      :widths: 10 5 10 10 10 10

      *  - Source
         - df
         - Sum of Squares
         - Mean Square
         - F-statistic
         - p-value
      *  - Regression
         - 1
         - :math:`555.7126`
         - :math:`555.7126`
         - ?
         - ?
      *  - Error
         - :math:`9`
         - :math:`382.8267`
         - :math:`42.5363`
         - 
         - 
      *  - Total
         - :math:`10`
         - :math:`938.5393`
         - 
         - 
         - 
         
   **Step 1: Parameter Definition**

   This step is skipped for model utility tests.

   **Step 2: Hypotheses**:

   - :math:`H_0`: There is no linear association between patient age and change in blood pressure.
   - :math:`H_a`: There is a linear association between patient age and change in blood pressure.

   **Step 3: Computation**

   .. math::
      f_{TS} = \frac{\text{MSR}}{\text{MSE}} = \frac{555.7126}{42.5363} = 13.06

   The test statistic has two degrees of freedom: :math:`df_1=1` and :math:`df_2=9`. The :math:`p`-value is:

   .. math:: 
      P(F_{1,9} > 13.06) = 0.0055.

   **Step 4: Conclusion**

   At :math:`\alpha = 0.05`, we reject :math:`H_0` and conclude that there is sufficient evidence of a 
   linear association between patient age and change in blood pressure.


------------------------------------------------------------------

.. raw:: html

   <div class="video-placeholder" role="group" aria-labelledby="video-ch13-8">
      <iframe
         id="video-ch13-8"
         title="STAT 350 â€“ Chapter 13.8 Simple Linear Regression Model Inference - Slope and Intercept Video"
         src="https://www.youtube.com/embed/_XCCR_oXcL0?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
         allowfullscreen>
      </iframe>
   </div>


Distributional Properties of the Slope and Intercept Estimators
------------------------------------------------------------------

To develop inference procedures for :math:`\beta_0` and :math:`\beta_1`, we need to understand the statistical properties 
of their **estimators**, :math:`b_0` and :math:`b_1`. This draws distinction from our focus on
:math:`b_0` and :math:`b_1` as **estimates** so far, yielding one set of realized values based on a single data. 
We would now like to study their behaviors across many different datasets.

1. Slope and Intecept Estimators Are Linear Combinations of the Responses
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The first step in constructing inference for regression parameters is to recognize that their estimators are
linear combinations of the response variables, :math:`Y_i, i=1,\cdots, n`. Capital :math:`Y` is
used throughout this section to emphasize that we are discussing responses as **random variables**, not their observed values.
The values of the explanatory variable are still considered given and fixed.

We first show that the slope estimator :math:`b_1` is a linear combination of :math:`Y_i, i=1,\cdots, n`, starting with its
definition:

.. math::
   :label: slope-linear-comb

   b_1 &= \frac{\sum_{i=1}^n x_iY_i - n\bar{x}\bar{Y}}{S_{XX}} \\
       &= \frac{\sum_{i=1}^n x_iY_i - \sum_{i=1}^n\bar{x}Y_i}{S_{XX}}\\
       &= \sum_{i=1}^n \left(\frac{x_i - \bar{x}}{S_{XX}}\right)  Y_i

Each coefficient to the responses above only consists of non-random quantities involving the
explanatory variable. 

For the intercept estimator, we borrow the result of Eq. :eq:`slope-linear-comb`.

.. math::

   b_0 &= \bar{Y} -b_1\bar{x} \\
       &= \sum_{i=1}^n \frac{1}{n}Y_i - \bar{x}\sum_{i=1}^n \left(\frac{x_i - \bar{x}}{S_{XX}}\right)  Y_i\\
       &= \sum_{i=1}^n \left(\frac{1}{n} - \frac{\bar{x}}{S_{XX}}(x_i - \bar{x})\right) Y_i\\

Again, the coefficient of each term above consists of non-random quantities involving only the
sample size and the explanatory values.

**â€¼ï¸ Key Observation**

Recall that each :math:`Y_i` is **normally disributed** given an observed :math:`x_i`. Since both
:math:`b_1` and :math:`b_0`
are linear combinations of normal random variables, they must also be normally distributed. We now
proceed to state their expectations and variances. 

2. The Expectation and Variance of the Parameter Estimators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**The Slope Estimator**

.. math::
   &E[b_1] = \beta_1\\
   &\text{Var}(b_1) = \frac{\sigma^2}{S_{XX}}

**The Intercept Estimator**

.. math::
   &E[b_0] = \beta_0\\
   &\text{Var}(b_0) = \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)

**Key Insights**:

- Both :math:`b_1` and :math:`b_0` are **unbiased estimators** of :math:`\beta_1` and :math:`\beta_0`, respectively.
- The slope variance depends only on the error variance and the spread of the explanatory values.
- The intercept variance includes additional uncertainty when :math:`\bar{x} \neq 0`.

.. admonition:: Derive the Results as an Independent Exercise
   :class: important

   The expectations and variances of both estimators can be derived using general properties 
   and some algebraic manipulation. You are encouraged to verify these results as an independent exercise.


3. The Complete Distribution of the Estimators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Provided that all model assumptions hold,

.. math::

   b_1 \sim N\left(\beta_1, \frac{\sigma^2}{S_{XX}}\right)


.. math::

   b_0 \sim N\left(\beta_0, \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)\right)


4. Estimated Standard Errors and t-Distribution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Since :math:`\sigma^2` is unknown, we replace it with its estimate :math:`s^2 = MSE` to obtain the 
estimated standard errors:

.. math::

   \widehat{SE}(b_1) = \sqrt{\frac{MSE}{S_{XX}}}

.. math::

   \widehat{SE}(b_0) = \sqrt{MSE\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)}

Studentization of each estimator then gives us a :math:`t`-distributed random variable, both with :math:`df=n-2`:

.. math::

   \frac{b_1 - \beta_1}{\widehat{SE}(b_1)} \sim t_{n-2}

.. math::

   \frac{b_0 - \beta_0}{\widehat{SE}(b_0)} \sim t_{n-2}

We use these as the foundation for confidence regions and hypothesis tests on the true
values of the regression parameters, :math:`\beta_1` and :math:`\beta_0`.

Confidence Regions for Parameters
---------------------------------------

Recall the general form of a :math:`t`-confidence region:

.. math::
   \text{estimate} \pm t_{crit} \widehat{SE}(\text{estimate})

The table below applies the general form to all possible parameter-side combination:

.. flat-table::
   :header-rows: 1

   * - Side
     - :math:`\beta_0`
     - :math:`\beta_1`
   
   * - CI
     - .. math::
         b_0 \pm t_{\alpha/2, n-2}\sqrt{MSE\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)}
     - .. math::
         b_1 \pm t_{\alpha/2, n-2} \sqrt{\frac{MSE}{S_{XX}}}

   * - UCB
     - .. math::
         b_0 + t_{\alpha, n-2}\sqrt{MSE\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)}
     - .. math::
         b_1 + t_{\alpha, n-2} \sqrt{\frac{MSE}{S_{XX}}}

   * - LCB
     - .. math::
         b_0 - t_{\alpha, n-2}\sqrt{MSE\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}}\right)}
     - .. math::
         b_1 - t_{\alpha, n-2} \sqrt{\frac{MSE}{S_{XX}}}

Hypothesis Testing for the Slope Parameter
-------------------------------------------------

**Step 1: Parameter of Interest**

Define :math:`\beta_1` as the slope of the **true regression line** relating [explanatory variable] to [response variable].
Make sure to use experiment-specific variable names and context.

**Step 2: Hypotheses**

It is possible to construct a one-sided or two-sided hypothesis test for the slope parameter against any null value :math:`\beta_{10}`. 

.. math::
   &H_0: \beta_{1} (=, \leq, \geq) \beta_{10}\\
   &H_1: \beta_{1} (\neq, >, <) \beta_{10}

The most common hypothesis is the two-sided variant with the null value :math:`\beta_{10} = 0`. 

.. math::
   &H_0: \beta_1 = 0\\
   &H_a: \beta_1 \neq 0

**Step 3: Test Statistic and P-value**

.. math::

   T_{ts} = \frac{b_1 - \beta_{10}}{\widehat{SE}(b_1)}

has :math:`t`-distribution with :math:`df = n-2`.  Once an observed value :math:`t_{TS}` is obtained,
the :math:`p`-value computation should align with the sidedness of the hypothesis.

* Two-sided: :math:`p`-value :math:`= 2P(T_{n-2} > |t_{TS}|)`
* Upper-tailed: :math:`p`-value :math:`= P(T_{n-2} > t_{TS})` 
* Lower-tailed: :math:`p`-value :math:`= P(T_{n-2} < t_{TS})`

**Step 4: Decision and Conclusion**

Compare p-value to :math:`\alpha` and draw conclusions about the slope parameter **in context**.

.. admonition:: Hypothesis Tests for the Intercept
   :class: important

   Hypothesis tests on :math:`\beta_0` can be constructed in a similar manner. They are not explored
   in detail because:
   
   (1) The procedure only differs by the form of the standard error estimate, and
   (2) The intercept often does not have any practial significance.

..
   Implementation in R
   -------------------

   **Fitting the Model**:

   ```r
   # Fit linear model
   fit <- lm(response ~ explanatory, data = dataset_name)

   # Extract coefficients
   coefficients(fit)  # or fit`coefficients

   # Extract residuals and fitted values  
   residuals(fit)     # or fit`residuals
   fitted(fit)        # or fit`fitted.values
   ```

   **Getting Inference Results**:

   ```r
   # Complete summary with tests and R-squared
   summary(fit)

   # ANOVA table for F-test
   anova(fit)
   # or
   summary(aov(fit))
   ```

   **Manual Calculations** (when given summary statistics rather than raw data):

   ```r
   # Calculate test statistics manually
   F_stat <- MSR / MSE
   t_stat <- (b1 - beta1_null) / SE_b1

   # Calculate p-values
   p_value_F <- pf(F_stat, df1 = 1, df2 = n-2, lower.tail = FALSE)
   p_value_t <- 2 * pt(abs(t_stat), df = n-2, lower.tail = FALSE)

   # Calculate confidence intervals
   margin_error <- qt(alpha/2, df = n-2, lower.tail = FALSE) * SE_b1
   CI_lower <- b1 - margin_error
   CI_upper <- b1 + margin_error
   ```

.. admonition:: Example ðŸ’¡: Blood Pressure Study, Continued
   :class: note 

   For the blood pressure vs age data set, perform a hypothesis test to determine if the slope parameter is non-zero.
   Then, construct a corresponding confidence region.

   **Step 1: Define the parameter of interest**
   
   We are interested in :math:`\beta_1`, the true change in blood pressure per year increase in patient age.

   **Step 2: State the hypotheses**

   .. math::
      &H_0: \beta_1 = 0\\
      &H_a: \beta_1 \neq 0

   **Step 3: Computation**

   The test statistic is:

   .. math::
      t_{TS} = \frac{b_0-0}{\sqrt{\text{MSE}/S_{xx}}}\frac{-0.526 - 0}{0.146} = -3.61.

   The :math:`p`-value is:

   .. math::
      2P(T_9 > 3.61)  = 0.0055.
   
   **Step 4: Conclusion**

   :math:`p`-value :math:`=0.0055 < \alpha`. The null hypothesis is rejected.
   At :math:`\alpha=0.05`, there is enough evidence to conclude that the slope of the true linear 
   association between age and change in blood pressure is different than :math:`0`.

   **95% Confidence Interval**:

   The confidence region corresponding to a two-sided hypothesis test is a confidence interval.
   Use R to compute the critical value:

   .. code-block:: r

      qt(0.025, df=9, lower.tail=FALSE)
      #2.262

   Then putting the components together,

   .. math::
      -0.526 \pm (2.262)(0.146) = -0.526 \pm 0.330 = (-0.856, -0.196).

   We are 95% confident that each additional year of age is associated with a decrease in blood pressure between 
   0.196 and 0.856 mm Hg after treatment.


..
   Standard Error Formulas and Confidence Intervals
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-interval-slope.png
      :width: 80%
      
      Confidence interval formula for the slope parameter with all components clearly labeled

   Since :math:`\sigma^2` is unknown, we estimate it using :math:`s^2 = \text{MSE}` and use the t-distribution:

   **Standard Error of the Slope**:

   .. math::

      SE(b_1) = \sqrt{\frac{\text{MSE}}{S_{xx}}}

   where :math:`S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2`.

   **Confidence Interval for the Slope**:

   .. math::

      b_1 \pm t_{\alpha/2, n-2} \sqrt{\frac{\text{MSE}}{S_{xx}}}

   This provides a range of plausible values for the true slope :math:`\beta_1` with :math:`(1-\alpha) \times 100\%` confidence.

   **Interpretation**: We are :math:`(1-\alpha) \times 100\%` confident that the true change in the response variable for each one-unit increase in the explanatory variable lies within this interval.

   Complete Hypothesis Testing Framework
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-hypothesis-test-slope.png
      :width: 80%
      
      Complete framework for hypothesis testing about the slope parameter

   **Step 1: Parameter of Interest**

   We are interested in :math:`\beta_1`, the population slope of the mean response line :math:`\mu_{Y|X=x}`.

   **Step 2: Hypotheses**

   The general form allows for various null values :math:`\beta_{10}`:

   .. math::

      H_0: \beta_1 = \beta_{10} \quad \text{vs} \quad H_a: \beta_1 \neq \beta_{10}

   Alternative formulations:

   - One-sided upper: :math:`H_a: \beta_1 > \beta_{10}`
   - One-sided lower: :math:`H_a: \beta_1 < \beta_{10}`

   **Step 3: Test Statistic and P-value**

   .. math:

      t = \frac{b_1 - \beta_{10}}{SE(b_1)} = \frac{b_1 - \beta_{10}}{\sqrt{\text{MSE}/S_{xx}}}

   The test statistic follows a t-distribution with :math:`n-2` degrees of freedom.

   **P-value calculation depends on the alternative**:

   - Two-sided: :math:`\text{p-value} = 2P(t_{n-2} > |t|)`
   - Upper tail: :math:`\text{p-value} = P(t_{n-2} > t)`  
   - Lower tail: :math:`\text{p-value} = P(t_{n-2} < t)`

   **Step 4: Decision and Conclusion**

   Compare p-value to :math:`\alpha` and state conclusions in context of the problem.

Special Case: Equivalence of F-test and t-test
----------------------------------------------------------------------------------

For simple linear regresssion, the hypothesis test on :math:`H_a: \beta_1 \neq 0` is equivalent 
to the :math:`F`-test for model utility. Intuitively, this makes sense because, in order for the model
to meaningfully describe the response variable, the explanatory variable must
have a significant association with it. 

Mathematically, this equivalence is explained through the special equality: 

.. math::

   T_{ts}^2 = F_{TS}

To see why this is true, recall that :math:`b_1 = \frac{S_{XY}}{S_{XX}}` 
and :math:`\text{MSR} = b_1\text{S}_{XY}`. Using these, 

.. math:: 

   T_{ts}^2 = \frac{b_1^2}{\text{MSE}/S_{XX}} = \frac{b_1 \frac{S_{XY}}{S_{XX}} S_{XX}}{MSE} = \frac{MSR}{MSE}

This implies that both tests provide identical p-values and conclusions.

**The Equivalence Only Holds for Simple LR**

The two tests serve different purposes in general linear regression with multiple predictors:

- The :math:`F`-test assesses whether at least one of the predictors provide useful information about the response variable.
- The :math:`t`-test on a slope parameter assesses whether the single corresponding predictor is useful.

.. admonition:: Example ðŸ’¡: Blood Pressure Study, Continued
   :class: note 
      
   For the blood pressure dataset, verify the equivalence of the model utility :math:`F`-test and the 
   two-tailed :math:`t`-test on the slope parameter against :math:`\beta_{10}=0`.

   From the two previous examples, 

   - :math:`t_{TS}^2 = (-3.61)^2 = 13.03`
   - :math:`f_{TS}=13.06`
   - There is a small difference due to rounding, but both tests give :math:`p`-value :math:`= 0.0055`. âœ”


..
   Implementation in R: Complete Workflow
   -----------------------------------------

   .. figure:: https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-r-implementation.png
      :width: 80%
      
      Complete R workflow for fitting models and conducting inference

   **Model Fitting and Basic Output**:

   .. code-block:: r

      # Fit the linear model
      fit <- lm(response_variable ~ explanatory_variable, data = dataFrame)
      
      # Extract key components
      coefficients(fit)     # Get b0 and b1
      residuals(fit)        # Get residuals for diagnostics
      fitted.values(fit)    # Get predicted values
      
      # Complete inference summary
      summary(fit)          # Includes RÂ², F-test, t-tests, standard errors
      
      # ANOVA table
      anova(fit)           # Or summary(aov(fit))

   **What summary(fit) Provides**:

   - Coefficient estimates (:math:`b_0`, :math:`b_1`) with standard errors
   - t-statistics and p-values for testing each coefficient equals zero
   - R-squared and adjusted R-squared
   - F-statistic and p-value for overall model utility
   - Residual standard error (estimate of :math:`\sigma`)

   **Manual Calculations** (useful for understanding or when given summary statistics):

   .. code-block:: r

      # Calculate standard error of slope manually
      SE_b1 <- sqrt(MSE / Sxx)
      
      # Calculate t-statistic
      t_stat <- (b1 - beta1_null) / SE_b1
      
      # Calculate confidence interval
      t_critical <- qt(alpha/2, df = n-2, lower.tail = FALSE)
      CI_lower <- b1 - t_critical * SE_b1
      CI_upper <- b1 + t_critical * SE_b1
      
      # Calculate p-values
      p_value_two_sided <- 2 * pt(abs(t_stat), df = n-2, lower.tail = FALSE)
      p_value_F <- pf(F_stat, df1 = 1, df2 = n-2, lower.tail = FALSE)


..
   This part is more appropriate for lession 4. And an example going through the whole workflow
   already exists in the following section.
   Summary of Linear Regression Workflow
   -----------------------------------------------

   **1. Exploratory Analysis**

   - Determine which variable should be explanatory vs. response
   - Create scatter plot to assess initial relationship 

   **2. Model Fitting**

   - Fit least squares regression line and calculate basic summary statistics such as
   the slope and parameter estimates, MSE, correlation coefficient, and R-squared.

   **3. Diagnostic Checking** (Critical before inference!)

   - Create residual plots to check linearity and constant variance.
   - Examine histograms and QQ plots of residuals for normality.
   - Identify any influential points or assumption violations.

   **4. Inference** (only if assumptions are reasonable)

   - Perform :math:`F`-test for overall model utility and inference for individual parameters.
   - Interpret results in context of the original problem.

   **5. Predictions** (if the model utility is significant)

   - Make predictions with appropriate uncertainty quantification. Covered in the upcoming lesson.

Bring It All Tgether
---------------------

.. admonition:: Key Takeaways ðŸ“
   :class: important

   1. **Diagnostic plots** are essential for verifying model assumptions 
      before conducting hypothesis tests or constructing confidence intervals.

   2. **Four key diagnostic tools work together**: scatter plots, residual plots, histograms of residuals, and QQ plots 
      provide complementary information about different aspects of model adequacy.

   3. **The model utility F-test** assesses whether the linear relationship explains a significant portion of 
      the variability in the response variable.

   4. Inference on the slope and intercept **adapts familiar principles** used in previous inference procedures
      on single parameters. 

   6. In simple linear regression, the model utility **F-test** is equivalent
      to the :math:`t`-test on :math:`H_a: \beta_1 \neq 0`.

Exercises
~~~~~~~~~~~~~~~~~

1. **Diagnostic Interpretation**: For each residual plot pattern described, identify the assumption violation and 
   potential consequences:

   a) Residuals form a cone shape, spreading out as X increases.
   b) Residuals show a clear curved (U-shaped) pattern.
   c) Residuals appear randomly scattered around zero with constant spread.
   d) Most residuals are near zero with a few extremely large positive and negative values.
   e) Residuals show alternating positive and negative values in sequence.

2. **ANOVA Table Completion**: Given the following partial ANOVA table for a regression with n = 15, complete all 
   missing values:

   .. flat-table::
      :header-rows: 1
      
      * - Source
        - df
        - Sum of Squares
        - Mean Square
        - F-statistic
      * - Regression
        - ?
        - 240
        - ?
        - ?
      * - Error
        - ?
        - ?
        - 18
        - 
      * - Total
        - ?
        - 474
        - 
        -

3. **Parameter Inference**: A study of house prices yields the regression equation 

   .. math::
      \text{Price} = 45,000 + 120(\text{Size}), 
      
   where Price is in dollars and Size is in square feet. With 
   
   * :math:`n = 20`
   * :math:`MSE = 50,000,000`, and 
   * :math:`S_{XX} = 2500`,

   a) Calculate the standard error of the slope.
   b) Construct a 95% confidence interval for the slope.
   c) Test :math:`H_0: Î²_1 = 100` vs :math:`H_a: Î²_1 \neq 100` at :math:`\alpha = 0.05`.
   d) Interpret the slope coefficient and its interval in context.

4. **F-test vs t-test**: Using the house price data from Exercise 3:

   a) Conduct the :math:`F`-test for model utility.
   b) Conduct the :math:`t`-test for :math:`H_0: Î²_1 = 0` vs :math:`H_a: Î²_1 \neq 0`.
   c) Verify that :math:`f_{TS} = t^2_{TS}` and explain why this relationship holds.
   d) Discuss when you might prefer one test over the other.

5. **Critical Evaluation**: A researcher reports: "The regression has :math:`R^2 = 0.95`, so the model is excellent and all 
   assumptions are satisfied."

   a) What's wrong with this reasoning?
   b) What additional information would you need to evaluate the model?
   c) Describe how a high :math:`R^2` could coexist with serious assumption violations.
   d) What would you recommend the researcher do?

6. **Design Considerations**: Explain how each factor affects the precision of slope estimation:

   a) Increasing the sample size :math:`n`
   b) Increasing the range of :math:`X` values observed
   c) Reducing the error variance :math:`Ïƒ^2s`

7. **Confidence Interval Interpretation**: For each confidence interval interpretation, identify whether it's correct 
   or incorrect and explain:

   a) "There's a 95% chance that the true slope lies in this interval."
   b) "95% of sample slopes will fall in this interval."
   c) "If we repeated this study many times, 95% of the intervals would contain the true slope."
   d) "We're 95% confident about the slope value for this specific dataset."

8. **Hypothesis Testing Scenarios**: For each research scenario, formulate appropriate hypotheses:

   a) Is there any linear relationship between study hours and test scores?
   b) Is the slope of salary vs. experience at least $2000 per year?
   c) Is the relationship between temperature and ice cream sales negative?
   d) Is the effect of fertilizer on plant growth different than 5 cm per gram?

9. **Comprehensive Case Study**: A medical researcher studies the relationship between patient age (:math:`X`) and recovery 
   time in days (:math:`Y`) for a surgical procedure. With :math:`n = 25` patients, the analysis yields:
    
   - Fitted model: :math:`Å¶ = 8.5 + 0.3X`
   - :math:`\text{SSR} = 156`, :math:`\text{SSE} = 234`, :math:`\text{SST} = 390`
   - :math:`S_{XX} = 1200`
    
   a) Discuss what diagnostic plots you would need to see.
   b) Provide the complete ANOVA table and :math:`R^2`.
   c) Perform the :math:`F`-test for model utility.
   d) Compute the 95% confidence interval for the slope.
   e) Test whether the slope exceeds :math:`0.25` days per year.
   f) Provide a practical interpretation of all results.

