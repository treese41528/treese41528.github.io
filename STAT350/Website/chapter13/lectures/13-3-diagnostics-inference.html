

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>13.3. Model Diagnostics and Statistical Inference &mdash; STAT 350</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3c686048" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT350/course_site/chapter13/lectures/13-3-diagnostics-inference.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=8512422d"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="13.4. Prediction, Robustness, and Applied Examples" href="13-4-prediction-robustness.html" />
    <link rel="prev" title="13.2. Simple Linear Regression" href="13-2-simple-linear-regression.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Orientation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-intro.html">Course Introduction &amp; Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#why-statistics-why-now">Why Statistics? Why Now?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#course-roadmap">Course Roadmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#a-clear-note-on-using-ai">A Clear Note on Using AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#getting-started-a-checklist">Getting Started: A Checklist</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#homework-assignments-on-edfinity">Homework Assignments on Edfinity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#miscellaneous-tips">Miscellaneous Tips</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Exam Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../exams/exams_index.html">Course Examinations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../exams/exams_index.html#general-exam-policies">General Exam Policies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../exams/exam_materials/exam1.html">Exam 1</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam1.html#exam-locations-by-section">Exam Locations by Section</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam1.html#exam-coverage">Exam Coverage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam1.html#preparation-materials">Preparation Materials</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../exams/exam_materials/exam2.html">Exam 2</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#exam-locations-by-section">Exam Locations by Section</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#exam-coverage">Exam Coverage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#additional-resources">Additional Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#preparation-materials">Preparation Materials</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../exams/exam_materials/final_exam.html">Final Exam</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#about-the-final-exam">About the Final Exam</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#required-review-materials">Required Review Materials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#post-exam-2-preparation-materials">Post Exam 2 Preparation Materials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#study-guide-resource">Study Guide Resource</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../chapter1/index.html">1. Introduction to Statistics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-1-intro.html">1.1. What Is Statistics?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-2-probability-inference.html">1.2. Probability &amp; Statistical Inference: How Are They Associated?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter2/index.html">2. Graphical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-1-understanding-the-structure-of-data-set.html">2.1. Data Set Structure and Variable Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-2-tools-for-categorical-qualitative-data.html">2.2. Tools for Categorical (Qualitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-3-tools-for-numerical-quantitative-data.html">2.3. Tools for Numerical (Quantitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-4-exploring-quantitative-distributions-modality-shape-and-outliers.html">2.4. Exploring Quantitative Distributions: Modality, Shape &amp; Outliers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter3/index.html">3. Numerical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-1-intro-numerical-summaries.html">3.1. Introduction to Numerical Summaries: Notation and Terminology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-2-measures-of-central-tendency.html">3.2. Measures of Central Tendency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-3-measures-of-variability-range-variance-and-SD.html">3.3. Measures of Variability - Range, Variance, and Standard Deviation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-4-measures-of-variability-IQR-and-5-number-summary.html">3.4. Measures of Variability - Interquartile Range and Five-Number Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-5-choosing-the-right-measure-resistance-to-extreme-values.html">3.5. Choosing the Right Measure &amp; Comparing Measures Across Data Sets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter4/index.html">4. Probability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-1-basic-set-theory.html">4.1. Basic Set Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-2-probability.html">4.2. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-3-conditional-probability.html">4.3. Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-4-law-of-total-probability-and-bayes-rule.html">4.4. Law of Total Probability and Bayes’ Rule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-5-bayes-update-rule-example.html">4.5. Bayesian Updating: Sequential Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-6-independence-of-events.html">4.6. Independence of Events</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter5/index.html">5. Discrete Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-1-discrete-rvs-and-pmfs.html">5.1. Discrete Random Variables and Probability Mass Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-2-joint-pmfs.html">5.2. Joint Probability Mass Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-3-expected-value-of-discrete-rv.html">5.3. Expected Value of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-4-variance-of-discrete-rv.html">5.4. Varianace of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-5-covariance-of-dependent-rvs.html">5.5. Covariance of Dependent Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-6-binomial-distribution.html">5.6. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-7-poisson-distribution.html">5.7. Poisson Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter6/index.html">6. Continuous Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-1-continuous-rvs-and-pdfs.html">6.1. Continuous Random Variables and Probability Density Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-2-expected-value-and-variance-of-cts-rvs.html">6.2. Expected Value and Variance of Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-3-cdfs.html">6.3. Cumulative Distribution Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-4-normal-distribution.html">6.4. Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-5-uniform-distribution.html">6.5. Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-6-exponential-distribution.html">6.6. Exponential Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter7/index.html">7. Sampling Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-1-statistics-and-sampling-distributions.html">7.1. Statistics and Sampling Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-2-sampling-distribution-for-the-sample-mean.html">7.2. Sampling Distribution for the Sample Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-3-clt.html">7.3. The Central Limit Theorem (CLT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-4-discret-rvs-and-clt.html">7.4. Discrete Random Variables and the CLT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter8/index.html">8. Experimental Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-1-experimental-and-sampling-designs.html">8.1. Experimental and Sampling Designs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-2-experimental-design-principles.html">8.2. Experimental Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-3-basic-types-of-experimental-design.html">8.3. Basic Types of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-4-addressing-potential-flaws-in-experimental-design.html">8.4. Addressing Potential Flaws in Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-5-examples-of-experimental-design.html">8.5. Examples of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-6-sampling-design.html">8.6. Sampling Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-7-sampling-bias.html">8.7. Sampling Bias</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter9/index.html">9. Confidence Intervals and Bounds</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-1-intro-statistical-inference.html">9.1. Introduction to Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-2-ci-sigma-known.html">9.2. Confidence Intervals for the Population Mean, When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-3-precision-of-ci-and-sample-size-calculation.html">9.3. Precision of a Confidence Interval and Sample Size Calculation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-4-cb-sigma-known.html">9.4. Confidence Bounds for the Poulation Mean When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-5-ci-cb-sigma-unknown.html">9.5. Confidence Intervals and Bounds When σ is Unknown</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter10/index.html">10. Hypothesis Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-1-ht-errors-and-power.html">10.1. Type I Error, Type II Error, and Power</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-2-ht-for-mean-sigma-known.html">10.2. Hypothesis Test for the Population Mean When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-3-ht-for-mean-sigma-unknown.html">10.3. Hypothesis Test for the Population Mean When σ Is Unknown</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-4-pvalue-significance-conclusion.html">10.4. P-values, Statistical Significance, and Formal Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter11/index.html">11. Two Sample Procedures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-1-ci-ht-two-samples.html">11.1. Confidence Interval/Bound and Hypothesis Test for Two Samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-2-comparing-two-means-independent-sigmas-known.html">11.2. Comparing the Means of Two Independent Populations - Population Variances Are Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-3-comparing-two-means-independent-pooled.html">11.3. Comparing the Means of Two Independent Populations - Pooled Variance Estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-4-comparing-two-means-independent-unpooled.html">11.4. Comparing the Means of Two Independent Populations - No Equal Variance Assumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-5-mean-of-paired-differences-two-dependent-populations.html">11.5. Analyzing the Mean of Paired Differences Between two Dependent Populations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter12/index.html">12. ANOVA</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-1-intro-one-way-anova.html">12.1. Introduction to One-Way ANOVA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-2-sources-of-variability.html">12.2. Different Sources of Variability in an ANOVA Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-3-f-test-and-relationship-to-t-test.html">12.3. One-Way ANOVA F-Test and Its Relationship to Two-Sample t-Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-4-multiple-comparison-procedures-family-wise-error-rates.html">12.4. Multiple Comparison Procedures and Family-Wise Error Rates</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">13. Simple Linear Regression</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="13-1-intro-to-lr-correlation-scatter-plots.html">13.1. Introduction to Linear Regression: Correlation and Scatter Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="13-2-simple-linear-regression.html">13.2. Simple Linear Regression</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">13.3. Model Diagnostics and Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="13-4-prediction-robustness.html">13.4. Prediction, Robustness, and Applied Examples</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">13. </span>Simple Linear Regression</a></li>
      <li class="breadcrumb-item active"><span class="section-number">13.3. </span>Model Diagnostics and Statistical Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/chapter13/lectures/13-3-diagnostics-inference.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="model-diagnostics-and-statistical-inference">
<span id="diagnostics-inference"></span><h1><span class="section-number">13.3. </span>Model Diagnostics and Statistical Inference<a class="headerlink" href="#model-diagnostics-and-statistical-inference" title="Link to this heading"></a></h1>
<p>Having developed the simple linear regression model and methods for fitting it using least squares,
we now face a critical question: how do we know if our model is appropriate for the data? Before
conducting any statistical inference—hypothesis tests, confidence intervals, or predictions—we must
first verify that our model assumptions are reasonable. Violating these assumptions can lead to
invalid conclusions and unreliable inference procedures.</p>
<p>This chapter combines three essential components of regression analysis: diagnostic procedures for
checking model assumptions, the F-test for overall model utility, and inference procedures for
individual model parameters. Together, these tools provide a complete framework for validating
and drawing conclusions from simple linear regression models.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Problem we will solve</strong> – How to verify that regression model assumptions are satisfied, test whether our model provides useful information about the relationship between variables, and conduct formal inference about model parameters with appropriate uncertainty quantification</p></li>
<li><p><strong>Tools we’ll learn</strong> – Residual plots and diagnostic graphics for assumption checking, F-test for overall model utility, t-tests and confidence intervals for slope and intercept parameters, and the mathematical relationship between different inference approaches</p></li>
<li><p><strong>How it fits</strong> – This completes our regression toolkit by ensuring model validity before inference, testing overall model usefulness, and providing methods for parameter-specific conclusions—preparing us for prediction and more advanced regression techniques</p></li>
</ul>
</div>
<section id="the-critical-importance-of-assumption-checking">
<h2><span class="section-number">13.3.1. </span>The Critical Importance of Assumption Checking<a class="headerlink" href="#the-critical-importance-of-assumption-checking" title="Link to this heading"></a></h2>
<div class="video-placeholder" role="group" aria-labelledby="video-ch13-6">
   <iframe
      id="video-ch13-6"
      title="STAT 350 – Chapter 13.6 Diagnostics for Model Assumptions Video"
      src="https://www.youtube.com/embed/p8kRL-vUpVo?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
      allowfullscreen>
   </iframe>
</div><p>Before conducting any statistical inference procedures, we must verify that our model assumptions are reasonable. If we have strong violations of these assumptions, our statistical inference procedures will not be accurate—we won’t be able to trust the results and won’t be able to convey the information we want about the relationship between our variables.</p>
<section id="review-of-simple-linear-regression-assumptions">
<h3>Review of Simple Linear Regression Assumptions<a class="headerlink" href="#review-of-simple-linear-regression-assumptions" title="Link to this heading"></a></h3>
<figure class="align-default" id="id41">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-assumptions.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-assumptions.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-assumptions.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.31 </span><span class="caption-text">The four fundamental assumptions of simple linear regression that must be verified before conducting inference</span><a class="headerlink" href="#id41" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Our simple linear regression model <span class="math notranslate nohighlight">\(Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i\)</span> requires four key assumptions:</p>
<p><strong>Assumption 1: Independence and Identical Distribution (IID)</strong></p>
<p>The observed pairs <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> for <span class="math notranslate nohighlight">\(i \in \{1, 2, \ldots, n\}\)</span> are such that <span class="math notranslate nohighlight">\(y_i\)</span> represents a simple random sample for each fixed <span class="math notranslate nohighlight">\(x_i\)</span>. This means:</p>
<ul class="simple">
<li><p>We plan in advance which explanatory variable values <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_n\)</span> to collect</p></li>
<li><p>We then measure the response as output for each fixed <span class="math notranslate nohighlight">\(x_i\)</span></p></li>
<li><p>Each response <span class="math notranslate nohighlight">\(y_i\)</span> is independent of the others</p></li>
<li><p>The responses constitute a simple random sample for their respective <span class="math notranslate nohighlight">\(x\)</span> values</p></li>
</ul>
<p>This assumption is primarily ensured through proper experimental design and data collection procedures. It’s difficult to verify statistically after data collection, so we must rely on understanding how the data was gathered.</p>
<p><strong>Assumption 2: Linearity</strong></p>
<p>The association between the explanatory variable and the response is, on average, linear. The mean response follows the straight line <span class="math notranslate nohighlight">\(E[Y|X] = \beta_0 + \beta_1 X\)</span>. If this assumption is violated, using a linear model to describe a non-linear relationship will lead to poor fits and misleading conclusions.</p>
<p><strong>Assumption 3: Normality</strong></p>
<p>The error terms (and hence the response values) are normally distributed:</p>
<div class="math notranslate nohighlight">
\[\varepsilon_i \stackrel{iid}{\sim} N(0, \sigma^2) \quad \text{for } i = 1, 2, \ldots, n\]</div>
<p>This leads to the conditional distribution:</p>
<div class="math notranslate nohighlight">
\[Y_i | X_i = x_i \stackrel{iid}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)\]</div>
<p><strong>Assumption 4: Homoscedasticity (Equal Variance)</strong></p>
<p>The error terms have constant variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> across all values of <span class="math notranslate nohighlight">\(X\)</span>. The spread of <span class="math notranslate nohighlight">\(Y\)</span> values around the regression line remains the same regardless of the <span class="math notranslate nohighlight">\(X\)</span> value. Violations of this assumption are called <strong>heteroscedasticity</strong>.</p>
</section>
</section>
<section id="diagnostic-tools-scatter-plots-and-residual-plots">
<h2><span class="section-number">13.3.2. </span>Diagnostic Tools: Scatter Plots and Residual Plots<a class="headerlink" href="#diagnostic-tools-scatter-plots-and-residual-plots" title="Link to this heading"></a></h2>
<figure class="align-default" id="id42">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-scatterplot-assumptions.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-scatterplot-assumptions.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-scatterplot-assumptions.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.32 </span><span class="caption-text">Using scatter plots to check linearity and constant variance assumptions</span><a class="headerlink" href="#id42" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Scatter Plots for Initial Assessment</strong></p>
<p>Scatter plots serve as our primary tool for initial assumption checking:</p>
<ul class="simple">
<li><p><strong>Linearity</strong>: Points should roughly follow a straight line pattern</p></li>
<li><p><strong>Constant variance</strong>: The spread of points around the apparent trend should remain consistent across the range of <span class="math notranslate nohighlight">\(X\)</span> values</p></li>
<li><p><strong>Outliers</strong>: Identify observations that don’t fit the general pattern</p></li>
</ul>
<p>However, scatter plots cannot help us assess the normality assumption—we need additional tools for that.</p>
<p><strong>The Power of Residual Plots</strong></p>
<figure class="align-default" id="id43">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-residual-plot-construction.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-residual-plot-construction.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-residual-plot-construction.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.33 </span><span class="caption-text">Construction of residual plots from scatter plots, showing how residuals transform the regression analysis</span><a class="headerlink" href="#id43" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Residual plots provide a more sensitive diagnostic tool than scatter plots alone. A <strong>residual plot</strong> displays the residuals <span class="math notranslate nohighlight">\(e_i = y_i - \hat{y}_i\)</span> on the vertical axis against the explanatory variable <span class="math notranslate nohighlight">\(x_i\)</span> on the horizontal axis.</p>
<p><strong>Construction Process</strong>:</p>
<ol class="arabic simple">
<li><p>Fit the regression line to obtain predicted values <span class="math notranslate nohighlight">\(\hat{y}_i\)</span></p></li>
<li><p>Calculate residuals: <span class="math notranslate nohighlight">\(e_i = y_i - \hat{y}_i\)</span> for each observation</p></li>
<li><p>Plot residuals (vertical axis) against <span class="math notranslate nohighlight">\(x_i\)</span> values (horizontal axis)</p></li>
</ol>
<p><strong>What Residual Plots Reveal</strong>:</p>
<p>The residual plot essentially rotates the regression line to be horizontal around the x-axis and amplifies deviations from the fitted relationship. This makes patterns easier to detect than in the original scatter plot.</p>
<section id="applied-example-blood-pressure-study-revisited">
<h3>Applied Example: Blood Pressure Study Revisited<a class="headerlink" href="#applied-example-blood-pressure-study-revisited" title="Link to this heading"></a></h3>
<figure class="align-default" id="id44">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-diagnostics.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-diagnostics.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-diagnostics.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.34 </span><span class="caption-text">Diagnostic analysis of the blood pressure treatment study showing scatter plot with fitted line</span><a class="headerlink" href="#id44" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Let’s apply our diagnostic procedures to the blood pressure treatment study, where we examined the relationship between patient age and change in blood pressure after 24 hours of treatment.</p>
<p><strong>Scatter Plot Assessment</strong>:</p>
<p>Looking at the scatter plot with the fitted line <span class="math notranslate nohighlight">\(\hat{y} = 20.12 - 0.5263x\)</span>, we can trace our finger across the plot to assess the spread of points around the line. The linear relationship appears reasonable, though with only 11 observations, it’s challenging to definitively assess the constant variance assumption.</p>
<p><strong>Residual Plot Analysis</strong>:</p>
<figure class="align-default" id="id45">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-residuals.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-residuals.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-residuals.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.35 </span><span class="caption-text">Residual plot for the blood pressure data showing individual residuals labeled by vehicle type</span><a class="headerlink" href="#id45" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The residual plot for the car efficiency data shows each vehicle’s residual clearly labeled. When we examine the spread across different cylinder volume ranges:</p>
<ul class="simple">
<li><p>Left region (around 1.5L): Limited observations make assessment difficult</p></li>
<li><p>Middle region (around 1.8-2.0L): Several observations with varied residuals</p></li>
<li><p>Right region (around 2.5L): Adequate spread above and below zero</p></li>
</ul>
<p>The residual plot suggests potential minor violations of the constant variance assumption, but nothing strong enough to invalidate our analysis given the small sample size.</p>
</section>
</section>
<section id="recognizing-assumption-violations">
<h2><span class="section-number">13.3.3. </span>Recognizing Assumption Violations<a class="headerlink" href="#recognizing-assumption-violations" title="Link to this heading"></a></h2>
<p>Understanding what various patterns in residual plots indicate is crucial for proper model assessment.</p>
<p><strong>Constant Variance Violations (Heteroscedasticity)</strong></p>
<figure class="align-default" id="id46">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-heteroscedasticity-patterns.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-heteroscedasticity-patterns.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-heteroscedasticity-patterns.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.36 </span><span class="caption-text">Common patterns indicating violations of the constant variance assumption</span><a class="headerlink" href="#id46" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Cone Pattern</strong>: As <span class="math notranslate nohighlight">\(X\)</span> increases, the residual errors become larger. This indicates that <span class="math notranslate nohighlight">\(\text{Var}(\varepsilon_i)\)</span> is an increasing function of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p><strong>Hourglass Pattern</strong>: For extreme values of <span class="math notranslate nohighlight">\(X\)</span> (both large and small), the spread is larger than in the middle range. Variance depends on <span class="math notranslate nohighlight">\(X\)</span> in a non-constant way.</p>
<p><strong>Reverse Cone Pattern</strong>: As <span class="math notranslate nohighlight">\(X\)</span> increases, the residual errors become smaller. Again, variance is a function of <span class="math notranslate nohighlight">\(X\)</span> rather than constant.</p>
<p>These patterns indicate strong violations of the equal variance assumption, requiring more advanced techniques like weighted regression (beyond this course’s scope).</p>
<p><strong>Linearity Violations</strong></p>
<figure class="align-default" id="id47">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-linearity-violations.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-linearity-violations.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-linearity-violations.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.37 </span><span class="caption-text">Residual plot patterns indicating violations of the linearity assumption</span><a class="headerlink" href="#id47" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>What We Want to See</strong>: Random scatter of points around the horizontal line at zero, with no discernible pattern. This indicates both linearity and constant variance assumptions are satisfied.</p>
<p><strong>Curved Patterns</strong>: If residuals show systematic curved patterns, this suggests the true population relationship is non-linear. For example, if the true model should be <span class="math notranslate nohighlight">\(Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \varepsilon\)</span> but we fit only <span class="math notranslate nohighlight">\(Y = \beta_0 + \beta_1 X + \varepsilon\)</span>, the quadratic component gets absorbed into the residuals, creating a curved pattern.</p>
<p><strong>Key Insight</strong>: Patterns in residual plots indicate that our linear model is missing important systematic relationships that exist in the data.</p>
<p><strong>Assessing Normality of Residuals</strong></p>
<figure class="align-default" id="id48">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-normality-assessment.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-normality-assessment.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-normality-assessment.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.38 </span><span class="caption-text">Methods for assessing normality of residuals using histograms and QQ plots</span><a class="headerlink" href="#id48" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>For normality assessment, we use the same tools we’ve employed throughout the course, but applied to the residuals <span class="math notranslate nohighlight">\(e_1, e_2, \ldots, e_n\)</span>:</p>
<p><strong>Histogram of Residuals</strong>: Should approximate a normal distribution shape, centered around zero with the characteristic bell curve.</p>
<p><strong>QQ Plot of Residuals</strong>: Should show points following approximately a straight diagonal line from lower left to upper right. Systematic deviations from this line suggest departures from normality.</p>
<p>The residuals should behave like a random sample from <span class="math notranslate nohighlight">\(N(0, \sigma^2)\)</span>, so standard normality assessment techniques apply directly.</p>
<p><strong>Comprehensive Diagnostic Examples</strong></p>
<figure class="align-default" id="id49">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-diagnostic-examples.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-diagnostic-examples.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-diagnostic-examples.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.39 </span><span class="caption-text">Multiple examples showing different combinations of assumption violations and their appearance in diagnostic plots</span><a class="headerlink" href="#id49" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Example 1: Good Model Fit</strong></p>
<ul class="simple">
<li><p>Scatter plot shows clear linear trend with consistent spread</p></li>
<li><p>Residual plot shows random scatter with no patterns</p></li>
<li><p>Histogram of residuals approximates normal distribution</p></li>
<li><p>QQ plot shows points close to diagonal line</p></li>
</ul>
<p><strong>Example 2: Non-linearity Problem</strong></p>
<ul class="simple">
<li><p>Scatter plot shows slight curvature</p></li>
<li><p>Residual plot reveals systematic curved pattern</p></li>
<li><p>Normality plots may look reasonable since the issue is functional form, not error distribution</p></li>
</ul>
<p>The lesson: visual inspection of multiple diagnostic plots provides complementary information about different aspects of model adequacy.</p>
</section>
<section id="the-f-test-for-model-utility">
<h2><span class="section-number">13.3.4. </span>The F-Test for Model Utility<a class="headerlink" href="#the-f-test-for-model-utility" title="Link to this heading"></a></h2>
<div class="video-placeholder" role="group" aria-labelledby="video-ch13-7">
   <iframe
      id="video-ch13-7"
      title="STAT 350 – Chapter 13.7 Simple Linear Regression Model Inference - F-test Video"
      src="https://www.youtube.com/embed/mTQ3GU9rpys?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
      allowfullscreen>
   </iframe>
</div><p>Once we’ve verified that our model assumptions are reasonably satisfied, we can proceed with statistical inference. The first question we typically ask is: “Does our simple linear regression model provide useful information about the relationship between our explanatory and response variables?”</p>
<section id="understanding-the-anova-decomposition">
<h3>Understanding the ANOVA Decomposition<a class="headerlink" href="#understanding-the-anova-decomposition" title="Link to this heading"></a></h3>
<figure class="align-default" id="id50">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-anova-decomposition.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-anova-decomposition.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-anova-decomposition.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.40 </span><span class="caption-text">Complete ANOVA table for simple linear regression showing all components and formulas</span><a class="headerlink" href="#id50" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The F-test for model utility builds on the ANOVA decomposition we developed in Chapter 13.2, but now we understand it in the context of hypothesis testing about model usefulness.</p>
<p><strong>The Fundamental Identity</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SST} = \text{SSR} + \text{SSE}\]</div>
<p>This decomposes the total variability in our response variable into two meaningful components:</p>
<figure class="align-default" id="id51">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-sst-explanation.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-sst-explanation.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-sst-explanation.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.41 </span><span class="caption-text">Visual explanation of Sum of Squares Total as baseline variability ignoring the explanatory variable</span><a class="headerlink" href="#id51" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Sum of Squares Total (SST)</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SST} = \sum_{i=1}^n (y_i - \bar{y})^2\]</div>
<p>SST measures how much the response values deviate from their overall mean, completely ignoring any information from the explanatory variable. If there were no explanatory variable, <span class="math notranslate nohighlight">\(\bar{y}\)</span> would be our best estimate for modeling the response.</p>
<figure class="align-default" id="id52">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-ssr-explanation.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-ssr-explanation.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-ssr-explanation.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.42 </span><span class="caption-text">Visual explanation of Sum of Squares Regression as improvement over the baseline mean model</span><a class="headerlink" href="#id52" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Sum of Squares Regression (SSR)</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SSR} = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2\]</div>
<p>SSR measures how much the fitted values deviate from the overall mean response. This quantifies the improvement we get by using the linear relationship instead of simply averaging all response values.</p>
<p><strong>Model Utility Interpretation</strong>: If our model is useful, we want <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> values to be different from <span class="math notranslate nohighlight">\(\bar{y}\)</span>. If <span class="math notranslate nohighlight">\(\hat{y}_i \approx \bar{y}\)</span> for all observations, our explanatory variable provides no additional information beyond the overall mean.</p>
<p><strong>Connection to the Slope</strong>: Recall that <span class="math notranslate nohighlight">\(\text{SSR} = b_1^2 \sum_{i=1}^n (x_i - \bar{x})^2\)</span>. If the slope <span class="math notranslate nohighlight">\(b_1 \to 0\)</span>, then <span class="math notranslate nohighlight">\(\text{SSR} \to 0\)</span>, indicating no linear relationship.</p>
<figure class="align-default" id="id53">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-sse-explanation.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-sse-explanation.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-sse-explanation.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.43 </span><span class="caption-text">Visual explanation of Sum of Squares Error as unexplained variation after fitting the model</span><a class="headerlink" href="#id53" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Sum of Squares Error (SSE)</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SSE} = \sum_{i=1}^n (y_i - \hat{y}_i)^2\]</div>
<p>SSE measures how much the observed values deviate from the fitted line—the unexplained variability. This is exactly what we minimized when fitting the least squares regression line.</p>
<p><strong>What We Want</strong>: We want SSE to be small relative to SST, meaning our model explains most of the variation. If SSE ≈ SST, our model provides little improvement over simply using <span class="math notranslate nohighlight">\(\bar{y}\)</span>.</p>
</section>
<section id="understanding-degrees-of-freedom-multiple-perspectives">
<h3>Understanding Degrees of Freedom: Multiple Perspectives<a class="headerlink" href="#understanding-degrees-of-freedom-multiple-perspectives" title="Link to this heading"></a></h3>
<figure class="align-default" id="id54">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-degrees-freedom-explanation.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-degrees-freedom-explanation.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-degrees-freedom-explanation.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.44 </span><span class="caption-text">Explanation of degrees of freedom concept and calculation for regression ANOVA</span><a class="headerlink" href="#id54" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The concept of degrees of freedom is fundamental to understanding statistical inference, and there are several complementary ways to think about why we get <span class="math notranslate nohighlight">\(n-2\)</span> degrees of freedom for error in simple linear regression. Understanding these different perspectives will help you develop intuition for more complex statistical procedures.</p>
<p><strong>The Information Lens: “Paying the Price of Estimation”</strong></p>
<p>Every observation <span class="math notranslate nohighlight">\(y_i\)</span> initially contributes one independent piece of information about the population. However, when we estimate parameters from the data, we “use up” some of this information.</p>
<p>Think of it this way: we start with <span class="math notranslate nohighlight">\(n\)</span> independent pieces of random variation from our <span class="math notranslate nohighlight">\(n\)</span> observations. To estimate our intercept <span class="math notranslate nohighlight">\(b_0\)</span> and slope <span class="math notranslate nohighlight">\(b_1\)</span>, we must impose constraints on the data that “consume” two pieces of this randomness:</p>
<ul class="simple">
<li><p>Estimating <span class="math notranslate nohighlight">\(b_0\)</span> requires that <span class="math notranslate nohighlight">\(\sum_{i=1}^n e_i = 0\)</span> (residuals sum to zero)</p></li>
<li><p>Estimating <span class="math notranslate nohighlight">\(b_1\)</span> requires that <span class="math notranslate nohighlight">\(\sum_{i=1}^n x_i e_i = 0\)</span> (residuals are uncorrelated with X)</p></li>
</ul>
<p>After paying this “price” of estimation, only <span class="math notranslate nohighlight">\(n - 2\)</span> independent pieces of information remain in the residuals.</p>
<p><strong>Connection to Familiar Concepts</strong>: This matches the intuition you developed with one-sample t-tests, where estimating the sample mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> used up 1 degree of freedom, leaving <span class="math notranslate nohighlight">\(n-1\)</span> degrees of freedom for the sample variance. Here, estimating two parameters uses up 2 degrees of freedom.</p>
<p><strong>The Constraint Lens: “Equations the Data Must Satisfy”</strong></p>
<p>When we fit <span class="math notranslate nohighlight">\(Y_i = \hat{y}_i + e_i\)</span> using least squares, we’re solving an optimization problem. The solution must satisfy exactly two linear constraints:</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n e_i = 0 \quad \text{and} \quad \sum_{i=1}^n x_i e_i = 0\]</div>
<p>These aren’t just mathematical curiosities—they’re fundamental requirements that our residuals must satisfy. With <span class="math notranslate nohighlight">\(n\)</span> residual values but 2 constraints, only <span class="math notranslate nohighlight">\(n-2\)</span> residuals can vary independently. The remaining 2 are completely determined by these constraints.</p>
<p><strong>General Pattern</strong>: In multiple regression with <span class="math notranslate nohighlight">\(p\)</span> explanatory variables plus an intercept, we estimate <span class="math notranslate nohighlight">\(p+1\)</span> parameters, creating <span class="math notranslate nohighlight">\(p+1\)</span> constraints on the residuals. This leaves <span class="math notranslate nohighlight">\(n-(p+1)\)</span> degrees of freedom for error.</p>
<p><strong>The Geometric Lens: “Dimensions of Solution Spaces”</strong></p>
<p>From a linear algebra perspective, we can think about the “space” of possible residual vectors. Our <span class="math notranslate nohighlight">\(n\)</span> observations define an <span class="math notranslate nohighlight">\(n\)</span>-dimensional space. When we fit the regression model, we project the data onto a 2-dimensional subspace (spanned by the intercept and slope).</p>
<p>The residuals live in the remaining <span class="math notranslate nohighlight">\(n-2\)</span> dimensional space—the part of the full <span class="math notranslate nohighlight">\(n\)</span>-dimensional space that’s orthogonal (perpendicular) to our fitted model. This geometric perspective shows that the dimension of the “leftover” space after fitting is exactly our error degrees of freedom.</p>
<p><strong>The Distribution Lens: “Where the Chi-Square Comes From”</strong></p>
<p>The degrees of freedom directly determine the shape of our sampling distributions. Under our normality assumptions:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\frac{\text{SSE}}{\sigma^2} \sim \chi^2_{n-2}\)</span> (chi-square with <span class="math notranslate nohighlight">\(n-2\)</span> degrees of freedom)</p></li>
<li><p>This feeds into our t-statistics: <span class="math notranslate nohighlight">\(t = \frac{b_1 - \beta_1}{SE(b_1)} \sim t_{n-2}\)</span></p></li>
<li><p>And our F-statistic: <span class="math notranslate nohighlight">\(F = \frac{\text{MSR}}{\text{MSE}} \sim F_{1,n-2}\)</span></p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(n-2\)</span> parameter isn’t arbitrary—it’s precisely the dimension of the space where our residuals can vary independently.</p>
<p><strong>For Simple Linear Regression, Specifically</strong>:</p>
<ul class="simple">
<li><p><strong>df_regression = 1</strong>: We’re fitting one slope parameter (the intercept is determined once we choose the slope)</p></li>
<li><p><strong>df_error = n - 2</strong>: After estimating intercept and slope, <span class="math notranslate nohighlight">\(n-2\)</span> residuals remain free to vary</p></li>
<li><p><strong>df_total = n - 1</strong>: Total variation around the overall mean <span class="math notranslate nohighlight">\(\bar{y}\)</span> has the familiar <span class="math notranslate nohighlight">\(n-1\)</span> degrees of freedom</p></li>
</ul>
<p>The degrees of freedom always sum: <span class="math notranslate nohighlight">\((n-1) = 1 + (n-2)\)</span>.</p>
<p><strong>Why Multiple Perspectives Matter</strong></p>
<p>Different situations call for different ways of thinking about degrees of freedom:</p>
<ul class="simple">
<li><p>The <strong>information lens</strong> helps with intuitive understanding and connects to familiar concepts</p></li>
<li><p>The <strong>constraint lens</strong> is useful when working with model equations and understanding why certain relationships hold</p></li>
<li><p>The <strong>geometric lens</strong> becomes powerful in multiple regression and advanced modeling</p></li>
<li><p>The <strong>distribution lens</strong> is essential for understanding test statistics and p-values</p></li>
</ul>
<p>As you encounter more complex statistical procedures—ANOVA with multiple factors, multiple regression, mixed effects models—you’ll find that some of these perspectives provide clearer insight than others. Having multiple ways to think about the same concept makes you a more flexible and intuitive statistical thinker.</p>
</section>
<section id="conducting-the-f-test-step-by-step-process">
<h3>Conducting the F-Test: Step-by-Step Process<a class="headerlink" href="#conducting-the-f-test-step-by-step-process" title="Link to this heading"></a></h3>
<figure class="align-default" id="id55">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-f-test-steps.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-f-test-steps.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-f-test-steps.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.45 </span><span class="caption-text">Complete four-step process for conducting the F-test for model utility</span><a class="headerlink" href="#id55" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Step 1: Parameter of Interest</strong> (Can be skipped)</p>
<p>For the model utility test, we’re not focusing on a specific parameter but rather on the overall usefulness of the linear relationship. We can skip the explicit parameter statement.</p>
<p><strong>Step 2: Hypotheses</strong></p>
<div class="math notranslate nohighlight">
\[H_0: \text{There is no linear association between } X \text{ and } Y\]</div>
<div class="math notranslate nohighlight">
\[H_a: \text{There is a linear association between } X \text{ and } Y\]</div>
<p><strong>Important</strong>: Always state these hypotheses in the context of your specific problem, replacing “X” and “Y” with the actual variable names and context.</p>
<p><strong>Step 3: Test Statistic and P-value</strong></p>
<p>Degrees of freedom: <span class="math notranslate nohighlight">\(df_1 = 1\)</span>, <span class="math notranslate nohighlight">\(df_2 = n-2\)</span></p>
<p>P-value calculation in R:
<code class="docutils literal notranslate"><span class="pre">`r</span>
<span class="pre">p_value</span> <span class="pre">&lt;-</span> <span class="pre">pf(F_statistic,</span> <span class="pre">df1</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">df2</span> <span class="pre">=</span> <span class="pre">n-2,</span> <span class="pre">lower.tail</span> <span class="pre">=</span> <span class="pre">FALSE)</span>
<span class="pre">`</span></code></p>
<p><strong>Why Always Upper Tail?</strong> The F-distribution is right-skewed and bounded below by zero. Large F-values provide evidence against the null hypothesis (that the model is not useful), so we always calculate <span class="math notranslate nohighlight">\(P(F &gt; F_{\text{observed}})\)</span>.</p>
<p><strong>Step 4: Decision and Conclusion</strong></p>
<p>Compare the p-value to the significance level <span class="math notranslate nohighlight">\(\alpha\)</span>:</p>
<ul class="simple">
<li><p>If p-value ≤ <span class="math notranslate nohighlight">\(\alpha\)</span>: Reject <span class="math notranslate nohighlight">\(H_0\)</span>. We have evidence of a linear association.</p></li>
<li><p>If p-value &gt; <span class="math notranslate nohighlight">\(\alpha\)</span>: Fail to reject <span class="math notranslate nohighlight">\(H_0\)</span>. We do not have sufficient evidence of a linear association.</p></li>
</ul>
<p><strong>Conclusion Template</strong>:</p>
<ul class="simple">
<li><p>If rejecting <span class="math notranslate nohighlight">\(H_0\)</span>: “At the <span class="math notranslate nohighlight">\(\alpha\)</span> significance level, we have sufficient evidence to conclude that there is a linear association between [explanatory variable] and [response variable] in [context].”</p></li>
<li><p>If failing to reject <span class="math notranslate nohighlight">\(H_0\)</span>: “At the <span class="math notranslate nohighlight">\(\alpha\)</span> significance level, we do not have sufficient evidence to conclude that there is a linear association between [explanatory variable] and [response variable] in [context].”</p></li>
</ul>
</section>
</section>
<section id="inference-for-individual-parameters">
<h2><span class="section-number">13.3.5. </span>Inference for Individual Parameters<a class="headerlink" href="#inference-for-individual-parameters" title="Link to this heading"></a></h2>
<div class="video-placeholder" role="group" aria-labelledby="video-ch13-8">
   <iframe
      id="video-ch13-8"
      title="STAT 350 – Chapter 13.8 Simple Linear Regression Model Inference - Slope and Intercept Video"
      src="https://www.youtube.com/embed/_XCCR_oXcL0?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
      allowfullscreen>
   </iframe>
</div><p>While the F-test tells us whether our model provides useful information overall, we often want to make specific inferences about the slope and intercept parameters. This allows us to quantify the nature of the relationship and test specific hypotheses about the parameters.</p>
<section id="rewriting-estimates-as-linear-combinations">
<h3>Rewriting Estimates as Linear Combinations<a class="headerlink" href="#rewriting-estimates-as-linear-combinations" title="Link to this heading"></a></h3>
<figure class="align-default" id="id56">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-parameter-linear-combinations.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-parameter-linear-combinations.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-parameter-linear-combinations.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.46 </span><span class="caption-text">Rewriting slope and intercept estimates as linear combinations of the response values</span><a class="headerlink" href="#id56" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>To develop inference procedures for <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>, we need to understand the statistical properties of our estimates <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span>. The key insight is rewriting these estimates as linear combinations of the response values <span class="math notranslate nohighlight">\(Y_i\)</span>, since the responses are the only random components in our model.</p>
<p><strong>Intercept Rewrite</strong>:</p>
<p>Starting from <span class="math notranslate nohighlight">\(b_0 = \bar{y} - b_1\bar{x}\)</span> and substituting the expression for <span class="math notranslate nohighlight">\(b_1\)</span>, we can show:</p>
<p><strong>Slope Rewrite</strong>:</p>
<p>Starting from the least squares formula and using algebraic manipulation:</p>
<p><strong>Why This Matters</strong>: Both estimates are linear combinations (weighted averages) of the normally distributed response values <span class="math notranslate nohighlight">\(Y_i\)</span>. Since linear combinations of independent normal random variables are also normally distributed, both <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> follow normal distributions under our model assumptions.</p>
<p><strong>Expected Values and Variances</strong></p>
<p>Through careful application of expectation and variance properties:</p>
<p><strong>For the Intercept</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E[b_0] = \beta_0\)</span> (unbiased estimate)</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(b_0) = \sigma^2 \left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\)</span></p></li>
</ul>
<p><strong>For the Slope</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(E[b_1] = \beta_1\)</span> (unbiased estimate)</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(b_1) = \frac{\sigma^2}{S_{xx}}\)</span></p></li>
</ul>
<p><strong>Distributions Under Normality</strong>:</p>
</section>
<section id="standard-errors-and-t-distribution">
<h3>Standard Errors and t-Distribution<a class="headerlink" href="#standard-errors-and-t-distribution" title="Link to this heading"></a></h3>
<p>Since <span class="math notranslate nohighlight">\(\sigma^2\)</span> is unknown, we estimate it using the mean squared error:</p>
<p><strong>Standard Errors</strong>:</p>
<p><strong>t-Distribution Result</strong>: When we replace <span class="math notranslate nohighlight">\(\sigma^2\)</span> with <span class="math notranslate nohighlight">\(s^2\)</span>, the normal distribution becomes a t-distribution with <span class="math notranslate nohighlight">\(n-2\)</span> degrees of freedom:</p>
</section>
<section id="confidence-intervals-for-parameters">
<h3>Confidence Intervals for Parameters<a class="headerlink" href="#confidence-intervals-for-parameters" title="Link to this heading"></a></h3>
<p><strong>General Form</strong>:</p>
<p><strong>For the Slope</strong>:</p>
<p><strong>For the Intercept</strong>:</p>
<p><strong>Interpretation</strong>: We are <span class="math notranslate nohighlight">\((1-\alpha) \times 100\%\)</span> confident that the true parameter value lies within the calculated interval.</p>
<p><strong>Critical Value in R</strong>:
<code class="docutils literal notranslate"><span class="pre">`r</span>
<span class="pre">t_critical</span> <span class="pre">&lt;-</span> <span class="pre">qt(alpha/2,</span> <span class="pre">df</span> <span class="pre">=</span> <span class="pre">n-2,</span> <span class="pre">lower.tail</span> <span class="pre">=</span> <span class="pre">FALSE)</span>
<span class="pre">`</span></code></p>
</section>
<section id="hypothesis-testing-for-parameters">
<h3>Hypothesis Testing for Parameters<a class="headerlink" href="#hypothesis-testing-for-parameters" title="Link to this heading"></a></h3>
<p><strong>The Four-Step Process for Slope Testing</strong>:</p>
<p><strong>Step 1: Parameter of Interest</strong></p>
<p>We are interested in <span class="math notranslate nohighlight">\(\beta_1\)</span>, the true slope of the population regression line relating [explanatory variable] to [response variable].</p>
<p><strong>Step 2: Hypotheses</strong></p>
<p>Most commonly:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span> (no linear relationship)</p></li>
<li><p><span class="math notranslate nohighlight">\(H_a: \beta_1 \neq 0\)</span> (linear relationship exists)</p></li>
</ul>
<p>But we can test other values:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \beta_1 = \beta_{10}\)</span> (for any specified value <span class="math notranslate nohighlight">\(\beta_{10}\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(H_a: \beta_1 \neq \beta_{10}\)</span> (or <span class="math notranslate nohighlight">\(&gt;\)</span> or <span class="math notranslate nohighlight">\(&lt;\)</span> for one-sided tests)</p></li>
</ul>
<p><strong>Step 3: Test Statistic and P-value</strong></p>
<p>Degrees of freedom: <span class="math notranslate nohighlight">\(n-2\)</span></p>
<p>P-value calculation depends on the alternative hypothesis:
<a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>r
# Two-sided test
p_value &lt;- 2 * pt(abs(t_stat), df = n-2, lower.tail = FALSE)</p>
<p># Upper tail test
p_value &lt;- pt(t_stat, df = n-2, lower.tail = FALSE)</p>
<p># Lower tail test
p_value &lt;- pt(t_stat, df = n-2, lower.tail = TRUE)
<a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<p><strong>Step 4: Decision and Conclusion</strong></p>
<p>Compare p-value to <span class="math notranslate nohighlight">\(\alpha\)</span> and draw conclusions about the slope parameter in context.</p>
<p><strong>Connection Between F-test and t-test</strong></p>
<p>For simple linear regression with one explanatory variable, there’s a direct mathematical relationship:</p>
<p>when testing <span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span>.</p>
<p><strong>Why Both Tests Matter</strong>: In simple linear regression, the F-test and t-test for <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> are equivalent. However, in multiple regression:</p>
<ul class="simple">
<li><p>The F-test assesses overall model utility (are any of the explanatory variables useful?)</p></li>
<li><p>Individual t-tests assess each explanatory variable separately (is this specific variable useful?)</p></li>
</ul>
<p>Both perspectives provide valuable but different information about model components.</p>
</section>
</section>
<section id="implementation-in-r">
<h2><span class="section-number">13.3.6. </span>Implementation in R<a class="headerlink" href="#implementation-in-r" title="Link to this heading"></a></h2>
<p><strong>Fitting the Model</strong>:</p>
<p><a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>r
# Fit linear model
fit &lt;- lm(response ~ explanatory, data = dataset_name)</p>
<p># Extract coefficients
coefficients(fit)  # or fit`coefficients</p>
<p># Extract residuals and fitted values
residuals(fit)     # or fit`residuals
fitted(fit)        # or fit`fitted.values
<a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a></p>
<p><strong>Getting Inference Results</strong>:</p>
<p><a href="#id17"><span class="problematic" id="id18">``</span></a><a href="#id19"><span class="problematic" id="id20">`</span></a>r
# Complete summary with tests and R-squared
summary(fit)</p>
<p># ANOVA table for F-test
anova(fit)
# or
summary(aov(fit))
<a href="#id21"><span class="problematic" id="id22">``</span></a><a href="#id23"><span class="problematic" id="id24">`</span></a></p>
<p><strong>Manual Calculations</strong> (when given summary statistics rather than raw data):</p>
<p><a href="#id25"><span class="problematic" id="id26">``</span></a><a href="#id27"><span class="problematic" id="id28">`</span></a>r
# Calculate test statistics manually
F_stat &lt;- MSR / MSE
t_stat &lt;- (b1 - beta1_null) / SE_b1</p>
<p># Calculate p-values
p_value_F &lt;- pf(F_stat, df1 = 1, df2 = n-2, lower.tail = FALSE)
p_value_t &lt;- 2 * pt(abs(t_stat), df = n-2, lower.tail = FALSE)</p>
<p># Calculate confidence intervals
margin_error &lt;- qt(alpha/2, df = n-2, lower.tail = FALSE) * SE_b1
CI_lower &lt;- b1 - margin_error
CI_upper &lt;- b1 + margin_error
<a href="#id29"><span class="problematic" id="id30">``</span></a><a href="#id31"><span class="problematic" id="id32">`</span></a></p>
</section>
<section id="integrated-example-blood-pressure-study-complete-analysis">
<h2><span class="section-number">13.3.7. </span>Integrated Example: Blood Pressure Study Complete Analysis<a class="headerlink" href="#integrated-example-blood-pressure-study-complete-analysis" title="Link to this heading"></a></h2>
<p>Let’s work through a complete analysis of the blood pressure treatment study, incorporating all diagnostic and inference procedures.</p>
<p><strong>Research Context</strong>: Investigating whether patient age affects the change in blood pressure after 24 hours of a new treatment (<span class="math notranslate nohighlight">\(n = 11\)</span> patients).</p>
<p><strong>Step 1: Diagnostic Analysis</strong></p>
<p><a href="#id33"><span class="problematic" id="id34">``</span></a><a href="#id35"><span class="problematic" id="id36">`</span></a>r
# Check assumptions
plot(age, bp_change)  # Scatter plot for linearity and homoscedasticity
abline(lm(bp_change ~ age))</p>
<p># Fit model and create residual plot
fit &lt;- lm(bp_change ~ age)
plot(age, residuals(fit))  # Residual plot
abline(h = 0)</p>
<p># Check normality of residuals
hist(residuals(fit))
qqnorm(residuals(fit))
qqline(residuals(fit))
<a href="#id37"><span class="problematic" id="id38">``</span></a><a href="#id39"><span class="problematic" id="id40">`</span></a></p>
<p><strong>Assessment</strong>: Linear relationship appears reasonable, constant variance assumption shows some minor violations but nothing severe enough to invalidate analysis with <span class="math notranslate nohighlight">\(n = 11\)</span>. Normality appears reasonable.</p>
<p><strong>Step 2: F-test for Model Utility</strong></p>
<p>Using our fitted model <span class="math notranslate nohighlight">\(\hat{y} = 20.11 - 0.526x\)</span> with <span class="math notranslate nohighlight">\(\text{SSR} = 556\)</span>, <span class="math notranslate nohighlight">\(\text{SSE} = 383\)</span>, <span class="math notranslate nohighlight">\(\text{SST} = 939\)</span>:</p>
<p><strong>Hypotheses</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0\)</span>: There is no linear association between patient age and change in blood pressure</p></li>
<li><p><span class="math notranslate nohighlight">\(H_a\)</span>: There is a linear association between patient age and change in blood pressure</p></li>
</ul>
<p><strong>Test Statistic</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{MSR} = \text{SSR}/1 = 556\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{MSE} = \text{SSE}/9 = 42.56\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(F = 556/42.56 = 13.06\)</span></p></li>
</ul>
<p><strong>P-value</strong>: <span class="math notranslate nohighlight">\(P(F_{1,9} &gt; 13.06) = 0.0055\)</span></p>
<p><strong>Conclusion</strong>: At <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, we reject <span class="math notranslate nohighlight">\(H_0\)</span> and conclude there is sufficient evidence of a linear association between patient age and change in blood pressure.</p>
<p><strong>Step 3: Inference for the Slope</strong></p>
<p><strong>Parameter of Interest</strong>: <span class="math notranslate nohighlight">\(\beta_1\)</span>, the true change in blood pressure reduction per year increase in patient age.</p>
<p><strong>Hypotheses</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(H_a: \beta_1 \neq 0\)</span></p></li>
</ul>
<p><strong>Test Statistic</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(SE(b_1) = \sqrt{\text{MSE}/S_{xx}} = \sqrt{42.56/2008} = 0.146\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(t = \frac{-0.526 - 0}{0.146} = -3.61\)</span></p></li>
</ul>
<p><strong>P-value</strong>: <span class="math notranslate nohighlight">\(P(|t_9| &gt; 3.61) = 2 \times P(t_9 &gt; 3.61) = 0.0055\)</span></p>
<p><strong>95% Confidence Interval</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(t_{0.025,9} = 2.262\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(-0.526 \pm 2.262 \times 0.146 = -0.526 \pm 0.330 = (-0.856, -0.196)\)</span></p></li>
</ul>
<p><strong>Conclusion</strong>: We are 95% confident that each additional year of age is associated with a decrease in blood pressure between 0.196 and 0.856 mm Hg after treatment.</p>
<p><strong>Note</strong>: The F-test and t-test give identical p-values (0.0055) since <span class="math notranslate nohighlight">\(F = t^2 = (-3.61)^2 = 13.03 \approx 13.06\)</span> (small rounding differences).</p>
</section>
<section id="parameter-distribution-properties">
<h2><span class="section-number">13.3.8. </span>Parameter Distribution Properties<a class="headerlink" href="#parameter-distribution-properties" title="Link to this heading"></a></h2>
<figure class="align-default" id="id57">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-parameter-properties.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-parameter-properties.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-parameter-properties.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.47 </span><span class="caption-text">Summary of the statistical properties of slope and intercept estimators showing unbiasedness and variance formulas</span><a class="headerlink" href="#id57" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The mathematical foundation for our inference procedures rests on the key statistical properties of our parameter estimates.</p>
<p><strong>Slope Properties</strong>:</p>
<ul class="simple">
<li><p><strong>Unbiased</strong>: <span class="math notranslate nohighlight">\(E[b_1] = \beta_1\)</span></p></li>
<li><p><strong>Variance</strong>: <span class="math notranslate nohighlight">\(\text{Var}(b_1) = \frac{\sigma^2}{S_{xx}}\)</span></p></li>
<li><p><strong>Distribution</strong>: <span class="math notranslate nohighlight">\(b_1 \sim N\left(\beta_1, \frac{\sigma^2}{S_{xx}}\right)\)</span></p></li>
</ul>
<p><strong>Intercept Properties</strong>:</p>
<ul class="simple">
<li><p><strong>Unbiased</strong>: <span class="math notranslate nohighlight">\(E[b_0] = \beta_0\)</span></p></li>
<li><p><strong>Variance</strong>: <span class="math notranslate nohighlight">\(\text{Var}(b_0) = \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\)</span></p></li>
<li><p><strong>Distribution</strong>: <span class="math notranslate nohighlight">\(b_0 \sim N\left(\beta_0, \sigma^2\left(\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right)\right)\)</span></p></li>
</ul>
<p><strong>Key Insights</strong>:</p>
<ul class="simple">
<li><p>Both estimates are linear combinations of the normally distributed response values</p></li>
<li><p>The slope variance depends only on the error variance and the spread of X values</p></li>
<li><p>The intercept variance includes additional uncertainty when <span class="math notranslate nohighlight">\(\bar{x} \neq 0\)</span></p></li>
</ul>
<section id="standard-error-formulas-and-confidence-intervals">
<h3>Standard Error Formulas and Confidence Intervals<a class="headerlink" href="#standard-error-formulas-and-confidence-intervals" title="Link to this heading"></a></h3>
<figure class="align-default" id="id58">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-interval-slope.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-interval-slope.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-interval-slope.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.48 </span><span class="caption-text">Confidence interval formula for the slope parameter with all components clearly labeled</span><a class="headerlink" href="#id58" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Since <span class="math notranslate nohighlight">\(\sigma^2\)</span> is unknown, we estimate it using <span class="math notranslate nohighlight">\(s^2 = \text{MSE}\)</span> and use the t-distribution:</p>
<p><strong>Standard Error of the Slope</strong>:</p>
<div class="math notranslate nohighlight">
\[SE(b_1) = \sqrt{\frac{\text{MSE}}{S_{xx}}}\]</div>
<p>where <span class="math notranslate nohighlight">\(S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2\)</span>.</p>
<p><strong>Confidence Interval for the Slope</strong>:</p>
<div class="math notranslate nohighlight">
\[b_1 \pm t_{\alpha/2, n-2} \sqrt{\frac{\text{MSE}}{S_{xx}}}\]</div>
<p>This provides a range of plausible values for the true population slope <span class="math notranslate nohighlight">\(\beta_1\)</span> with <span class="math notranslate nohighlight">\((1-\alpha) \times 100\%\)</span> confidence.</p>
<p><strong>Interpretation</strong>: We are <span class="math notranslate nohighlight">\((1-\alpha) \times 100\%\)</span> confident that the true change in the response variable for each one-unit increase in the explanatory variable lies within this interval.</p>
</section>
<section id="complete-hypothesis-testing-framework">
<h3>Complete Hypothesis Testing Framework<a class="headerlink" href="#complete-hypothesis-testing-framework" title="Link to this heading"></a></h3>
<figure class="align-default" id="id59">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-hypothesis-test-slope.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-hypothesis-test-slope.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-hypothesis-test-slope.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.49 </span><span class="caption-text">Complete framework for hypothesis testing about the slope parameter</span><a class="headerlink" href="#id59" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Step 1: Parameter of Interest</strong></p>
<p>We are interested in <span class="math notranslate nohighlight">\(\beta_1\)</span>, the true population slope of the mean response line <span class="math notranslate nohighlight">\(\mu_{Y|X=x}\)</span>.</p>
<p><strong>Step 2: Hypotheses</strong></p>
<p>The general form allows for various null values <span class="math notranslate nohighlight">\(\beta_{10}\)</span>:</p>
<div class="math notranslate nohighlight">
\[H_0: \beta_1 = \beta_{10} \quad \text{vs} \quad H_a: \beta_1 \neq \beta_{10}\]</div>
<p>Alternative formulations:</p>
<ul class="simple">
<li><p>One-sided upper: <span class="math notranslate nohighlight">\(H_a: \beta_1 &gt; \beta_{10}\)</span></p></li>
<li><p>One-sided lower: <span class="math notranslate nohighlight">\(H_a: \beta_1 &lt; \beta_{10}\)</span></p></li>
</ul>
<p><strong>Step 3: Test Statistic and P-value</strong></p>
<p>The test statistic follows a t-distribution with <span class="math notranslate nohighlight">\(n-2\)</span> degrees of freedom.</p>
<p><strong>P-value calculation depends on the alternative</strong>:</p>
<ul class="simple">
<li><p>Two-sided: <span class="math notranslate nohighlight">\(\text{p-value} = 2P(t_{n-2} &gt; |t|)\)</span></p></li>
<li><p>Upper tail: <span class="math notranslate nohighlight">\(\text{p-value} = P(t_{n-2} &gt; t)\)</span></p></li>
<li><p>Lower tail: <span class="math notranslate nohighlight">\(\text{p-value} = P(t_{n-2} &lt; t)\)</span></p></li>
</ul>
<p><strong>Step 4: Decision and Conclusion</strong></p>
<p>Compare p-value to <span class="math notranslate nohighlight">\(\alpha\)</span> and state conclusions in context of the problem.</p>
</section>
<section id="special-case-equivalence-of-f-test-and-t-test">
<h3>Special Case: Equivalence of F-test and t-test<a class="headerlink" href="#special-case-equivalence-of-f-test-and-t-test" title="Link to this heading"></a></h3>
<figure class="align-default" id="id60">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-f-t-equivalence.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-f-t-equivalence.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-f-t-equivalence.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.50 </span><span class="caption-text">Mathematical relationship between F-test and t-test when testing slope equals zero</span><a class="headerlink" href="#id60" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>When testing <span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span> versus <span class="math notranslate nohighlight">\(H_a: \beta_1 \neq 0\)</span>, there’s a direct mathematical relationship:</p>
<p>Specifically:</p>
<p>This equivalence means both tests provide identical p-values and lead to identical conclusions for this specific hypothesis.</p>
<p><strong>Why Both Tests Matter</strong>: While equivalent in simple linear regression, they serve different purposes in multiple regression:</p>
<ul class="simple">
<li><p>F-test: Tests overall model utility (are any predictors useful?)</p></li>
<li><p>t-tests: Test individual predictors (is this specific predictor useful?)</p></li>
</ul>
<p><strong>The F-test is more general</strong> and extends naturally to multiple regression scenarios where we test several slopes simultaneously.</p>
</section>
</section>
<section id="implementation-in-r-complete-workflow">
<h2><span class="section-number">13.3.9. </span>Implementation in R: Complete Workflow<a class="headerlink" href="#implementation-in-r-complete-workflow" title="Link to this heading"></a></h2>
<figure class="align-default" id="id61">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-r-implementation.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-r-implementation.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-r-implementation.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.51 </span><span class="caption-text">Complete R workflow for fitting models and conducting inference</span><a class="headerlink" href="#id61" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Model Fitting and Basic Output</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the linear model</span>
<span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">response_variable</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">explanatory_variable</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dataFrame</span><span class="p">)</span>

<span class="c1"># Extract key components</span>
<span class="nf">coefficients</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">     </span><span class="c1"># Get b0 and b1</span>
<span class="nf">residuals</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">        </span><span class="c1"># Get residuals for diagnostics</span>
<span class="nf">fitted.values</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">    </span><span class="c1"># Get predicted values</span>

<span class="c1"># Complete inference summary</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">          </span><span class="c1"># Includes R², F-test, t-tests, standard errors</span>

<span class="c1"># ANOVA table</span>
<span class="nf">anova</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">           </span><span class="c1"># Or summary(aov(fit))</span>
</pre></div>
</div>
<p><strong>What summary(fit) Provides</strong>:</p>
<ul class="simple">
<li><p>Coefficient estimates (<span class="math notranslate nohighlight">\(b_0\)</span>, <span class="math notranslate nohighlight">\(b_1\)</span>) with standard errors</p></li>
<li><p>t-statistics and p-values for testing each coefficient equals zero</p></li>
<li><p>R-squared and adjusted R-squared</p></li>
<li><p>F-statistic and p-value for overall model utility</p></li>
<li><p>Residual standard error (estimate of <span class="math notranslate nohighlight">\(\sigma\)</span>)</p></li>
</ul>
<p><strong>Manual Calculations</strong> (useful for understanding or when given summary statistics):</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate standard error of slope manually</span>
<span class="n">SE_b1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="n">MSE</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Sxx</span><span class="p">)</span>

<span class="c1"># Calculate t-statistic</span>
<span class="n">t_stat</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="n">b1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">beta1_null</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">SE_b1</span>

<span class="c1"># Calculate confidence interval</span>
<span class="n">t_critical</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">qt</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="n">lower.tail</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span>
<span class="n">CI_lower</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">b1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t_critical</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">SE_b1</span>
<span class="n">CI_upper</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">b1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">t_critical</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">SE_b1</span>

<span class="c1"># Calculate p-values</span>
<span class="n">p_value_two_sided</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">pt</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">t_stat</span><span class="p">),</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="n">lower.tail</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span>
<span class="n">p_value_F</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">pf</span><span class="p">(</span><span class="n">F_stat</span><span class="p">,</span><span class="w"> </span><span class="n">df1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">df2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="m">-2</span><span class="p">,</span><span class="w"> </span><span class="n">lower.tail</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="comprehensive-example-blood-pressure-study-final-analysis">
<h2><span class="section-number">13.3.10. </span>Comprehensive Example: Blood Pressure Study Final Analysis<a class="headerlink" href="#comprehensive-example-blood-pressure-study-final-analysis" title="Link to this heading"></a></h2>
<p>Let’s complete our blood pressure analysis with full parameter inference.</p>
<p><strong>Given Information</strong>:</p>
<ul class="simple">
<li><p>Sample size: <span class="math notranslate nohighlight">\(n = 11\)</span></p></li>
<li><p>Fitted model: <span class="math notranslate nohighlight">\(\hat{y} = 20.11 - 0.526x\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{SSE} = 383\)</span>, so <span class="math notranslate nohighlight">\(\text{MSE} = 383/9 = 42.56\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(S_{xx} = 2008\)</span> (from our previous calculations)</p></li>
</ul>
<p><strong>Slope Inference</strong>:</p>
<p><strong>Standard Error</strong>:</p>
<p><span class="math notranslate nohighlight">\(SE(b_1) = \sqrt{42.56/2008} = \sqrt{0.0212} = 0.146\)</span></p>
<p><strong>95% Confidence Interval</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(t_{0.025,9} = 2.262\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(-0.526 \pm 2.262 \times 0.146 = -0.526 \pm 0.330\)</span></p></li>
<li><p><strong>Interval</strong>: <span class="math notranslate nohighlight">\((-0.856, -0.196)\)</span></p></li>
</ul>
<p><strong>Interpretation</strong>: We are 95% confident that each additional year of patient age is associated with an additional decrease in blood pressure between 0.196 and 0.856 mm Hg after treatment.</p>
<p><strong>Hypothesis Test</strong> (<span class="math notranslate nohighlight">\(H_0: \beta_1 = 0\)</span> vs <span class="math notranslate nohighlight">\(H_a: \beta_1 \neq 0\)</span>):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(t = \frac{-0.526 - 0}{0.146} = -3.61\)</span></p></li>
<li><p>p-value = <span class="math notranslate nohighlight">\(2 \times P(t_9 &gt; 3.61) = 0.0055\)</span></p></li>
<li><p><strong>Conclusion</strong>: At <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, we reject <span class="math notranslate nohighlight">\(H_0\)</span> and conclude there is significant evidence that patient age affects the change in blood pressure after treatment.</p></li>
</ul>
<p><strong>Verification of F-test and t-test Equivalence</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(t^2 = (-3.61)^2 = 13.03\)</span></p></li>
<li><p>Our F-statistic was 13.06 (small rounding differences)</p></li>
<li><p>Both tests give p-value ≈ 0.0055</p></li>
</ul>
</section>
<section id="summary-of-diagnostic-and-inference-workflow">
<h2><span class="section-number">13.3.11. </span>Summary of Diagnostic and Inference Workflow<a class="headerlink" href="#summary-of-diagnostic-and-inference-workflow" title="Link to this heading"></a></h2>
<p>The complete regression analysis workflow involves these essential steps:</p>
<p><strong>1. Exploratory Analysis</strong></p>
<ul class="simple">
<li><p>Create scatter plot to assess initial relationship</p></li>
<li><p>Determine which variable should be explanatory vs. response</p></li>
<li><p>Look for obvious outliers or non-linear patterns</p></li>
</ul>
<p><strong>2. Model Fitting</strong></p>
<ul class="simple">
<li><p>Fit least squares regression line</p></li>
<li><p>Calculate basic summary statistics and R-squared</p></li>
</ul>
<p><strong>3. Diagnostic Checking</strong> (Critical - do before inference!)</p>
<ul class="simple">
<li><p>Create residual plots to check linearity and constant variance</p></li>
<li><p>Examine histograms and QQ plots of residuals for normality</p></li>
<li><p>Identify any influential points or assumption violations</p></li>
</ul>
<p><strong>4. Inference Procedures</strong> (only if assumptions are reasonable)</p>
<ul class="simple">
<li><p>F-test for overall model utility</p></li>
<li><p>t-tests and confidence intervals for individual parameters</p></li>
<li><p>Interpret all results in context of the original problem</p></li>
</ul>
<p><strong>5. Model Use</strong> (if diagnostics and tests support the model)</p>
<ul class="simple">
<li><p>Make predictions with appropriate uncertainty quantification</p></li>
<li><p>Draw scientific conclusions about the relationship</p></li>
</ul>
</section>
<section id="when-assumptions-are-violated">
<h2><span class="section-number">13.3.12. </span>When Assumptions Are Violated<a class="headerlink" href="#when-assumptions-are-violated" title="Link to this heading"></a></h2>
<p>If diagnostic procedures reveal serious assumption violations:</p>
<p><strong>Linearity Violations</strong>:</p>
<ul class="simple">
<li><p>Consider transformations of variables (log, square root, etc.)</p></li>
<li><p>Fit polynomial or other non-linear models (beyond course scope)</p></li>
<li><p>Use piecewise or segmented regression for different regions</p></li>
</ul>
<p><strong>Constant Variance Violations</strong>:</p>
<ul class="simple">
<li><p>Variable transformations may help stabilize variance</p></li>
<li><p>Weighted least squares methods (beyond course scope)</p></li>
<li><p>Robust regression techniques</p></li>
</ul>
<p><strong>Normality Violations</strong>:</p>
<ul class="simple">
<li><p>Often less critical for large sample sizes (Central Limit Theorem)</p></li>
<li><p>Bootstrap methods for inference (beyond course scope)</p></li>
<li><p>Non-parametric alternatives</p></li>
</ul>
<p><strong>Independence Violations</strong>:</p>
<ul class="simple">
<li><p>Time series methods for correlated observations</p></li>
<li><p>Mixed effects models for clustered data</p></li>
<li><p>These require specialized techniques beyond this course</p></li>
</ul>
<p><strong>Important Principle</strong>: When assumptions are seriously violated, our inference procedures may not be reliable. It’s better to address the violations or acknowledge limitations than to proceed with invalid analysis.</p>
</section>
<section id="building-toward-prediction-and-advanced-topics">
<h2><span class="section-number">13.3.13. </span>Building Toward Prediction and Advanced Topics<a class="headerlink" href="#building-toward-prediction-and-advanced-topics" title="Link to this heading"></a></h2>
<p>The diagnostic and inference tools developed in this chapter provide the foundation for the final components of regression analysis:</p>
<p><strong>What’s Coming Next</strong>:
- <strong>Prediction intervals</strong>: Using our fitted model to predict new observations with appropriate uncertainty
- <strong>Confidence intervals for the mean response</strong>: Estimating expected values at specific X values
- <strong>Model comparison and selection</strong>: Comparing different potential models
- <strong>Introduction to multiple regression</strong>: Extending to multiple explanatory variables</p>
<p><strong>The Bigger Picture</strong>: The workflow established here—visual exploration, model fitting, diagnostic checking, and formal inference—forms the backbone of all statistical modeling. Whether working with simple linear regression, multiple regression, or advanced modeling techniques, this systematic approach ensures reliable and interpretable results.</p>
<p>The combination of diagnostic tools and inference procedures gives us confidence in our conclusions while maintaining appropriate humility about the limitations of our models. This balance between statistical rigor and practical insight represents the essence of effective statistical analysis.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Assumption checking must precede inference</strong>: Diagnostic plots are essential for verifying that our model is appropriate before conducting hypothesis tests or constructing confidence intervals.</p></li>
<li><p><strong>Residual plots are more sensitive than scatter plots</strong>: They amplify patterns and make assumption violations easier to detect, particularly for constant variance and linearity.</p></li>
<li><p><strong>Four key diagnostics work together</strong>: Scatter plots, residual plots, histograms of residuals, and QQ plots provide complementary information about different aspects of model adequacy.</p></li>
<li><p><strong>The F-test assesses overall model utility</strong>: It tests whether the linear relationship explains a significant portion of the variability in the response variable.</p></li>
<li><p><strong>ANOVA decomposition provides intuitive understanding</strong>: SST = SSR + SSE shows how total variation splits into explained and unexplained components.</p></li>
<li><p><strong>Parameter inference follows familiar patterns</strong>: Confidence intervals and hypothesis tests for slope and intercept use the same principles as previous inference procedures, adapted for regression context.</p></li>
<li><p><strong>Standard errors incorporate both error variance and design</strong>: SE(b₁) = √(MSE/Sₓₓ) shows that precision depends on both residual variation and spread of X values.</p></li>
<li><p><strong>F-test and t-test are equivalent for simple regression</strong>: When testing β₁ = 0, both approaches give identical conclusions, but the F-test generalizes to multiple regression.</p></li>
<li><p><strong>Degrees of freedom reflect parameters estimated</strong>: df = n-2 accounts for estimating both slope and intercept from the data.</p></li>
<li><p><strong>R provides comprehensive output</strong>: The summary() function includes all essential inference results, while diagnostic plots require additional commands.</p></li>
<li><p><strong>Violations have consequences</strong>: Serious assumption violations can invalidate inference procedures, requiring alternative approaches or model modifications.</p></li>
<li><p><strong>Context drives interpretation</strong>: All statistical results must be interpreted in terms of the original research question and practical significance.</p></li>
</ol>
</div>
<section id="exercises">
<h3>Exercises<a class="headerlink" href="#exercises" title="Link to this heading"></a></h3>
<ol class="arabic">
<li><p><strong>Diagnostic Interpretation</strong>: For each residual plot pattern described, identify the assumption violation and potential consequences:</p>
<ol class="loweralpha simple">
<li><p>Residuals form a cone shape, spreading out as X increases</p></li>
<li><p>Residuals show a clear curved (U-shaped) pattern</p></li>
<li><p>Residuals appear randomly scattered around zero with constant spread</p></li>
<li><p>Most residuals are near zero with a few extremely large positive and negative values</p></li>
<li><p>Residuals show alternating positive and negative values in sequence</p></li>
</ol>
</li>
<li><p><strong>ANOVA Table Completion</strong>: Given the following partial ANOVA table for a regression with n = 15, complete all missing values:</p></li>
<li><p><strong>Parameter Inference</strong>: A study of house prices yields the regression equation Price = 45,000 + 120 × Size, where Price is in dollars and Size is in square feet. With n = 20, MSE = 50,000,000, and Sₓₓ = 2500:</p>
<ol class="loweralpha simple">
<li><p>Calculate the standard error of the slope</p></li>
<li><p>Construct a 95% confidence interval for the slope</p></li>
<li><p>Test H₀: β₁ = 100 vs Hₐ: β₁ ≠ 100 at α = 0.05</p></li>
<li><p>Interpret the slope coefficient in context</p></li>
</ol>
</li>
<li><p><strong>F-test vs t-test</strong>: Using the house price data from Exercise 3:</p>
<ol class="loweralpha simple">
<li><p>Conduct the F-test for model utility</p></li>
<li><p>Conduct the t-test for H₀: β₁ = 0 vs Hₐ: β₁ ≠ 0</p></li>
<li><p>Verify that F = t² and explain why this relationship holds</p></li>
<li><p>Discuss when you might prefer one test over the other</p></li>
</ol>
</li>
<li><p><strong>Assumption Checking Protocol</strong>: Design a systematic approach for checking regression assumptions:</p>
<ol class="loweralpha simple">
<li><p>List the specific plots you would create and in what order</p></li>
<li><p>Describe what to look for in each plot</p></li>
<li><p>Explain how you would decide whether violations are serious enough to invalidate analysis</p></li>
<li><p>Suggest potential remedies for each type of violation</p></li>
</ol>
</li>
<li><p><strong>Real Data Analysis</strong>: Collect data on two quantitative variables of interest (at least 15 observations):</p>
<ol class="loweralpha simple">
<li><p>Create appropriate exploratory plots</p></li>
<li><p>Fit a simple linear regression model</p></li>
<li><p>Conduct complete diagnostic analysis</p></li>
<li><p>Perform F-test and parameter inference</p></li>
<li><p>Interpret all results in context</p></li>
<li><p>Discuss any limitations or concerns</p></li>
</ol>
</li>
<li><p><strong>R Implementation</strong>: Write R code to perform a complete regression analysis:</p>
<ol class="loweralpha simple">
<li><p>Fit the model and extract basic output</p></li>
<li><p>Create all necessary diagnostic plots</p></li>
<li><p>Conduct F-test and t-tests manually (not using summary output)</p></li>
<li><p>Calculate confidence intervals for the slope</p></li>
<li><p>Compare your manual calculations to R’s built-in results</p></li>
</ol>
</li>
<li><p><strong>Critical Evaluation</strong>: A researcher reports: “The regression has R² = 0.95, so the model is excellent and all assumptions are satisfied.”</p>
<ol class="loweralpha simple">
<li><p>What’s wrong with this reasoning?</p></li>
<li><p>What additional information would you need to evaluate the model?</p></li>
<li><p>Describe how a high R² could coexist with serious assumption violations</p></li>
<li><p>What would you recommend the researcher do?</p></li>
</ol>
</li>
<li><p><strong>Design Considerations</strong>: Explain how each factor affects the precision of slope estimation:</p>
<ol class="loweralpha simple">
<li><p>Increasing the sample size n</p></li>
<li><p>Increasing the range of X values observed</p></li>
<li><p>Reducing the error variance σ²</p></li>
<li><p>Changing from X values clustered together to X values spread out</p></li>
</ol>
</li>
<li><p><strong>Confidence Interval Interpretation</strong>: For each confidence interval interpretation, identify whether it’s correct or incorrect and explain:</p>
<ol class="loweralpha simple">
<li><p>“There’s a 95% chance that the true slope lies in this interval”</p></li>
<li><p>“95% of sample slopes will fall in this interval”</p></li>
<li><p>“If we repeated this study many times, 95% of the intervals would contain the true slope”</p></li>
<li><p>“We’re 95% confident about the slope value for this specific dataset”</p></li>
</ol>
</li>
<li><p><strong>Hypothesis Testing Scenarios</strong>: For each research scenario, formulate appropriate hypotheses:</p>
<ol class="loweralpha simple">
<li><p>Testing whether there’s any linear relationship between study hours and test scores</p></li>
<li><p>Testing whether the slope of salary vs. experience is at least $2000 per year</p></li>
<li><p>Testing whether the relationship between temperature and ice cream sales is negative</p></li>
<li><p>Testing whether the effect of fertilizer on plant growth is exactly 5 cm per gram</p></li>
</ol>
</li>
<li><p><strong>Comprehensive Case Study</strong>: A medical researcher studies the relationship between patient age (X) and recovery time in days (Y) for a surgical procedure. With n = 25 patients, the analysis yields:</p>
<ul class="simple">
<li><p>Fitted model: Ŷ = 8.5 + 0.3X</p></li>
<li><p>SSR = 156, SSE = 234, SST = 390</p></li>
<li><p>Sₓₓ = 1200</p></li>
</ul>
<p>Conduct a complete analysis including:</p>
<ol class="loweralpha simple">
<li><p>ANOVA table and R² calculation</p></li>
<li><p>F-test for model utility</p></li>
<li><p>95% confidence interval for the slope</p></li>
<li><p>Test whether the slope exceeds 0.25 days per year</p></li>
<li><p>Practical interpretation of all results</p></li>
<li><p>Discussion of what diagnostic plots you would need to see</p></li>
</ol>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="13-2-simple-linear-regression.html" class="btn btn-neutral float-left" title="13.2. Simple Linear Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="13-4-prediction-robustness.html" class="btn btn-neutral float-right" title="13.4. Prediction, Robustness, and Applied Examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>