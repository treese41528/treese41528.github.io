

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>13.4. Prediction, Robustness, and Applied Examples &mdash; STAT 350</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bac617f8" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT350/course_site/chapter13/lectures/13-4-prediction-robustness.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="13.3. Model Diagnostics and Statistical Inference" href="13-3-diagnostics-inference.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Orientation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-intro.html">Course Introduction &amp; Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#why-statistics-why-now">Why Statistics? Why Now?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#course-roadmap">Course Roadmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#a-clear-note-on-using-ai">A Clear Note on Using AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#getting-started-a-checklist">Getting Started: A Checklist</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#homework-assignments-on-edfinity">Homework Assignments on Edfinity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#miscellaneous-tips">Miscellaneous Tips</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../chapter1/index.html">1. Introduction to Statistics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-1-intro.html">1.1. What Is Statistics?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-2-probability-inference.html">1.2. Probability &amp; Statistical Inference: How Are They Associated?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter2/index.html">2. Graphical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-1-understanding-the-structure-of-data-set.html">2.1. Data Set Structure and Variable Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-2-tools-for-categorical-qualitative-data.html">2.2. Tools for Categorical (Qualitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-3-tools-for-numerical-quantitative-data.html">2.3. Tools for Numerical (Quantitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-4-exploring-quantitative-distributions-modality-shape-and-outliers.html">2.4. Exploring Quantitative Distributions: Modality, Shape &amp; Outliers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter3/index.html">3. Numerical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-1-intro-numerical-summaries.html">3.1. Introduction to Numerical Summaries: Notation and Terminology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-2-measures-of-central-tendency.html">3.2. Measures of Central Tendency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-3-measures-of-variability-range-variance-and-SD.html">3.3. Measures of Variability - Range, Variance, and Standard Deviation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-4-measures-of-variability-IQR-and-5-number-summary.html">3.4. Measures of Variability - Interquartile Range and Five-Number Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-5-choosing-the-right-measure-resistance-to-extreme-values.html">3.5. Choosing the Right Measure &amp; Comparing Measures Across Data Sets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter4/index.html">4. Probability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-1-basic-set-theory.html">4.1. Basic Set Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-2-probability.html">4.2. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-3-conditional-probability.html">4.3. Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-4-law-of-total-probability-and-bayes-rule.html">4.4. Law of Total Probability and Bayes’ Rule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-5-bayes-update-rule-example.html">4.5. Bayesian Updating: Sequential Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-6-independence-of-events.html">4.6. Independence of Events</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter5/index.html">5. Discrete Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-1-discrete-rvs-and-pmfs.html">5.1. Discrete Random Variables and Probability Mass Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-2-joint-pmfs.html">5.2. Joint Probability Mass Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-3-expected-value-of-discrete-rv.html">5.3. Expected Value of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-4-variance-of-discrete-rv.html">5.4. Varianace of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-5-covariance-of-dependent-rvs.html">5.5. Covariance of Dependent Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-6-binomial-distribution.html">5.6. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-7-poisson-distribution.html">5.7. Poisson Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter6/index.html">6. Continuous Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-1-continuous-rvs-and-pdfs.html">6.1. Continuous Random Variables and Probability Density Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-2-expected-value-and-variance-of-cts-rvs.html">6.2. Expected Value and Variance of Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-3-cdfs.html">6.3. Cumulative Distribution Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-4-normal-distribution.html">6.4. Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-5-uniform-distribution.html">6.5. Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-6-exponential-distribution.html">6.6. Exponential Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter7/index.html">7. Sampling Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-1-statistics-and-sampling-distributions.html">7.1. Statistics and Sampling Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-2-sampling-distribution-for-the-sample-mean.html">7.2. Sampling Distribution for the Sample Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-3-clt.html">7.3. The Central Limit Theorem (CLT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-4-discret-rvs-and-clt.html">7.4. Discrete Random Variables and the CLT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter8/index.html">8. Experimental Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-1-experimental-and-sampling-designs.html">8.1. Experimental and Sampling Designs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-2-experimental-design-principles.html">8.2. Experimental Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-3-basic-types-of-experimental-design.html">8.3. Basic Types of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-4-addressing-potential-flaws-in-experimental-design.html">8.4. Addressing Potential Flaws in Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-5-examples-of-experimental-design.html">8.5. Examples of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-6-sampling-design.html">8.6. Sampling Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-7-sampling-bias.html">8.7. Sampling Bias</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter9/index.html">9. Confidence Intervals and Bounds</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-1-intro-statistical-inference.html">9.1. Introduction to Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-2-ci-sigma-known.html">9.2. Confidence Intervals for the Population Mean, When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-3-precision-of-ci-and-sample-size-calculation.html">9.3. Precision of a Confidence Interval and Sample Size Calculation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-4-cb-sigma-known.html">9.4. Confidence Bounds for the Poulation Mean When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-5-ci-cb-sigma-unknown.html">9.5. Confidence Intervals and Bounds When σ is Unknown</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter10/index.html">10. Hypothesis Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-1-ht-errors-and-power.html">10.1. Type I Error, Type II Error, and Power</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-2-ht-for-mean-sigma-known.html">10.2. Hypothesis Test for the Population Mean When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-3-ht-for-mean-sigma-unknown.html">10.3. Hypothesis Test for the Population Mean When σ Is Unknown</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-4-pvalue-significance-conclusion.html">10.4. P-values, Statistical Significance, and Formal Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter11/index.html">11. Two Sample Procedures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-1-ci-ht-two-samples.html">11.1. Confidence Interval/Bound and Hypothesis Test for Two Samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-2-comparing-two-means-independent-sigmas-known.html">11.2. Comparing the Means of Two Independent Populations - Population Variances Are Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-3-comparing-two-means-independent-pooled.html">11.3. Comparing the Means of Two Independent Populations - Pooled Variance Estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-4-comparing-two-means-independent-unpooled.html">11.4. Comparing the Means of Two Independent Populations - No Equal Variance Assumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-5-mean-of-paired-differences-two-dependent-populations.html">11.5. Analyzing the Mean of Paired Differences Between two Dependent Populations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter12/index.html">12. ANOVA</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-1-intro-one-way-anova.html">12.1. Introduction to One-Way ANOVA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-2-sources-of-variability.html">12.2. Different Sources of Variability in an ANOVA Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-3-f-test-and-relationship-to-t-test.html">12.3. One-Way ANOVA F-Test and Its Relationship to Two-Sample t-Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-4-multiple-comparison-procedures-family-wise-error-rates.html">12.4. Multiple Comparison Procedures and Family-Wise Error Rates</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">13. Simple Linear Regression</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="13-1-intro-to-lr-correlation-scatter-plots.html">13.1. Introduction to Linear Regression: Correlation and Scatter Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="13-2-simple-linear-regression.html">13.2. Simple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="13-3-diagnostics-inference.html">13.3. Model Diagnostics and Statistical Inference</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">13.4. Prediction, Robustness, and Applied Examples</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">13. </span>Simple Linear Regression</a></li>
      <li class="breadcrumb-item active"><span class="section-number">13.4. </span>Prediction, Robustness, and Applied Examples</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/chapter13/lectures/13-4-prediction-robustness.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="video-placeholder" role="group" aria-labelledby="video-ch13-9">
   <iframe
      id="video-ch13-9"
      title="STAT 350 – Chapter 13.9 Prediction and Uncertainty - Confidence Intervals for the Mean Response at a Point/Prediction Intervals at a Point Video"
      src="https://www.youtube.com/embed/zUyxH0AL530?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
      allowfullscreen>
   </iframe>
</div><section id="prediction-robustness-and-applied-examples">
<h1><span class="section-number">13.4. </span>Prediction, Robustness, and Applied Examples<a class="headerlink" href="#prediction-robustness-and-applied-examples" title="Link to this heading"></a></h1>
<p>We have now developed the complete foundation for simple linear regression: model fitting, assumption checking,
and statistical inference for model parameters. This final chapter completes our regression toolkit by
addressing three critical questions that arise in practical applications: How do we make predictions with
appropriate uncertainty quantification? When can we trust our inference procedures despite violations of
the normality assumption? How do we apply these methods to solve real-world problems?</p>
<p>This chapter represents the culmination of our statistical journey through STAT 350, bringing together concepts
from descriptive statistics, probability, sampling distributions, and inference into a comprehensive framework
for understanding relationships between quantitative variables.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Problem we will solve</strong> – How to use fitted regression models for prediction with proper uncertainty quantification, understand when our inference procedures remain reliable under assumption violations, and apply the complete regression workflow to real-world problems from start to finish</p></li>
<li><p><strong>Tools we’ll learn</strong> – Confidence intervals for mean response at specific values, prediction intervals for individual observations, robustness analysis using Central Limit Theorem principles, and comprehensive applied analysis including R implementation</p></li>
<li><p><strong>How it fits</strong> – This completes our regression analysis framework and ties together all major statistical concepts from the entire course, demonstrating how descriptive statistics, probability models, sampling distributions, and statistical inference work together to solve practical problems</p></li>
</ul>
</div>
<section id="the-utility-of-regression-for-prediction">
<h2><span class="section-number">13.4.1. </span>The Utility of Regression for Prediction<a class="headerlink" href="#the-utility-of-regression-for-prediction" title="Link to this heading"></a></h2>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-utility.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-utility.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-utility.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.52 </span><span class="caption-text">Using the fitted regression line for prediction, showing the distinction between interpolation and extrapolation</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>One of the most important applications of linear regression is <strong>prediction</strong>—using our fitted model to estimate the response variable for new values of the explanatory variable. Given our “best fit” least squares regression line <span class="math notranslate nohighlight">\(\hat{y} = b_0 + b_1 x\)</span>, we can predict the response for any “reasonable” explanatory input value <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p><strong>The Fundamental Question</strong>: What constitutes a “reasonable” input value?</p>
<p>The answer lies in understanding the difference between <strong>interpolation</strong> and <strong>extrapolation</strong>:</p>
<section id="safe-vs-unsafe-predictions">
<h3>Safe vs Unsafe Predictions<a class="headerlink" href="#safe-vs-unsafe-predictions" title="Link to this heading"></a></h3>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-interpolation-extrapolation.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-interpolation-extrapolation.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-interpolation-extrapolation.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.53 </span><span class="caption-text">Visual distinction between safe interpolation region and dangerous extrapolation regions</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Interpolation</strong> involves making predictions for explanatory variable values that fall <strong>within the range</strong> of values used to create the regression line. These predictions are generally trustworthy because:</p>
<ul class="simple">
<li><p>Our model has been “trained” on data within this range</p></li>
<li><p>We have evidence that the linear relationship holds in this region</p></li>
<li><p>Our model assumptions have been validated using data from this range</p></li>
</ul>
<p><strong>Extrapolation</strong> involves using the regression line for prediction <strong>outside the range</strong> of observed explanatory variable values <span class="math notranslate nohighlight">\(\{x_1, x_2, \ldots, x_n\}\)</span>. This is risky because:</p>
<ul class="simple">
<li><p>We have no evidence that the linear relationship continues outside the observed range</p></li>
<li><p>The true relationship might be non-linear beyond our data range</p></li>
<li><p>Model assumptions may not hold in unobserved regions</p></li>
<li><p><strong>Critical principle</strong>: Extrapolation should be avoided whenever possible</p></li>
</ul>
<p><strong>Example Context</strong>: In our car efficiency example, we observed cylinder volumes from 1.5L to 2.5L. Predicting horsepower for a 2.0L engine (interpolation) is reasonable, but predicting for a 0.25L engine or 4.0L engine (extrapolation) would be unreliable and potentially meaningless.</p>
</section>
</section>
<section id="two-types-of-prediction-intervals">
<h2><span class="section-number">13.4.2. </span>Two Types of Prediction Intervals<a class="headerlink" href="#two-types-of-prediction-intervals" title="Link to this heading"></a></h2>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-types.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-types.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-types.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.54 </span><span class="caption-text">Conceptual difference between predicting mean response versus individual response values</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>When making predictions, we need to distinguish between two fundamentally different questions:</p>
<ol class="arabic simple">
<li><p><strong>What is the average response</strong> for all observations with explanatory value <span class="math notranslate nohighlight">\(x^*\)</span>?</p></li>
<li><p><strong>What will be the specific response</strong> for a single new observation with explanatory value <span class="math notranslate nohighlight">\(x^*\)</span>?</p></li>
</ol>
<p>These questions require different types of intervals with different interpretations and different levels of uncertainty.</p>
<p><strong>Confidence Intervals for Mean Response</strong></p>
<p>A <strong>confidence interval for the mean response</strong> at <span class="math notranslate nohighlight">\(x^*\)</span> provides an interval estimate for <span class="math notranslate nohighlight">\(\mu_{Y|X=x^*} = \beta_0 + \beta_1 x^*\)</span>—the true population mean of all response values when the explanatory variable equals <span class="math notranslate nohighlight">\(x^*\)</span>.</p>
<p><strong>Mathematical representation</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat{\mu}^* = b_0 + b_1 x^*\]</div>
<p>This estimates the <strong>average tendency</strong> or <strong>expected value</strong> of the response at the specified explanatory value.</p>
<p><strong>Prediction Intervals for Individual Response</strong></p>
<p>A <strong>prediction interval</strong> provides an interval estimate for a <strong>single new observation</strong> <span class="math notranslate nohighlight">\(Y^*\)</span> when <span class="math notranslate nohighlight">\(X = x^*\)</span>:</p>
<p><strong>Mathematical representation</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat{Y}^* = b_0 + b_1 x^* + \varepsilon^*\]</div>
<p>This accounts for both the uncertainty in estimating the mean response <strong>plus</strong> the additional variability of individual observations around that mean.</p>
<p><strong>Key Insight</strong>: Prediction intervals are <strong>always wider</strong> than confidence intervals for the mean response because they must account for additional sources of uncertainty.</p>
</section>
<section id="mathematical-development-of-mean-response-intervals">
<h2><span class="section-number">13.4.3. </span>Mathematical Development of Mean Response Intervals<a class="headerlink" href="#mathematical-development-of-mean-response-intervals" title="Link to this heading"></a></h2>
<p>To develop confidence intervals for the mean response, we must first express our estimate as a linear combination of the response values <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
<section id="rewriting-the-mean-response-estimate">
<h3>Rewriting the Mean Response Estimate<a class="headerlink" href="#rewriting-the-mean-response-estimate" title="Link to this heading"></a></h3>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-derivation.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-derivation.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-derivation.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.55 </span><span class="caption-text">Step-by-step algebraic derivation showing how the mean response estimate becomes a linear combination of response values</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Our estimate for the mean response at <span class="math notranslate nohighlight">\(x^*\)</span> is:</p>
<p>Substituting our formulas for the intercept and slope:</p>
<p>Now substituting the expression for <span class="math notranslate nohighlight">\(b_1\)</span>:</p>
<p>Combining terms:</p>
<p>This can be written as a single linear combination:</p>
<p><strong>Critical Insight</strong>: The mean response estimate is a <strong>weighted average</strong> of all the response values, where the weights depend on both the sample size and how far <span class="math notranslate nohighlight">\(x^*\)</span> is from <span class="math notranslate nohighlight">\(\bar{x}\)</span>.</p>
</section>
<section id="statistical-properties-of-the-mean-response-estimate">
<h3>Statistical Properties of the Mean Response Estimate<a class="headerlink" href="#statistical-properties-of-the-mean-response-estimate" title="Link to this heading"></a></h3>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-properties.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-properties.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-properties.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.56 </span><span class="caption-text">Expected value and variance calculations for the mean response estimate</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Since <span class="math notranslate nohighlight">\(\hat{\mu}^*\)</span> is a linear combination of normally distributed random variables <span class="math notranslate nohighlight">\(Y_i\)</span>, we can determine its statistical properties:</p>
<p><strong>Expected Value (Unbiasedness)</strong>:</p>
<p>This confirms that our estimate is unbiased for the true mean response.</p>
<p><strong>Variance Calculation</strong>:</p>
<p>Using the variance properties of linear combinations and the independence of the <span class="math notranslate nohighlight">\(Y_i\)</span> values:</p>
<p>After algebraic simplification (using the fact that <span class="math notranslate nohighlight">\(\text{Var}[Y_i] = \sigma^2\)</span> and the responses are independent):</p>
<p><strong>Distribution Under Normality</strong>:</p>
</section>
</section>
<section id="confidence-intervals-for-mean-response">
<h2><span class="section-number">13.4.4. </span>Confidence Intervals for Mean Response<a class="headerlink" href="#confidence-intervals-for-mean-response" title="Link to this heading"></a></h2>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-confidence-interval.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-confidence-interval.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-confidence-interval.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.57 </span><span class="caption-text">Complete confidence interval formula for mean response with all components labeled</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Since <span class="math notranslate nohighlight">\(\sigma^2\)</span> is unknown, we estimate it with <span class="math notranslate nohighlight">\(s^2 = \text{MSE}\)</span> and use the t-distribution.</p>
<p><strong>Standard Error of the Mean Response</strong>:</p>
<p><strong>Important Observation</strong>: The standard error increases as <span class="math notranslate nohighlight">\(x^*\)</span> moves further from <span class="math notranslate nohighlight">\(\bar{x}\)</span>. This means our predictions are most precise near the center of our data and become less precise toward the extremes.</p>
<p><strong>Confidence Interval Formula</strong>:</p>
<p><strong>Interpretation</strong>: We are <span class="math notranslate nohighlight">\((1-\alpha) \times 100\%\)</span> confident that the true mean response for all observations with explanatory variable value <span class="math notranslate nohighlight">\(x^*\)</span> lies within this interval.</p>
</section>
<section id="mathematical-development-of-prediction-intervals">
<h2><span class="section-number">13.4.5. </span>Mathematical Development of Prediction Intervals<a class="headerlink" href="#mathematical-development-of-prediction-intervals" title="Link to this heading"></a></h2>
<figure class="align-default" id="id7">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-interval-development.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-interval-development.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-interval-development.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.58 </span><span class="caption-text">Mathematical development showing why prediction intervals include additional uncertainty</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>For predicting an individual new response <span class="math notranslate nohighlight">\(Y^*\)</span> at <span class="math notranslate nohighlight">\(x^*\)</span>, we must account for <strong>two sources of variability</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Uncertainty in estimating the mean response</strong> (same as before)</p></li>
<li><p><strong>Natural variability of individual observations</strong> around the mean response</p></li>
</ol>
<p><strong>The Prediction Model</strong>:</p>
<p>where <span class="math notranslate nohighlight">\(\varepsilon^* \sim N(0, \sigma^2)\)</span> is the error term for the new observation, independent of the data used to fit the model.</p>
<p><strong>Expected Value</strong>:</p>
<p><strong>Variance Calculation</strong>:</p>
<p>Since the new error term <span class="math notranslate nohighlight">\(\varepsilon^*\)</span> is independent of the data used to fit the regression:</p>
<p><strong>Key Insight</strong>: The additional “<span class="math notranslate nohighlight">\(+1\)</span>” term represents the uncertainty from the new individual observation. This is why prediction intervals are always wider than confidence intervals for the mean response.</p>
<p><strong>Distribution</strong>:</p>
<section id="prediction-interval-formula">
<h3>Prediction Interval Formula<a class="headerlink" href="#prediction-interval-formula" title="Link to this heading"></a></h3>
<figure class="align-default" id="id8">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-interval-formula.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-interval-formula.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-interval-formula.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.59 </span><span class="caption-text">Complete prediction interval formula showing the additional uncertainty component</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Standard Error for Individual Prediction</strong>:</p>
<p><strong>Prediction Interval</strong>:</p>
<p><strong>Interpretation</strong>: We are <span class="math notranslate nohighlight">\((1-\alpha) \times 100\%\)</span> confident that a new individual response with explanatory variable value <span class="math notranslate nohighlight">\(x^*\)</span> will fall within this interval.</p>
<p><strong>Comparing the Formulas</strong>: The only difference between confidence intervals for mean response and prediction intervals is the additional “<span class="math notranslate nohighlight">\(+1\)</span>” inside the square root for prediction intervals, but this makes a substantial practical difference in interval width.</p>
</section>
</section>
<section id="confidence-and-prediction-bands">
<h2><span class="section-number">13.4.6. </span>Confidence and Prediction Bands<a class="headerlink" href="#confidence-and-prediction-bands" title="Link to this heading"></a></h2>
<figure class="align-default" id="id9">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-bands.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-bands.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-bands.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.60 </span><span class="caption-text">Definition and construction of confidence bands across the range of explanatory variable values</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Confidence Bands</strong></p>
<p>A <strong>confidence band</strong> is constructed by computing confidence intervals for the mean response at many different values of the explanatory variable and connecting these intervals to form smooth curves.</p>
<p>The confidence band provides a visual representation of the range of plausible values for the <strong>true mean response line</strong> <span class="math notranslate nohighlight">\(\mu_{Y|X=x} = \beta_0 + \beta_1 x\)</span> across the entire range of explanatory variable values.</p>
<p><strong>Key Properties</strong>:</p>
<ul class="simple">
<li><p>Narrowest at <span class="math notranslate nohighlight">\(x = \bar{x}\)</span> (center of the data)</p></li>
<li><p>Widens as we move toward the extremes of the explanatory variable range</p></li>
<li><p>Provides uncertainty quantification for the fitted line itself</p></li>
</ul>
<p><strong>Prediction Bands</strong></p>
<figure class="align-default" id="id10">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-bands.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-bands.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-prediction-bands.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.61 </span><span class="caption-text">Definition and construction of prediction bands showing wider intervals for individual predictions</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>A <strong>prediction band</strong> is constructed similarly but uses prediction intervals at each explanatory variable value. This band is inherently wider than the confidence band because it accounts for both:</p>
<ol class="arabic simple">
<li><p><strong>Uncertainty in estimating the regression coefficients</strong></p></li>
<li><p><strong>Additional uncertainty in predicting a new response</strong> based on the estimated regression line</p></li>
</ol>
<p><strong>Visual Relationship</strong>: The prediction band always contains the confidence band, with the confidence band representing uncertainty about the mean response and the prediction band representing uncertainty about individual responses.</p>
<section id="important-considerations-for-multiple-predictions">
<h3>Important Considerations for Multiple Predictions<a class="headerlink" href="#important-considerations-for-multiple-predictions" title="Link to this heading"></a></h3>
<figure class="align-default" id="id11">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-multiple-testing-correction.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-multiple-testing-correction.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-multiple-testing-correction.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.62 </span><span class="caption-text">Warning about multiple comparisons when making simultaneous predictions</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Multiple Testing Issue</strong>: If we construct many confidence or prediction intervals simultaneously, the overall Type I error rate could become large.</p>
<p>For example, if we construct 20 individual 95% confidence intervals, the probability that <strong>at least one</strong> interval fails to contain its true parameter could be much higher than 5%.</p>
<p><strong>Solutions</strong>:
- Use more conservative confidence levels (e.g., 99% instead of 95%)
- Apply multiple comparison corrections (beyond this course’s scope)
- Understand that individual intervals have the stated coverage probability, but simultaneous coverage is lower</p>
<p><strong>Practical Advice</strong>: When making multiple predictions, acknowledge this limitation and consider the purpose of the analysis when choosing confidence levels.</p>
</section>
</section>
<section id="visualization-of-confidence-and-prediction-bands">
<h2><span class="section-number">13.4.7. </span>Visualization of Confidence and Prediction Bands<a class="headerlink" href="#visualization-of-confidence-and-prediction-bands" title="Link to this heading"></a></h2>
<figure class="align-default" id="id12">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-prediction-visualization.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-prediction-visualization.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-confidence-prediction-visualization.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.63 </span><span class="caption-text">Comprehensive visualization showing both confidence and prediction bands with data points</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The visual representation of confidence and prediction bands reveals several important features:</p>
<p><strong>Band Shape and Width</strong>:</p>
<ul class="simple">
<li><p>Both bands have a “bow-tie” or curved shape, narrowest at <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span></p></li>
<li><p>Width increases as we move away from the center of the data</p></li>
<li><p>Prediction bands are uniformly wider than confidence bands</p></li>
</ul>
<p><strong>Center Point Special Property</strong>:</p>
<ul class="simple">
<li><p>At <span class="math notranslate nohighlight">\(x = \bar{x}\)</span>, the standard error formulas simplify significantly</p></li>
<li><p>For mean response: <span class="math notranslate nohighlight">\(SE_{\hat{\mu}^*} = \sqrt{\text{MSE}/n}\)</span></p></li>
<li><p>For individual prediction: <span class="math notranslate nohighlight">\(SE_{\hat{Y}^*} = \sqrt{\text{MSE}(1 + 1/n)}\)</span></p></li>
</ul>
<p><strong>Practical Interpretation</strong>:</p>
<ul class="simple">
<li><p>Points falling outside the prediction band suggest potential outliers or model inadequacy</p></li>
<li><p>The confidence band shows where we expect the true regression line to lie</p></li>
<li><p>The prediction band shows where we expect new individual observations to fall</p></li>
</ul>
</section>
<section id="robustness-to-normality-assumptions">
<h2><span class="section-number">13.4.8. </span>Robustness to Normality Assumptions<a class="headerlink" href="#robustness-to-normality-assumptions" title="Link to this heading"></a></h2>
<div class="video-placeholder" role="group" aria-labelledby="video-ch13-10">
   <iframe
      id="video-ch13-10"
      title="STAT 350 – Chapter 13.10 Robustness to Normality Assumptions Video"
      src="https://www.youtube.com/embed/J8NtyRd48QU?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
      allowfullscreen>
   </iframe>
</div><p>A critical practical question is: What happens to our statistical inference procedures when the normality assumption is violated? This mirrors our earlier discussions about robustness in single-sample and two-sample procedures, but regression presents some unique considerations.</p>
<section id="the-central-limit-theorem-in-regression-context">
<h3>The Central Limit Theorem in Regression Context<a class="headerlink" href="#the-central-limit-theorem-in-regression-context" title="Link to this heading"></a></h3>
<figure class="align-default" id="id13">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-clt-regression-overview.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-clt-regression-overview.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-clt-regression-overview.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.64 </span><span class="caption-text">Overview of how Central Limit Theorem applies to different regression inference procedures</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The key insight is that different regression procedures have different levels of robustness to normality violations, depending on whether they rely solely on <strong>averaged quantities</strong> or also involve <strong>individual new observations</strong>.</p>
<p><strong>When CLT Provides Protection</strong>:</p>
<p>All of our regression parameter estimates (<span class="math notranslate nohighlight">\(b_0\)</span>, <span class="math notranslate nohighlight">\(b_1\)</span>) and mean response predictions (<span class="math notranslate nohighlight">\(\hat{\mu}^*\)</span>) are <strong>weighted averages</strong> of the response values <span class="math notranslate nohighlight">\(Y_i\)</span>. Even if the individual error terms <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> are not normally distributed, these weighted averages will approach normality as the sample size increases, provided the error distribution is not too far from normal.</p>
<p><strong>When CLT Cannot Help</strong>:</p>
<p>Prediction intervals for <strong>individual new observations</strong> involve a <strong>new error term</strong> <span class="math notranslate nohighlight">\(\varepsilon^*\)</span> that is not averaged with anything. No amount of sample size increase can make this individual error term normal if the underlying error distribution is non-normal.</p>
</section>
<section id="robustness-analysis-for-each-procedure">
<h3>Robustness Analysis for Each Procedure<a class="headerlink" href="#robustness-analysis-for-each-procedure" title="Link to this heading"></a></h3>
<figure class="align-default" id="id14">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-robustness-analysis.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-robustness-analysis.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-robustness-analysis.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.65 </span><span class="caption-text">Detailed analysis of which regression procedures are robust to normality violations</span><a class="headerlink" href="#id14" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Parameter Estimation (Robust)</strong></p>
<p><strong>Slope and Intercept Estimates</strong>:</p>
<p>Both estimates are weighted averages of the <span class="math notranslate nohighlight">\(Y_i\)</span> values. <strong>Central Limit Theorem applies</strong>: With sufficiently large sample sizes, these estimates will be approximately normally distributed even if the error terms are not exactly normal.</p>
<p><strong>Practical Implication</strong>: Hypothesis tests and confidence intervals for <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> remain approximately valid with moderate departures from normality, especially when <span class="math notranslate nohighlight">\(n\)</span> is large (generally <span class="math notranslate nohighlight">\(n \geq 30\)</span>).</p>
<p><strong>Mean Response Prediction (Robust)</strong></p>
<figure class="align-default" id="id15">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-robustness.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-robustness.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-mean-response-robustness.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.66 </span><span class="caption-text">Explanation of why mean response predictions benefit from Central Limit Theorem</span><a class="headerlink" href="#id15" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>This is also a weighted average of the <span class="math notranslate nohighlight">\(Y_i\)</span> values. <strong>Central Limit Theorem applies</strong>: Confidence intervals for the mean response will have approximately correct coverage rates even with moderate normality violations, provided the sample size is adequate.</p>
<p><strong>Individual Response Prediction (NOT Robust)</strong></p>
<figure class="align-default" id="id16">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-individual-prediction-not-robust.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-individual-prediction-not-robust.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-individual-prediction-not-robust.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.67 </span><span class="caption-text">Explanation of why individual predictions cannot rely on Central Limit Theorem</span><a class="headerlink" href="#id16" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>While <span class="math notranslate nohighlight">\(\hat{\mu}^*\)</span> benefits from CLT, the additional error term <span class="math notranslate nohighlight">\(\varepsilon^*\)</span> does not. This new error term represents a <strong>single draw</strong> from the error distribution, not an average.</p>
<p><strong>Critical Limitation</strong>: If the error terms are not normally distributed, prediction intervals may have incorrect coverage rates. The intervals might be too wide, too narrow, or asymmetric, depending on the true error distribution.</p>
<p><strong>Practical Implications for Real Data Analysis</strong></p>
<p><strong>What This Means in Practice</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Parameter inference is robust</strong>: With reasonable sample sizes (<span class="math notranslate nohighlight">\(n \geq 20-30\)</span>), slight to moderate violations of normality don’t invalidate hypothesis tests or confidence intervals for slopes and intercepts.</p></li>
<li><p><strong>Mean response intervals are robust</strong>: Confidence intervals for mean response maintain approximately correct coverage rates under mild normality violations with adequate sample sizes.</p></li>
<li><p><strong>Individual prediction intervals require caution</strong>: These are the most sensitive to normality violations. If diagnostic plots suggest non-normal errors, prediction intervals should be interpreted carefully.</p></li>
</ol>
<p><strong>Guidelines for Practice</strong>:</p>
<ul class="simple">
<li><p><strong>Always check normality assumptions</strong> using residual diagnostics</p></li>
<li><p><strong>For small samples</strong> (<span class="math notranslate nohighlight">\(n &lt; 20\)</span>), normality is more critical for all procedures</p></li>
<li><p><strong>For prediction intervals specifically</strong>, normality violations are particularly problematic</p></li>
<li><p><strong>Consider transformations</strong> if normality violations are severe</p></li>
<li><p><strong>Acknowledge limitations</strong> when reporting results with questionable normality</p></li>
</ul>
<p><strong>Sample Size Considerations</strong>:</p>
<p>The robustness provided by CLT depends on both sample size and the degree of non-normality:</p>
<ul class="simple">
<li><p><strong>Mild violations</strong> (slight skewness, light tails): <span class="math notranslate nohighlight">\(n \geq 20\)</span> often sufficient</p></li>
<li><p><strong>Moderate violations</strong> (noticeable skewness, outliers): <span class="math notranslate nohighlight">\(n \geq 30-50\)</span> may be needed</p></li>
<li><p><strong>Severe violations</strong> (heavy skewness, extreme outliers): Transformations or non-parametric methods may be necessary</p></li>
</ul>
</section>
</section>
<section id="complete-applied-example-cetane-number-analysis">
<h2><span class="section-number">13.4.9. </span>Complete Applied Example: Cetane Number Analysis<a class="headerlink" href="#complete-applied-example-cetane-number-analysis" title="Link to this heading"></a></h2>
<div class="video-placeholder" role="group" aria-labelledby="video-ch13-11">
   <iframe
      id="video-ch13-11"
      title="STAT 350 – Chapter 13.11 Linear Regression Prediction Example - Cetane Number Video"
      src="https://www.youtube.com/embed/XiQd9bhOSl4?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
      allowfullscreen>
   </iframe>
</div><p>To demonstrate the complete regression analysis workflow, we’ll work through a comprehensive real-world example that integrates all the concepts we’ve developed throughout this chapter and the entire course.</p>
<section id="research-context-and-problem-statement">
<h3>Research Context and Problem Statement<a class="headerlink" href="#research-context-and-problem-statement" title="Link to this heading"></a></h3>
<figure class="align-default" id="id17">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-cetane-number-context.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-cetane-number-context.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-cetane-number-context.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.68 </span><span class="caption-text">Background information on cetane number as a critical property for diesel fuel quality</span><a class="headerlink" href="#id17" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Research Problem</strong>: The <strong>cetane number</strong> is a critical property that specifies the ignition quality of fuel used in diesel engines. Determination of this number for biodiesel fuel is expensive and time-consuming. Researchers want to explore using a simple linear regression model to predict cetane number from the <strong>iodine value</strong>.</p>
<p><strong>Variables</strong>:</p>
<ul class="simple">
<li><p><strong>Response (Y)</strong>: Cetane Number (CN) - measures ignition quality</p></li>
<li><p><strong>Explanatory (X)</strong>: Iodine Value (IV) - the amount of iodine necessary to saturate a sample of 100 grams of oil</p></li>
</ul>
<p><strong>Study Design</strong>: A sample of 14 different biodiesel fuels was collected, with both iodine value and cetane number measured for each fuel.</p>
<p><strong>Research Question</strong>: Can iodine value be used to predict cetane number through a simple linear relationship?</p>
</section>
<section id="the-complete-dataset">
<h3>The Complete Dataset<a class="headerlink" href="#the-complete-dataset" title="Link to this heading"></a></h3>
<figure class="align-default" id="id18">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-cetane-data-table.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-cetane-data-table.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-cetane-data-table.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.69 </span><span class="caption-text">Complete dataset showing iodine values and corresponding cetane numbers for 14 biodiesel fuels</span><a class="headerlink" href="#id18" title="Link to this image"></a></p>
</figcaption>
</figure>
<table class="docutils align-default" id="id19">
<caption><span class="caption-number">Table 13.3 </span><span class="caption-text">Biodiesel Fuel Quality Data</span><a class="headerlink" href="#id19" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Obs</p></th>
<th class="head"><p>Iodine Value (IV)</p></th>
<th class="head"><p>Cetane Number (CN)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>132.0</p></td>
<td><p>46.0</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>129.0</p></td>
<td><p>48.0</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>120.0</p></td>
<td><p>51.0</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>113.2</p></td>
<td><p>52.1</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>105.0</p></td>
<td><p>54.0</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>92.0</p></td>
<td><p>52.0</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>84.0</p></td>
<td><p>59.0</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>83.2</p></td>
<td><p>58.7</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p>88.4</p></td>
<td><p>61.6</p></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><p>59.0</p></td>
<td><p>64.0</p></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><p>80.0</p></td>
<td><p>61.4</p></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><p>81.5</p></td>
<td><p>54.6</p></td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><p>71.0</p></td>
<td><p>58.8</p></td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><p>69.2</p></td>
<td><p>58.0</p></td>
</tr>
</tbody>
</table>
<p><strong>Initial Observations</strong>:</p>
<ul class="simple">
<li><p>Iodine values range from approximately 59 to 132</p></li>
<li><p>Cetane numbers range from approximately 46 to 64</p></li>
<li><p>There appears to be a negative relationship (as IV increases, CN tends to decrease)</p></li>
</ul>
</section>
<section id="step-1-exploratory-data-analysis-and-model-fitting">
<h3>Step 1: Exploratory Data Analysis and Model Fitting<a class="headerlink" href="#step-1-exploratory-data-analysis-and-model-fitting" title="Link to this heading"></a></h3>
<p><strong>R Implementation for Data Setup and Initial Analysis</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the dataset</span>
<span class="n">iodine_value</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">132.0</span><span class="p">,</span><span class="w"> </span><span class="m">129.0</span><span class="p">,</span><span class="w"> </span><span class="m">120.0</span><span class="p">,</span><span class="w"> </span><span class="m">113.2</span><span class="p">,</span><span class="w"> </span><span class="m">105.0</span><span class="p">,</span><span class="w"> </span><span class="m">92.0</span><span class="p">,</span><span class="w"> </span><span class="m">84.0</span><span class="p">,</span>
<span class="w">                  </span><span class="m">83.2</span><span class="p">,</span><span class="w"> </span><span class="m">88.4</span><span class="p">,</span><span class="w"> </span><span class="m">59.0</span><span class="p">,</span><span class="w"> </span><span class="m">80.0</span><span class="p">,</span><span class="w"> </span><span class="m">81.5</span><span class="p">,</span><span class="w"> </span><span class="m">71.0</span><span class="p">,</span><span class="w"> </span><span class="m">69.2</span><span class="p">)</span>
<span class="n">cetane_number</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">46.0</span><span class="p">,</span><span class="w"> </span><span class="m">48.0</span><span class="p">,</span><span class="w"> </span><span class="m">51.0</span><span class="p">,</span><span class="w"> </span><span class="m">52.1</span><span class="p">,</span><span class="w"> </span><span class="m">54.0</span><span class="p">,</span><span class="w"> </span><span class="m">52.0</span><span class="p">,</span><span class="w"> </span><span class="m">59.0</span><span class="p">,</span>
<span class="w">                   </span><span class="m">58.7</span><span class="p">,</span><span class="w"> </span><span class="m">61.6</span><span class="p">,</span><span class="w"> </span><span class="m">64.0</span><span class="p">,</span><span class="w"> </span><span class="m">61.4</span><span class="p">,</span><span class="w"> </span><span class="m">54.6</span><span class="p">,</span><span class="w"> </span><span class="m">58.8</span><span class="p">,</span><span class="w"> </span><span class="m">58.0</span><span class="p">)</span>

<span class="c1"># Create data frame</span>
<span class="n">cetane_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span>
<span class="w">  </span><span class="n">IodineValue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iodine_value</span><span class="p">,</span>
<span class="w">  </span><span class="n">CetaneNumber</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cetane_number</span>
<span class="p">)</span>

<span class="c1"># Initial scatter plot</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">cetane_data</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">IodineValue</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CetaneNumber</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;blue&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span>
<span class="w">    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Cetane Number vs Iodine Value&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Iodine Value (IV)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Cetane Number (CN)&quot;</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme_minimal</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Scatter Plot Assessment</strong>:</p>
<ul class="simple">
<li><p>Clear negative linear trend visible</p></li>
<li><p>Points roughly follow a straight line pattern</p></li>
<li><p>No obvious curvature or non-linear patterns</p></li>
<li><p>Constant variance appears reasonable (though limited by small sample size)</p></li>
<li><p>No extreme outliers apparent</p></li>
</ul>
<p><strong>Model Fitting</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the linear regression model</span>
<span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">CetaneNumber</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">IodineValue</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cetane_data</span><span class="p">)</span>

<span class="c1"># Get basic summary</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>

<span class="c1"># Extract coefficients</span>
<span class="n">b0</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">fit`coefficients[&#39;(Intercept)&#39;]</span>
<span class="n">b1 &lt;- fit`coefficients</span><span class="p">[</span><span class="s">&#39;IodineValue&#39;</span><span class="p">]</span>

<span class="c1"># Display fitted equation</span>
<span class="nf">cat</span><span class="p">(</span><span class="s">&quot;Fitted equation: CN =&quot;</span><span class="p">,</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="n">b0</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">),</span><span class="w"> </span><span class="s">&quot;+&quot;</span><span class="p">,</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="n">b1</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">),</span><span class="w"> </span><span class="s">&quot;* IV&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Fitted Model Results</strong>:</p>
<p><strong>Interpretation</strong>:</p>
<ul class="simple">
<li><p><strong>Intercept</strong> (75.212): Predicted cetane number when iodine value is 0 (not practically meaningful since IV = 0 is outside our data range)</p></li>
<li><p><strong>Slope</strong> (-0.2094): For each unit increase in iodine value, the cetane number decreases by an average of 0.2094 units</p></li>
</ul>
</section>
<section id="step-2-comprehensive-assumption-checking">
<h3>Step 2: Comprehensive Assumption Checking<a class="headerlink" href="#step-2-comprehensive-assumption-checking" title="Link to this heading"></a></h3>
<p>Before proceeding with inference, we must verify that our model assumptions are reasonable.</p>
<p><strong>Residual Analysis</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate residuals and fitted values</span>
<span class="n">cetane_data`residuals &lt;- residuals(fit)</span>
<span class="n">cetane_data`fitted</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">fitted</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>

<span class="c1"># Residual plot</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">cetane_data</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">IodineValue</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">residuals</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_hline</span><span class="p">(</span><span class="n">yintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;dashed&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span>
<span class="w">    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Residual Plot&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Iodine Value&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Residuals&quot;</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme_minimal</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Diagnostic Assessment</strong>:</p>
<figure class="align-default" id="id20">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-cetane-diagnostics.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-cetane-diagnostics.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-cetane-diagnostics.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.70 </span><span class="caption-text">Complete diagnostic plots for the cetane number analysis showing all four assumption checks</span><a class="headerlink" href="#id20" title="Link to this image"></a></p>
</figcaption>
</figure>
<p><strong>Linearity</strong>: The residual plot shows no obvious patterns or curvature, supporting the linearity assumption.</p>
<p><strong>Constant Variance</strong>: There appear to be some minor violations of the constant variance assumption. The residual plot shows some regions where points cluster more tightly than others, but with only 14 observations, it’s difficult to definitively assess this assumption. The violations don’t appear severe enough to invalidate the analysis.</p>
<p><strong>Normality Assessment</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Histogram of residuals</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">cetane_data</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">residuals</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_histogram</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">after_stat</span><span class="p">(</span><span class="n">density</span><span class="p">)),</span><span class="w"> </span><span class="n">bins</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span>
<span class="w">                 </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lightblue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;black&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_density</span><span class="p">(</span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">stat_function</span><span class="p">(</span><span class="n">fun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dnorm</span><span class="p">,</span>
<span class="w">                </span><span class="n">args</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">cetane_data`residuals),</span>
<span class="n">                           sd = sd(cetane_data`residuals</span><span class="p">)),</span>
<span class="w">                </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Histogram of Residuals with Normal Overlay&quot;</span><span class="p">)</span>

<span class="c1"># QQ plot</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">cetane_data</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span><span class="n">sample</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">residuals</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">stat_qq</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">stat_qq_line</span><span class="p">()</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;QQ Plot of Residuals&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Normality Conclusion</strong>: With only 14 observations, assessing normality is challenging. The histogram shows roughly symmetric distribution with no extreme outliers. The QQ plot shows some fluctuation but no systematic departures from linearity. The normality assumption appears reasonable, though not definitively satisfied.</p>
<p><strong>Independence</strong>: This assumption must be evaluated based on data collection procedures. Since these are measurements on different biodiesel fuels, independence appears reasonable.</p>
<p><strong>Overall Assessment</strong>: The model assumptions appear adequately satisfied for proceeding with inference, though we should acknowledge some uncertainty due to the small sample size.</p>
</section>
<section id="step-3-statistical-inference">
<h3>Step 3: Statistical Inference<a class="headerlink" href="#step-3-statistical-inference" title="Link to this heading"></a></h3>
<p><strong>Model Summary Output</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Key Results from R Output</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R^2 = 0.7906\)</span> (approximately 79% of variation explained)</p></li>
<li><p><strong>F-statistic</strong>: 45.35 on 1 and 12 DF, <strong>p-value</strong>: 2.091e-05</p></li>
<li><p><strong>Slope estimate</strong>: -0.20939, <strong>Standard error</strong>: 0.03109, <strong>t-value</strong>: -6.734, <strong>p-value</strong>: 2.09e-05</p></li>
</ul>
<p><strong>F-test for Model Utility</strong>:</p>
<p><strong>Hypotheses</strong>:
- <span class="math notranslate nohighlight">\(H_0\)</span>: There is no linear association between iodine value and cetane number
- <span class="math notranslate nohighlight">\(H_a\)</span>: There is a linear association between iodine value and cetane number</p>
<p><strong>Conclusion</strong>: With p-value &lt; 0.001, we reject <span class="math notranslate nohighlight">\(H_0\)</span> at any reasonable significance level. There is strong evidence of a linear association between iodine value and cetane number.</p>
<p><strong>Slope Inference</strong>:</p>
<p><strong>95% Confidence Interval for Slope</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate confidence interval for slope</span>
<span class="nf">confint</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;IodineValue&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Result</strong>: (-0.277, -0.142)</p>
<p><strong>Interpretation</strong>: We are 95% confident that each unit increase in iodine value is associated with a decrease in cetane number between 0.142 and 0.277 units.</p>
</section>
<section id="step-4-prediction-applications">
<h3>Step 4: Prediction Applications<a class="headerlink" href="#step-4-prediction-applications" title="Link to this heading"></a></h3>
<p><strong>Example Prediction</strong>: Predict the cetane number for a biodiesel fuel with iodine value of 75.</p>
<p><strong>Confidence Interval for Mean Response</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create new data for prediction</span>
<span class="n">new_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">IodineValue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">75</span><span class="p">)</span>

<span class="c1"># Confidence interval for mean response</span>
<span class="n">conf_interval</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_data</span><span class="p">,</span>
<span class="w">                        </span><span class="n">interval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;confidence&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.99</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">conf_interval</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Results</strong>:</p>
<ul class="simple">
<li><p><strong>Predicted mean cetane number</strong>: 59.51</p></li>
<li><p><strong>99% Confidence interval</strong>: (57.74, 61.28)</p></li>
</ul>
<p><strong>Interpretation</strong>: We are 99% confident that the average cetane number for all biodiesel fuels with iodine value 75 is between 57.74 and 61.28.</p>
<p><strong>Prediction Interval for Individual Response</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prediction interval for individual response</span>
<span class="n">pred_interval</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_data</span><span class="p">,</span>
<span class="w">                        </span><span class="n">interval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;prediction&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.99</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">pred_interval</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Results</strong>:</p>
<ul class="simple">
<li><p><strong>Predicted individual cetane number</strong>: 59.51 (same point prediction)</p></li>
<li><p><strong>99% Prediction interval</strong>: (54.19, 64.83)</p></li>
</ul>
<p><strong>Interpretation</strong>: We are 99% confident that a single new biodiesel fuel with iodine value 75 will have a cetane number between 54.19 and 64.83.</p>
<p><strong>Key Observation</strong>: The prediction interval (54.19, 64.83) is substantially wider than the confidence interval (57.74, 61.28), reflecting the additional uncertainty in predicting individual observations.</p>
</section>
<section id="step-5-comprehensive-visualization">
<h3>Step 5: Comprehensive Visualization<a class="headerlink" href="#step-5-comprehensive-visualization" title="Link to this heading"></a></h3>
<p><strong>Creating Confidence and Prediction Bands</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate confidence and prediction bands</span>
<span class="n">conf_band</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">interval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;confidence&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.99</span><span class="p">)</span>
<span class="n">pred_band</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">interval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;prediction&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.99</span><span class="p">)</span>

<span class="c1"># Comprehensive visualization</span>
<span class="nf">ggplot</span><span class="p">(</span><span class="n">cetane_data</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">IodineValue</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CetaneNumber</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="c1"># Prediction bands (outer)</span>
<span class="w">  </span><span class="nf">geom_ribbon</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">ymin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pred_band</span><span class="p">[,</span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="n">ymax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pred_band</span><span class="p">[,</span><span class="m">3</span><span class="p">]),</span>
<span class="w">              </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lightblue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.3</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="c1"># Confidence bands (inner)</span>
<span class="w">  </span><span class="nf">geom_ribbon</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">ymin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">conf_band</span><span class="p">[,</span><span class="m">2</span><span class="p">],</span><span class="w"> </span><span class="n">ymax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">conf_band</span><span class="p">[,</span><span class="m">3</span><span class="p">]),</span>
<span class="w">              </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;darkblue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="c1"># Data points</span>
<span class="w">  </span><span class="nf">geom_point</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;black&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="c1"># Regression line</span>
<span class="w">  </span><span class="nf">geom_smooth</span><span class="p">(</span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;lm&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;black&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span>
<span class="w">    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Cetane Number Prediction Model&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">subtitle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Dark blue: 99% Confidence bands, Light blue: 99% Prediction bands&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Iodine Value&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Cetane Number&quot;</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme_minimal</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Visual Insights</strong>:</p>
<ul class="simple">
<li><p>The confidence bands show our uncertainty about the true mean response line</p></li>
<li><p>The prediction bands show the range where we expect individual new observations</p></li>
<li><p>Both bands are narrowest in the center of the data (around IV ≈ 90) and widen toward the extremes</p></li>
<li><p>The substantial difference between confidence and prediction band widths illustrates the additional uncertainty in individual predictions</p></li>
</ul>
</section>
<section id="step-6-practical-conclusions-and-limitations">
<h3>Step 6: Practical Conclusions and Limitations<a class="headerlink" href="#step-6-practical-conclusions-and-limitations" title="Link to this heading"></a></h3>
<p><strong>Research Conclusions</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Strong Linear Relationship</strong>: There is compelling evidence (p &lt; 0.001) of a negative linear association between iodine value and cetane number in biodiesel fuels.</p></li>
<li><p><strong>Predictive Utility</strong>: The model explains approximately 79% of the variation in cetane number, suggesting that iodine value is a useful predictor for this important fuel quality measure.</p></li>
<li><p><strong>Practical Significance</strong>: Each unit increase in iodine value corresponds to an estimated decrease of about 0.21 units in cetane number, which may be practically significant for fuel quality assessment.</p></li>
<li><p><strong>Prediction Capability</strong>: The model can provide reasonable predictions for cetane number within the observed range of iodine values (approximately 59 to 132).</p></li>
</ol>
<p><strong>Important Limitations</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Sample Size</strong>: With only 14 observations, our conclusions should be considered preliminary. Larger studies would provide more definitive results.</p></li>
<li><p><strong>Assumption Concerns</strong>: Some minor violations of the constant variance assumption were noted, which could affect the reliability of prediction intervals.</p></li>
<li><p><strong>Scope of Inference</strong>: Predictions should only be made within the range of observed iodine values (59-132). Extrapolation beyond this range is not justified.</p></li>
<li><p><strong>Causation vs. Association</strong>: This observational study can only establish association, not causation. The relationship between iodine value and cetane number likely reflects underlying chemical processes but doesn’t prove that iodine value directly causes changes in cetane number.</p></li>
<li><p><strong>Model Complexity</strong>: This simple linear model may not capture all aspects of the relationship between these variables. More complex models with additional variables might provide better predictions.</p></li>
</ol>
<p><strong>Recommendations for Future Research</strong>:</p>
<ul class="simple">
<li><p>Collect larger sample sizes to improve precision and power</p></li>
<li><p>Investigate potential additional predictor variables</p></li>
<li><p>Validate the model on independent datasets</p></li>
<li><p>Consider non-linear relationships if supported by theory</p></li>
</ul>
</section>
</section>
<section id="bringing-it-all-together-the-statistical-journey">
<h2><span class="section-number">13.4.10. </span>Bringing It All Together: The Statistical Journey<a class="headerlink" href="#bringing-it-all-together-the-statistical-journey" title="Link to this heading"></a></h2>
<p>As we conclude Chapter 13 and STAT 350, it’s important to reflect on the remarkable statistical
journey we’ve taken and how all the pieces fit together into a coherent framework for understanding
and analyzing data.</p>
<section id="the-evolution-of-statistical-thinking">
<h3>The Evolution of Statistical Thinking<a class="headerlink" href="#the-evolution-of-statistical-thinking" title="Link to this heading"></a></h3>
<p><strong>From Description to Inference</strong>: Our journey began with <strong>descriptive statistics</strong> (Chapters 1-3), where we learned to summarize and visualize data using measures of center, spread, and graphical displays. This provided the foundation for understanding what our data tells us directly.</p>
<p>We then moved to <strong>probability models</strong> (Chapters 4-6), developing the theoretical framework for understanding uncertainty and variability. These probability concepts provided the mathematical foundation for moving beyond our observed data to make statements about larger populations.</p>
<p><strong>Sampling distributions</strong> (Chapter 7) formed the crucial bridge between probability theory and statistical inference, showing us how sample statistics behave when we repeatedly sample from populations. This led to the <strong>Central Limit Theorem</strong>, which became our gateway to reliable inference procedures.</p>
<p><strong>Study design</strong> (Chapter 8) reminded us that statistical methods are only as good as the data they analyze, emphasizing the critical importance of proper data collection procedures.</p>
<p><strong>Statistical inference</strong> (Chapters 9-13) provided the tools to draw conclusions about populations based on sample data, with appropriate quantification of uncertainty. We progressed systematically through increasingly complex scenarios:</p>
<ul class="simple">
<li><p><strong>Single populations</strong> (Chapters 9-10): Confidence intervals and hypothesis tests for population means</p></li>
<li><p><strong>Two populations</strong> (Chapter 11): Comparing means between groups using independent and paired procedures</p></li>
<li><p><strong>Multiple populations</strong> (Chapter 12): ANOVA for comparing several groups simultaneously</p></li>
<li><p><strong>Quantitative relationships</strong> (Chapter 13): Regression for studying associations between quantitative variables</p></li>
</ul>
</section>
<section id="the-unifying-themes">
<h3>The Unifying Themes<a class="headerlink" href="#the-unifying-themes" title="Link to this heading"></a></h3>
<p>Throughout this progression, several unifying themes have emerged:</p>
<p><strong>Parameter Estimation</strong>: In every context, we’ve estimated unknown population parameters using sample statistics, always acknowledging the uncertainty inherent in this process.</p>
<p><strong>Standard Errors</strong>: We’ve consistently used standard errors to quantify the precision of our estimates, with the specific formula depending on the parameter and sampling situation.</p>
<p><strong>Confidence Intervals</strong>: The general form <span class="math notranslate nohighlight">\(\text{Estimate} \pm \text{Critical Value} \times \text{Standard Error}\)</span> has appeared in every inference context, providing interval estimates for parameters.</p>
<p><strong>Hypothesis Testing</strong>: The four-step process (parameter, hypotheses, test statistic and p-value, conclusion) has provided a systematic framework for testing specific claims about parameters.</p>
<p><strong>Model Assumptions</strong>: Every procedure has required assumptions about the data generating process, and we’ve learned to check these assumptions and understand the consequences of violations.</p>
<p><strong>The t-Distribution</strong>: This distribution has been our constant companion, appearing whenever we estimate population standard deviations from sample data.</p>
</section>
<section id="linear-regression-as-the-culmination">
<h3>Linear Regression as the Culmination<a class="headerlink" href="#linear-regression-as-the-culmination" title="Link to this heading"></a></h3>
<p>Linear regression represents the culmination of our statistical education because it integrates virtually every concept we’ve studied:</p>
<p><strong>Descriptive Statistics</strong>: Scatter plots, correlation coefficients, and summary measures help us understand bivariate relationships.</p>
<p><strong>Probability Models</strong>: The regression model <span class="math notranslate nohighlight">\(Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i\)</span> with <span class="math notranslate nohighlight">\(\varepsilon_i \sim N(0, \sigma^2)\)</span> directly applies probability concepts to real relationships.</p>
<p><strong>Parameter Estimation</strong>: Least squares methods provide optimal estimates for <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>, with well-understood statistical properties.</p>
<p><strong>Sampling Distributions</strong>: Our estimates <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> follow known distributions that enable inference procedures.</p>
<p><strong>Confidence Intervals</strong>: We construct intervals for slopes, intercepts, mean responses, and individual predictions using our familiar framework.</p>
<p><strong>Hypothesis Testing</strong>: F-tests and t-tests allow us to test specific hypotheses about relationships and parameters.</p>
<p><strong>Assumption Checking</strong>: Diagnostic procedures ensure our inferences are valid, connecting back to our understanding of probability distributions and model requirements.</p>
<p><strong>Practical Application</strong>: Regression provides tools for prediction and understanding relationships, demonstrating the practical value of statistical thinking.</p>
</section>
<section id="what-we-haven-t-covered-but-now-you-re-prepared-for">
<h3>What We Haven’t Covered (But Now You’re Prepared For)<a class="headerlink" href="#what-we-haven-t-covered-but-now-you-re-prepared-for" title="Link to this heading"></a></h3>
<p>This course has provided a solid foundation, but statistical methods extend far beyond what we’ve covered:</p>
<p><strong>Multiple Regression</strong>: Extending to multiple explanatory variables, which requires matrix algebra but follows the same conceptual framework.</p>
<p><strong>Generalized Linear Models</strong>: Handling non-normal response variables (e.g., binary outcomes, count data) using similar estimation and inference principles.</p>
<p><strong>Non-parametric Methods</strong>: Procedures that don’t require distributional assumptions, providing alternatives when our assumptions are severely violated.</p>
<p><strong>Time Series Analysis</strong>: Methods for data collected over time, where independence assumptions are violated.</p>
<p><strong>Experimental Design</strong>: More sophisticated designs beyond the basic principles we’ve covered.</p>
<p><strong>Bayesian Statistics</strong>: A different philosophical approach to inference that updates beliefs based on data.</p>
<p><strong>Machine Learning</strong>: Prediction-focused methods that often sacrifice interpretability for predictive accuracy.</p>
<p><strong>Causal Inference</strong>: Methods for trying to establish causation from observational data.</p>
<p>The tools and thinking you’ve developed in this course provide the foundation for understanding all these advanced topics.</p>
</section>
<section id="the-bigger-picture-statistical-literacy-in-the-modern-world">
<h3>The Bigger Picture: Statistical Literacy in the Modern World<a class="headerlink" href="#the-bigger-picture-statistical-literacy-in-the-modern-world" title="Link to this heading"></a></h3>
<p>Perhaps most importantly, this course has developed your <strong>statistical literacy</strong>—the ability to think critically about data, understand uncertainty, and evaluate quantitative claims. In our data-rich world, these skills are increasingly valuable:</p>
<p><strong>Critical Thinking</strong>: You can now evaluate whether statistical claims in news reports, research studies, and business presentations are justified by the evidence presented.</p>
<p><strong>Understanding Uncertainty</strong>: You appreciate that all statistical conclusions come with uncertainty, and you can interpret confidence intervals and p-values appropriately.</p>
<p><strong>Research Evaluation</strong>: You can assess whether studies use appropriate methods, check important assumptions, and draw reasonable conclusions.</p>
<p><strong>Problem-Solving Framework</strong>: You have a systematic approach to analyzing data: explore, model, check assumptions, conduct inference, and interpret results in context.</p>
<p><strong>Communication Skills</strong>: You can explain statistical results to others, emphasizing practical significance alongside statistical significance.</p>
</section>
<section id="final-thoughts-statistics-as-a-way-of-thinking">
<h3>Final Thoughts: Statistics as a Way of Thinking<a class="headerlink" href="#final-thoughts-statistics-as-a-way-of-thinking" title="Link to this heading"></a></h3>
<p>Statistics is ultimately about making sense of an uncertain world using imperfect information. The specific formulas and procedures you’ve learned are important, but the deeper lesson is about <strong>thinking statistically</strong>:</p>
<ul class="simple">
<li><p>Recognizing that variability is natural and must be accounted for</p></li>
<li><p>Understanding that conclusions should be proportional to the strength of evidence</p></li>
<li><p>Appreciating that correlation doesn’t imply causation</p></li>
<li><p>Knowing that larger, well-designed studies provide more reliable information</p></li>
<li><p>Realizing that statistical significance doesn’t automatically mean practical importance</p></li>
</ul>
<p>These ways of thinking will serve you well beyond any specific statistical analysis you might conduct. Whether you’re evaluating a medical treatment, making a business decision, or simply reading the news, the statistical thinking you’ve developed will help you make more informed decisions.</p>
<p>As you move forward, remember that statistics is not just a collection of techniques—it’s a powerful framework for learning from data and making decisions under uncertainty. The journey you’ve completed in STAT 350 has equipped you with both the technical tools and the conceptual framework to continue learning and applying statistical methods throughout your career.</p>
<p>The combination of healthy skepticism and quantitative rigor that characterizes statistical thinking is perhaps one of the most valuable intellectual tools you can possess in the 21st century. Use it wisely.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>Two types of prediction intervals serve different purposes</strong>: Confidence intervals for mean response estimate average behavior, while prediction intervals for individual observations account for additional uncertainty from new error terms.</p></li>
<li><p><strong>Prediction intervals are always wider than confidence intervals</strong> because they include both estimation uncertainty and individual variability around the mean response.</p></li>
<li><p><strong>Interpolation is safe, extrapolation is dangerous</strong>: Predictions should only be made within the range of observed explanatory variable values used to fit the model.</p></li>
<li><p><strong>Mathematical derivations reveal why formulas work</strong>: Both confidence and prediction intervals follow from expressing estimates as linear combinations of response values and applying normal distribution theory.</p></li>
<li><p><strong>The Central Limit Theorem provides robustness</strong>: Parameter estimates and mean response predictions remain approximately valid under moderate departures from normality with adequate sample sizes.</p></li>
<li><p><strong>Individual predictions are not robust to normality violations</strong>: Prediction intervals require the normality assumption because they involve new error terms that don’t benefit from averaging.</p></li>
<li><p><strong>Confidence and prediction bands visualize uncertainty</strong>: These bands show how uncertainty varies across the range of explanatory variable values, being narrowest at the center of the data.</p></li>
<li><p><strong>Complete applied examples integrate all concepts</strong>: Real-world analysis requires careful attention to assumptions, appropriate interpretation of results, and acknowledgment of limitations.</p></li>
<li><p><strong>Statistical thinking transcends specific techniques</strong>: The framework of estimation, inference, and uncertainty quantification applies across all areas of statistics.</p></li>
<li><p><strong>This course provides foundation for advanced methods</strong>: The concepts of parameter estimation, hypothesis testing, confidence intervals, and assumption checking extend to much more sophisticated statistical procedures.</p></li>
<li><p><strong>Statistical literacy is increasingly valuable</strong>: The ability to think critically about data and understand uncertainty is essential in our data-driven world.</p></li>
<li><p><strong>Linear regression synthesizes the entire course</strong>: It demonstrates how descriptive statistics, probability, sampling distributions, and inference work together to solve practical problems.</p></li>
</ol>
</div>
</section>
<section id="exercises">
<h3>Exercises<a class="headerlink" href="#exercises" title="Link to this heading"></a></h3>
<ol class="arabic">
<li><p><strong>Prediction Types and Interpretation</strong>: For a study relating years of experience (X) to annual salary (Y) with fitted model <span class="math notranslate nohighlight">\(\hat{y} = 35000 + 2000x\)</span>:</p>
<ol class="loweralpha simple">
<li><p>Calculate the predicted salary for someone with 10 years of experience</p></li>
<li><p>Explain the difference between a confidence interval for mean salary and a prediction interval for an individual’s salary at X = 10</p></li>
<li><p>Which interval will be wider and why?</p></li>
<li><p>How would you explain these concepts to a non-statistical audience?</p></li>
</ol>
</li>
<li><p><strong>Mathematical Understanding</strong>: Given the variance formulas for mean response and individual prediction:</p>
<p><strong>Mean response</strong>: <span class="math notranslate nohighlight">\(\text{Var}[\hat{\mu}^*] = \sigma^2 \left(\frac{1}{n} + \frac{(x^* - \bar{x})^2}{S_{xx}}\right)\)</span></p>
<p><strong>Individual prediction</strong>: <span class="math notranslate nohighlight">\(\text{Var}[\hat{Y}^*] = \sigma^2 \left(1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{S_{xx}}\right)\)</span></p>
<ol class="loweralpha simple">
<li><p>Explain why the prediction variance includes the additional “+1” term</p></li>
<li><p>How does the variance change as <span class="math notranslate nohighlight">\(x^*\)</span> moves away from <span class="math notranslate nohighlight">\(\bar{x}\)</span>?</p></li>
<li><p>What happens to both variances as <span class="math notranslate nohighlight">\(n\)</span> increases?</p></li>
<li><p>Under what conditions would the two variances be most similar?</p></li>
</ol>
</li>
<li><p><strong>Robustness Analysis</strong>: Consider a regression analysis where residual plots suggest the error distribution is right-skewed rather than normal:</p>
<ol class="loweralpha simple">
<li><p>Which inference procedures would you still trust and why?</p></li>
<li><p>Which procedures would you be most concerned about?</p></li>
<li><p>How might sample size affect your confidence in the results?</p></li>
<li><p>What alternative approaches might you consider?</p></li>
</ol>
</li>
<li><p><strong>Interpolation vs. Extrapolation</strong>: A study of house prices (Y) versus square footage (X) uses data from houses ranging from 1000 to 3000 square feet:</p>
<ol class="loweralpha simple">
<li><p>Classify each prediction as interpolation or extrapolation: 1500 sq ft, 750 sq ft, 2800 sq ft, 4000 sq ft</p></li>
<li><p>For each extrapolation case, explain why it might be problematic</p></li>
<li><p>How might you determine if extrapolation is reasonable?</p></li>
<li><p>What additional information would help validate extrapolations?</p></li>
</ol>
</li>
<li><p><strong>Comprehensive Analysis Design</strong>: Design a complete regression analysis for studying the relationship between study hours per week (X) and GPA (Y):</p>
<ol class="loweralpha simple">
<li><p>Describe your data collection plan including sample size justification</p></li>
<li><p>List potential confounding variables and how you might control for them</p></li>
<li><p>Describe the complete analysis workflow from data collection to final conclusions</p></li>
<li><p>Identify the limitations of your study and how they affect interpretation</p></li>
</ol>
</li>
<li><p><strong>R Implementation</strong>: Using the cetane number dataset or similar data:</p>
<ol class="loweralpha simple">
<li><p>Fit the regression model and check all assumptions</p></li>
<li><p>Create confidence and prediction intervals for specific values</p></li>
<li><p>Generate confidence and prediction bands across the range of X</p></li>
<li><p>Create a comprehensive visualization showing data, fitted line, and both bands</p></li>
<li><p>Write a complete interpretation of all results</p></li>
</ol>
</li>
<li><p><strong>Assumption Violations</strong>: For each scenario, identify the primary assumption violation and discuss the implications:</p>
<ol class="loweralpha simple">
<li><p>Residual plot shows a clear curved pattern</p></li>
<li><p>Residual plot shows increasing variance as X increases</p></li>
<li><p>QQ plot of residuals shows heavy tails</p></li>
<li><p>Data points were collected sequentially over time and show temporal patterns</p></li>
<li><p>The dataset includes two distinct subgroups with different relationships</p></li>
</ol>
</li>
<li><p><strong>Band Interpretation</strong>: Looking at a plot with confidence and prediction bands:</p>
<ol class="loweralpha simple">
<li><p>Explain why both bands have a “bow-tie” shape</p></li>
<li><p>What does it mean if a data point falls outside the prediction band?</p></li>
<li><p>How would you use these bands to identify influential observations?</p></li>
<li><p>Explain how the bands would change if we used 90% confidence instead of 95%</p></li>
</ol>
</li>
<li><p><strong>Real-World Application</strong>: Choose a topic from your field of interest and:</p>
<ol class="loweralpha simple">
<li><p>Identify two quantitative variables that might be linearly related</p></li>
<li><p>Describe how you would collect appropriate data</p></li>
<li><p>Predict what type of relationship you might find and why</p></li>
<li><p>Discuss how you would validate your model</p></li>
<li><p>Explain how prediction intervals would be useful in your context</p></li>
</ol>
</li>
<li><p><strong>Statistical Communication</strong>: Write a brief report (2-3 paragraphs) explaining regression results to each audience:</p>
<ol class="loweralpha simple">
<li><p>A technical audience familiar with statistics</p></li>
<li><p>A business audience with minimal statistical background</p></li>
<li><p>A general public audience reading a newspaper article</p></li>
<li><p>Compare how your explanations differ and why</p></li>
</ol>
</li>
<li><p><strong>Course Integration</strong>: Explain how linear regression connects to other topics covered in the course:</p>
<ol class="loweralpha simple">
<li><p>How do confidence intervals in regression relate to confidence intervals for population means?</p></li>
<li><p>How does the F-test in regression relate to ANOVA F-tests?</p></li>
<li><p>How do residual diagnostics connect to normality testing in earlier chapters?</p></li>
<li><p>How does the concept of sampling distributions apply to regression parameters?</p></li>
</ol>
</li>
<li><p><strong>Critical Thinking</strong>: Evaluate this claim: “Our model has R² = 0.95, proving that X causes Y and that our predictions will be very accurate.”</p>
<ol class="loweralpha simple">
<li><p>Identify all the problems with this statement</p></li>
<li><p>Explain what R² actually tells us and what it doesn’t</p></li>
<li><p>Discuss the relationship between R² and prediction accuracy</p></li>
<li><p>Suggest how the statement could be revised to be more accurate</p></li>
</ol>
</li>
<li><p><strong>Advanced Considerations</strong>: Research and briefly explain how the concepts from this chapter extend to:</p>
<ol class="loweralpha simple">
<li><p>Multiple regression with several explanatory variables</p></li>
<li><p>Logistic regression for binary response variables</p></li>
<li><p>Time series analysis where independence is violated</p></li>
<li><p>How would the prediction interval concepts change in these contexts?</p></li>
</ol>
</li>
<li><p><strong>Ethical Considerations</strong>: Discuss the ethical implications of regression analysis:</p>
<ol class="loweralpha simple">
<li><p>When might prediction intervals be too wide to be practically useful?</p></li>
<li><p>How should uncertainty be communicated when making important decisions?</p></li>
<li><p>What responsibilities do analysts have when their models might influence policy?</p></li>
<li><p>How can we avoid misuse of regression models in high-stakes applications?</p></li>
</ol>
</li>
<li><p><strong>Reflection Essay</strong>: Write a reflective essay (500-750 words) on your statistical learning journey:</p>
<ol class="loweralpha simple">
<li><p>How has your understanding of uncertainty and variability evolved?</p></li>
<li><p>What statistical concepts do you find most challenging and why?</p></li>
<li><p>How do you see yourself using statistical thinking in your future career?</p></li>
<li><p>What questions about statistics are you most curious to explore further?</p></li>
</ol>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="13-3-diagnostics-inference.html" class="btn btn-neutral float-left" title="13.3. Model Diagnostics and Statistical Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>