

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>13.2. Simple Linear Regression &mdash; STAT 350</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=581abb6a" />

  
    <link rel="canonical" href="https://treese41528.github.io/STAT350/course_site/chapter13/lectures/13-2-simple-linear-regression.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../../_static/custom.js?v=de5959cf"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="13.3. Model Diagnostics and Statistical Inference" href="13-3-diagnostics-inference.html" />
    <link rel="prev" title="13.1. Introduction to Linear Regression: Correlation and Scatter Plots" href="13-1-intro-to-lr-correlation-scatter-plots.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            STAT 350: Introduction to Statistics
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Orientation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-intro.html">Course Introduction &amp; Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#why-statistics-why-now">Why Statistics? Why Now?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#course-roadmap">Course Roadmap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#a-clear-note-on-using-ai">A Clear Note on Using AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../course-intro.html#getting-started-a-checklist">Getting Started: A Checklist</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#homework-assignments-on-edfinity">Homework Assignments on Edfinity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../course-intro.html#miscellaneous-tips">Miscellaneous Tips</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Exam Information</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../exams/exams_index.html">Course Examinations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../exams/exams_index.html#general-exam-policies">General Exam Policies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../exams/exam_materials/exam1.html">Exam 1</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam1.html#exam-locations-by-section">Exam Locations by Section</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam1.html#exam-coverage">Exam Coverage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam1.html#preparation-materials">Preparation Materials</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../exams/exam_materials/exam2.html">Exam 2</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#exam-locations-by-section">Exam Locations by Section</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#exam-coverage">Exam Coverage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#additional-resources">Additional Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/exam2.html#preparation-materials">Preparation Materials</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../exams/exam_materials/final_exam.html">Final Exam</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#about-the-final-exam">About the Final Exam</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#required-review-materials">Required Review Materials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#post-exam-2-preparation-materials">Post Exam 2 Preparation Materials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../exams/exam_materials/final_exam.html#study-guide-resource">Study Guide Resource</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Worksheets</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../worksheets/worksheets_index.html">Course Worksheets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../worksheets/worksheets_index.html#pedagogical-philosophy">Pedagogical Philosophy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../worksheets/worksheets_index.html#implementation-guidelines">Implementation Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../worksheets/worksheets_index.html#why-these-worksheets-matter">Why These Worksheets Matter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../worksheets/worksheets_index.html#the-critical-role-of-simulation">The Critical Role of Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../worksheets/worksheets_index.html#worksheets">Worksheets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html">Worksheet 1: Exploring Data with R</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-1-loading-and-understanding-the-dataset">Part 1: Loading and Understanding the Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-2-initial-data-exploration">Part 2: Initial Data Exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-3-frequency-tables">Part 3: Frequency Tables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-4-univariate-analysis-of-uptake">Part 4: Univariate Analysis of Uptake</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-5-grouped-statistics-with-tapply">Part 5: Grouped Statistics with tapply</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-6-comparative-visualization-by-type">Part 6: Comparative Visualization by Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-7-exploring-the-concentration-effect">Part 7: Exploring the Concentration Effect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#part-8-advanced-visualization-with-multiple-categories">Part 8: Advanced Visualization with Multiple Categories</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#reference-key-functions">Reference: Key Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet1.html#submission-guidelines">Submission Guidelines</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html">Worksheet 2: Set Theory and Probability Fundamentals</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#part-1-set-theory-foundations">Part 1: Set Theory Foundations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#part-2-probability-axioms">Part 2: Probability Axioms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#part-3-applying-probability-rules">Part 3: Applying Probability Rules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#part-4-the-inclusion-exclusion-principle">Part 4: The Inclusion-Exclusion Principle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#simulation-exercise">Simulation Exercise</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet2.html#submission-guidelines">Submission Guidelines</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html">Worksheet 3: Conditional Probability and Bayes’ Theorem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html#part-1-understanding-conditional-probability">Part 1: Understanding Conditional Probability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html#part-2-tree-diagrams-and-sequential-sampling">Part 2: Tree Diagrams and Sequential Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html#part-3-bayes-theorem-and-sequential-updating">Part 3: Bayes’ Theorem and Sequential Updating</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet3.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html">Worksheet 4: Independence and Random Variables</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#part-1-independence-property">Part 1: Independence Property</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#part-2-independent-vs-mutually-exclusive-events">Part 2: Independent vs. Mutually Exclusive Events</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#part-3-introduction-to-random-variables">Part 3: Introduction to Random Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#part-4-probability-mass-functions">Part 4: Probability Mass Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#part-5-joint-probability-mass-functions">Part 5: Joint Probability Mass Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet4.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html">Worksheet 5: Expected Value and Variance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#part-1-expected-value-and-lotus">Part 1: Expected Value and LOTUS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#part-2-variance-and-its-properties">Part 2: Variance and Its Properties</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#part-3-sums-of-random-variables">Part 3: Sums of Random Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#part-4-joint-probability-mass-functions">Part 4: Joint Probability Mass Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet5.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html">Worksheet 6: Named Discrete Distributions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#part-1-the-bernoulli-distribution">Part 1: The Bernoulli Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#part-2-the-binomial-distribution">Part 2: The Binomial Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#part-3-the-poisson-distribution">Part 3: The Poisson Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#part-4-other-named-discrete-distributions">Part 4: Other Named Discrete Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet6.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html">Worksheet 7: Continuous Random Variables</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#part-1-probability-density-functions">Part 1: Probability Density Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#part-2-finding-constants-for-valid-pdfs">Part 2: Finding Constants for Valid PDFs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#part-3-expected-value-and-variance">Part 3: Expected Value and Variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#part-4-cumulative-distribution-functions">Part 4: Cumulative Distribution Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet7.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet8.html">Worksheet 8: Uniform and Exponential Distributions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet8.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet8.html#part-1-the-uniform-distribution">Part 1: The Uniform Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet8.html#part-2-the-exponential-distribution">Part 2: The Exponential Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet8.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html">Worksheet 9: The Normal Distribution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#part-1-the-normal-distribution">Part 1: The Normal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#part-2-the-empirical-rule">Part 2: The Empirical Rule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#part-3-the-standard-normal-table">Part 3: The Standard Normal Table</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#part-4-z-score-transformation">Part 4: Z-Score Transformation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet9.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet10.html">Worksheet 10: Checking Normality and Introduction to Sampling Distributions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet10.html#part-1-checking-normality">Part 1: Checking Normality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet10.html#part-2-introduction-to-sampling-distributions">Part 2: Introduction to Sampling Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../worksheets/worksheet_materials/worksheet10.html#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Computer Assignments</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html">R / RStudio Guide and Function Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#quick-reference-table">Quick Reference Table</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#quick-start-r-rstudio-setup">Quick Start: R / RStudio Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#recommended-workflow">Recommended workflow</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#course-pipeline-at-a-glance">Course Pipeline (At a Glance)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#packages-libraries-course-set">Packages / Libraries (Course Set)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#rstudio-orientation">RStudio Orientation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#getting-started-with-swirl">Getting Started with swirl</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#setup-and-use-swirl">Setup and Use swirl</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#alternative-r-learning-resources">Alternative R Learning Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#base-r">Base R</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#tidyverse">tidyverse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#r-markdown">R Markdown</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#statistical-computing">Statistical Computing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#video-resources">Video Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#assignment-tutorials-links">Assignment Tutorials (Links)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#function-reference-alphabetized-within-category">Function Reference (Alphabetized within Category)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#data-i-o-housekeeping">Data I/O &amp; Housekeeping</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#data-structures-creation">Data Structures &amp; Creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#data-wrangling-utilities">Data Wrangling &amp; Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#descriptive-statistics-correlation">Descriptive Statistics &amp; Correlation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#probability-distributions">Probability &amp; Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#simulation-functions">Simulation Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#inference-functions">Inference Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#graphics-ggplot2">Graphics (ggplot2)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#core-components">Core Components</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#geoms-geometric-objects">Geoms (Geometric Objects)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#plot-customization">Plot Customization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#tables-reporting">Tables &amp; Reporting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#best-practices-common-pitfalls">Best Practices &amp; Common Pitfalls</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#appendix-quick-links">Appendix: Quick Links</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#setup">Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#assignments">Assignments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../R_computerAssignments/r_computer_assignments.html#getting-help">Getting Help</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../chapter1/index.html">1. Introduction to Statistics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-1-intro.html">1.1. What Is Statistics?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter1/lectures/1-2-probability-inference.html">1.2. Probability &amp; Statistical Inference: How Are They Associated?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter2/index.html">2. Graphical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-1-understanding-the-structure-of-data-set.html">2.1. Data Set Structure and Variable Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-2-tools-for-categorical-qualitative-data.html">2.2. Tools for Categorical (Qualitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-3-tools-for-numerical-quantitative-data.html">2.3. Tools for Numerical (Quantitative) Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter2/lectures/2-4-exploring-quantitative-distributions-modality-shape-and-outliers.html">2.4. Exploring Quantitative Distributions: Modality, Skewness &amp; Outliers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter3/index.html">3. Numerical Summaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-1-intro-numerical-summaries.html">3.1. Introduction to Numerical Summaries: Notation and Terminology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-2-measures-of-central-tendency.html">3.2. Measures of Central Tendency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-3-measures-of-variability-range-variance-and-SD.html">3.3. Measures of Variability - Range, Variance, and Standard Deviation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-4-measures-of-variability-IQR-and-5-number-summary.html">3.4. Measures of Variability - Interquartile Range and Five-Number Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter3/lectures/3-5-choosing-the-right-measure-resistance-to-extreme-values.html">3.5. Choosing the Right Measure &amp; Comparing Measures Across Data Sets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter4/index.html">4. Probability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-1-basic-set-theory.html">4.1. Basic Set Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-2-probability.html">4.2. Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-3-conditional-probability.html">4.3. Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-4-law-of-total-probability-and-bayes-rule.html">4.4. Law of Total Probability and Bayes’ Rule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-5-bayes-update-rule-example.html">4.5. Bayesian Updating: Sequential Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter4/lectures/4-6-independence-of-events.html">4.6. Independence of Events</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter5/index.html">5. Discrete Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-1-discrete-rvs-and-pmfs.html">5.1. Discrete Random Variables and Probability Mass Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-2-joint-pmfs.html">5.2. Joint Probability Mass Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-3-expected-value-of-discrete-rv.html">5.3. Expected Value of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-4-variance-of-discrete-rv.html">5.4. Varianace of a Discrete Random Variable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-5-covariance-of-dependent-rvs.html">5.5. Covariance of Dependent Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-6-binomial-distribution.html">5.6. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter5/lectures/5-7-poisson-distribution.html">5.7. Poisson Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter6/index.html">6. Continuous Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-1-continuous-rvs-and-pdfs.html">6.1. Continuous Random Variables and Probability Density Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-2-expected-value-and-variance-of-cts-rvs.html">6.2. Expected Value and Variance of Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-3-cdfs.html">6.3. Cumulative Distribution Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-4-normal-distribution.html">6.4. Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-5-uniform-distribution.html">6.5. Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter6/lectures/6-6-exponential-distribution.html">6.6. Exponential Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter7/index.html">7. Sampling Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-1-statistics-and-sampling-distributions.html">7.1. Statistics and Sampling Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-2-sampling-distribution-for-the-sample-mean.html">7.2. Sampling Distribution for the Sample Mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-3-clt.html">7.3. The Central Limit Theorem (CLT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter7/lectures/7-4-discret-rvs-and-clt.html">7.4. Understanding Binomial and Poisson Distributions through CLT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter8/index.html">8. Experimental Design</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-1-experimental-and-sampling-designs.html">8.1. Experimental and Sampling Designs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-2-experimental-design-principles.html">8.2. Experimental Design Principles</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-3-basic-types-of-experimental-design.html">8.3. Basic Types of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-4-addressing-potential-flaws-in-experimental-design.html">8.4. Addressing Potential Flaws in Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-5-examples-of-experimental-design.html">8.5. Examples of Experimental Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-6-sampling-design.html">8.6. Sampling Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter8/lectures/8-7-sampling-bias.html">8.7. Sampling Bias</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter9/index.html">9. Confidence Intervals and Bounds</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-1-intro-statistical-inference.html">9.1. Introduction to Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-2-ci-sigma-known.html">9.2. Confidence Intervals for the Population Mean, When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-3-precision-of-ci-and-sample-size-calculation.html">9.3. Precision of a Confidence Interval and Sample Size Calculation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-4-cb-sigma-known.html">9.4. Confidence Bounds for the Poulation Mean When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter9/lectures/9-5-ci-cb-sigma-unknown.html">9.5. Confidence Intervals and Bounds When σ is Unknown</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter10/index.html">10. Hypothesis Testing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-1-ht-errors-and-power.html">10.1. Type I Error, Type II Error, and Power</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-2-ht-for-mean-sigma-known.html">10.2. Hypothesis Test for the Population Mean When σ is Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-3-ht-for-mean-sigma-unknown.html">10.3. Hypothesis Test for the Population Mean When σ Is Unknown</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter10/lectures/10-4-pvalue-significance-conclusion.html">10.4. P-values, Statistical Significance, and Formal Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter11/index.html">11. Two Sample Procedures</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-1-ci-ht-two-samples.html">11.1. Confidence Interval/Bound and Hypothesis Test for Two Samples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-2-comparing-two-means-independent-sigmas-known.html">11.2. Comparing the Means of Two Independent Populations - Population Variances Are Known</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-3-comparing-two-means-independent-pooled.html">11.3. Comparing the Means of Two Independent Populations - Pooled Variance Estimator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-4-comparing-two-means-independent-unpooled.html">11.4. Comparing the Means of Two Independent Populations - No Equal Variance Assumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter11/lectures/11-5-mean-of-paired-differences-two-dependent-populations.html">11.5. Analyzing the Mean of Paired Differences Between two Dependent Populations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter12/index.html">12. ANOVA</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-1-intro-one-way-anova.html">12.1. Introduction to One-Way ANOVA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-2-sources-of-variability.html">12.2. Different Sources of Variability in an ANOVA Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-3-f-test-and-relationship-to-t-test.html">12.3. One-Way ANOVA F-Test and Its Relationship to Two-Sample t-Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../chapter12/lectures/12-4-multiple-comparison-procedures-family-wise-error-rates.html">12.4. Multiple Comparison Procedures and Family-Wise Error Rates</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">13. Simple Linear Regression</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="13-1-intro-to-lr-correlation-scatter-plots.html">13.1. Introduction to Linear Regression: Correlation and Scatter Plots</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">13.2. Simple Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="13-3-diagnostics-inference.html">13.3. Model Diagnostics and Statistical Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="13-4-prediction-robustness.html">13.4. Prediction, Robustness, and Applied Examples</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">STAT 350: Introduction to Statistics</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">13. </span>Simple Linear Regression</a></li>
      <li class="breadcrumb-item active"><span class="section-number">13.2. </span>Simple Linear Regression</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/chapter13/lectures/13-2-simple-linear-regression.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="video-placeholder" role="group" aria-labelledby="video-ch13-3">
   <iframe
      id="video-ch13-3"
      title="STAT 350 – Chapter 13.3 Simple Linear Regression Model Video"
      src="https://www.youtube.com/embed/a9skiqjau8I?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
      allowfullscreen>
   </iframe>
</div><section id="id1">
<h1><span class="section-number">13.2. </span>Simple Linear Regression<a class="headerlink" href="#id1" title="Link to this heading"></a></h1>
<p>After exploring the relationship between two quantitative variables through scatter plots and determining
that a linear relationship provides the best description of their association, we need to move from visual
exploration to mathematical modeling. This involves formalizing our population-level model, developing methods
to estimate the parameters of this model, and creating tools to assess how well our model fits the data.</p>
<div class="important admonition">
<p class="admonition-title">Road Map 🧭</p>
<ul class="simple">
<li><p><strong>Problem we will solve</strong> – How to formalize the simple linear regression model, estimate its parameters using least squares methods, and assess model quality through ANOVA decomposition and the coefficient of determination</p></li>
<li><p><strong>Tools we’ll learn</strong> – The population regression model with assumptions, least squares estimation for slope and intercept, residual analysis, ANOVA table for regression, and R-squared as a measure of model fit</p></li>
<li><p><strong>How it fits</strong> – This transforms our visual understanding of relationships into a rigorous statistical framework with parameter estimation and model assessment, setting the foundation for hypothesis testing and prediction</p></li>
</ul>
</div>
<section id="the-population-model-for-simple-linear-regression">
<h2><span class="section-number">13.2.1. </span>The Population Model for Simple Linear Regression<a class="headerlink" href="#the-population-model-for-simple-linear-regression" title="Link to this heading"></a></h2>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-simple-linear-regression-model.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-simple-linear-regression-model.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-simple-linear-regression-model.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.18 </span><span class="caption-text">The simple linear regression population model showing the linear relationship between explanatory and response variables</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>After determining through scatter plot analysis that a linear relationship appropriately describes the association between our explanatory and response variables, we can formalize this relationship using a statistical model. This model provides the theoretical foundation for all our subsequent estimation and inference procedures.</p>
<section id="the-linear-regression-equation">
<h3>The Linear Regression Equation<a class="headerlink" href="#the-linear-regression-equation" title="Link to this heading"></a></h3>
<p>Our population model takes the form:</p>
<div class="math notranslate nohighlight">
\[Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i \quad \text{for } i = 1, 2, \ldots, n\]</div>
<p>This equation captures three essential components of the relationship:</p>
<p><strong>The Systematic Component</strong>: <span class="math notranslate nohighlight">\(\beta_0 + \beta_1 X_i\)</span> represents the <strong>mean response line</strong>—the average value of <span class="math notranslate nohighlight">\(Y\)</span> for any given value of <span class="math notranslate nohighlight">\(X\)</span>. This linear function defines the systematic relationship between the explanatory and response variables.</p>
<p><strong>The Random Component</strong>: <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> represents the <strong>error term</strong> or <strong>unexplained variation</strong>—the difference between each individual observation and the mean response line. This captures all variation in <span class="math notranslate nohighlight">\(Y\)</span> that cannot be explained by the linear relationship with <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p><strong>Population Parameters</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span> is the <strong>true population intercept</strong>—the expected value of <span class="math notranslate nohighlight">\(Y\)</span> when <span class="math notranslate nohighlight">\(X = 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_1\)</span> is the <strong>true population slope</strong>—the expected change in <span class="math notranslate nohighlight">\(Y\)</span> for a one-unit increase in <span class="math notranslate nohighlight">\(X\)</span></p></li>
</ul>
</section>
<section id="understanding-the-model-s-meaning">
<h3>Understanding the Model’s Meaning<a class="headerlink" href="#understanding-the-model-s-meaning" title="Link to this heading"></a></h3>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-model-assumptions.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-model-assumptions.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-model-assumptions.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.19 </span><span class="caption-text">Key assumptions of the simple linear regression model</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>This model framework tells us that for each fixed value of the explanatory variable <span class="math notranslate nohighlight">\(X_i\)</span>, the corresponding response <span class="math notranslate nohighlight">\(Y_i\)</span> is a random variable. The randomness comes from the error term <span class="math notranslate nohighlight">\(\varepsilon_i\)</span>, while the systematic relationship is captured by the linear function.</p>
<p><strong>Expected Value (Mean Response Line)</strong>:</p>
<div class="math notranslate nohighlight">
\[E[Y_i] = E[\beta_0 + \beta_1 X_i + \varepsilon_i] = \beta_0 + \beta_1 X_i\]</div>
<p>Since <span class="math notranslate nohighlight">\(\beta_0\)</span>, <span class="math notranslate nohighlight">\(\beta_1\)</span>, and <span class="math notranslate nohighlight">\(X_i\)</span> are constants, they pass through the expectation operator unchanged. The error term has expected value zero, so it disappears. This gives us the <strong>mean response line</strong>: <span class="math notranslate nohighlight">\(\mu_{Y|X=x} = \beta_0 + \beta_1 x\)</span>.</p>
<p><strong>Common Variance</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(Y_i) = \text{Var}(\beta_0 + \beta_1 X_i + \varepsilon_i) = \text{Var}(\varepsilon_i) = \sigma^2\]</div>
<p>Since constants have zero variance, only the error term contributes to the variability of <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
</section>
</section>
<section id="essential-assumptions-of-linear-regression">
<h2><span class="section-number">13.2.2. </span>Essential Assumptions of Linear Regression<a class="headerlink" href="#essential-assumptions-of-linear-regression" title="Link to this heading"></a></h2>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-assumptions-list.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-assumptions-list.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-assumptions-list.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.20 </span><span class="caption-text">The four fundamental assumptions required for valid linear regression analysis</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>For our regression procedures to be valid, we must make four key assumptions about our model:</p>
<p><strong>Assumption 1: Independence</strong></p>
<p>The observed pairs <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> for <span class="math notranslate nohighlight">\(i \in \{1, 2, \ldots, n\}\)</span> are such that <span class="math notranslate nohighlight">\(y_i\)</span> represents a simple random sample for each fixed value of <span class="math notranslate nohighlight">\(x_i\)</span>. Observations should not influence each other.</p>
<p><strong>Assumption 2: Linearity</strong></p>
<p>The association between the explanatory variable and the response is, on average, linear. The mean response follows the straight line <span class="math notranslate nohighlight">\(E[Y|X] = \beta_0 + \beta_1 X\)</span>.</p>
<p><strong>Assumption 3: Normality</strong></p>
<p>The error terms (and hence the <span class="math notranslate nohighlight">\(Y_i\)</span> values) are normally distributed:</p>
<div class="math notranslate nohighlight">
\[\varepsilon_i \stackrel{iid}{\sim} N(0, \sigma^2) \quad \text{for } i = 1, 2, \ldots, n\]</div>
<p><strong>Assumption 4: Equal Variance (Homoscedasticity)</strong></p>
<p>The error terms have constant variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> across all values of <span class="math notranslate nohighlight">\(X\)</span>. This means the spread of <span class="math notranslate nohighlight">\(Y\)</span> values around the regression line remains the same regardless of the <span class="math notranslate nohighlight">\(X\)</span> value.</p>
<section id="visualizing-the-regression-model">
<h3>Visualizing the Regression Model<a class="headerlink" href="#visualizing-the-regression-model" title="Link to this heading"></a></h3>
<figure class="align-default" id="id7">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-3d-regression-visualization.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-3d-regression-visualization.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-3d-regression-visualization.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.21 </span><span class="caption-text">Three-dimensional visualization showing normal distributions around the mean response line</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The regression model can be visualized as a three-dimensional structure:</p>
<ul class="simple">
<li><p><strong>The X-Y plane</strong> contains our scatter plot of data points</p></li>
<li><p><strong>The mean response line</strong> <span class="math notranslate nohighlight">\(\mu_{Y|X=x} = \beta_0 + \beta_1 x\)</span> runs through this plane</p></li>
<li><p><strong>Normal distributions</strong> are centered on this line at each <span class="math notranslate nohighlight">\(X\)</span> value, extending perpendicular to the X-Y plane</p></li>
</ul>
<p>For any fixed value of <span class="math notranslate nohighlight">\(X\)</span>, we observe a simple random sample of <span class="math notranslate nohighlight">\(Y\)</span> values from a normal distribution with:</p>
<ul class="simple">
<li><p><strong>Mean</strong>: <span class="math notranslate nohighlight">\(\beta_0 + \beta_1 x\)</span> (the value on the line)</p></li>
<li><p><strong>Variance</strong>: <span class="math notranslate nohighlight">\(\sigma^2\)</span> (the same for all <span class="math notranslate nohighlight">\(X\)</span> values)</p></li>
</ul>
<p>This is analogous to ANOVA, but instead of discrete group means, we have a continuous mean function that changes linearly with <span class="math notranslate nohighlight">\(X\)</span>.</p>
</section>
</section>
<section id="the-method-of-least-squares">
<h2><span class="section-number">13.2.3. </span>The Method of Least Squares<a class="headerlink" href="#the-method-of-least-squares" title="Link to this heading"></a></h2>
<p>Since we don’t know the true population parameters <span class="math notranslate nohighlight">\(\beta_0\)</span>, <span class="math notranslate nohighlight">\(\beta_1\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2\)</span>, we must estimate them from our sample data. The method of <strong>least squares</strong> provides the optimal approach for estimating the slope and intercept.</p>
<section id="the-least-squares-principle">
<h3>The Least Squares Principle<a class="headerlink" href="#the-least-squares-principle" title="Link to this heading"></a></h3>
<figure class="align-default" id="id8">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-finding-best-fit-line.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-finding-best-fit-line.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-finding-best-fit-line.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.22 </span><span class="caption-text">Illustration of the least squares principle using the car efficiency data</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The least squares method finds the line that minimizes the sum of squared deviations between the observed <span class="math notranslate nohighlight">\(y_i\)</span> values and the fitted values <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> predicted by our line.</p>
<p><strong>The Optimization Problem</strong>:</p>
<p>We want to find estimates <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> that minimize:</p>
<div class="math notranslate nohighlight">
\[g(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2\]</div>
<p><strong>Observable Errors (Residuals)</strong>:</p>
<p>For any candidate line <span class="math notranslate nohighlight">\(\hat{y} = b_0 + b_1 x\)</span>, we can compute residuals:</p>
<div class="math notranslate nohighlight">
\[e_i = y_i - \hat{y}_i = y_i - (b_0 + b_1 x_i)\]</div>
<p>The least squares method minimizes <span class="math notranslate nohighlight">\(\sum_{i=1}^n e_i^2\)</span>.</p>
</section>
<section id="derivation-of-least-squares-estimates">
<h3>Derivation of Least Squares Estimates<a class="headerlink" href="#derivation-of-least-squares-estimates" title="Link to this heading"></a></h3>
<figure class="align-default" id="id9">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-least-squares-calculus.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-least-squares-calculus.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-least-squares-calculus.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.23 </span><span class="caption-text">The calculus approach to finding least squares estimates</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>To find the minimum, we take partial derivatives with respect to both parameters and set them equal to zero.</p>
<p><strong>Optimizing with respect to</strong> <span class="math notranslate nohighlight">\(\beta_0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial g}{\partial \beta_0} = \frac{\partial}{\partial \beta_0} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 = 0\]</div>
<figure class="align-default" id="id10">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-intercept-derivation.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-intercept-derivation.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-intercept-derivation.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.24 </span><span class="caption-text">Step-by-step derivation of the intercept estimate</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Using the chain rule:</p>
<div class="math notranslate nohighlight">
\[-2 \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i) = 0\]</div>
<p>Dividing by -2 and expanding:</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n y_i - n\beta_0 - \beta_1 \sum_{i=1}^n x_i = 0\]</div>
<p>Solving for <span class="math notranslate nohighlight">\(\beta_0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_0 = \frac{1}{n}\sum_{i=1}^n y_i - \beta_1 \frac{1}{n}\sum_{i=1}^n x_i = \bar{y} - \hat{\beta}_1 \bar{x}\]</div>
<p><strong>Optimizing with respect to</strong> <span class="math notranslate nohighlight">\(\beta_1\)</span>:</p>
<figure class="align-default" id="id11">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-slope-derivation.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-slope-derivation.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-slope-derivation.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.25 </span><span class="caption-text">Detailed derivation of the slope estimate showing the algebraic steps</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="math notranslate nohighlight">
\[\frac{\partial g}{\partial \beta_1} = -2 \sum_{i=1}^n x_i(y_i - \beta_0 - \beta_1 x_i) = 0\]</div>
<p>Substituting our expression for <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and simplifying through several algebraic steps:</p>
<div class="math notranslate nohighlight">
\[\hat{\beta}_1 = \frac{\sum_{i=1}^n x_i y_i - n\bar{x}\bar{y}}{\sum_{i=1}^n x_i^2 - n\bar{x}^2}\]</div>
</section>
<section id="the-least-squares-formulas">
<h3>The Least Squares Formulas<a class="headerlink" href="#the-least-squares-formulas" title="Link to this heading"></a></h3>
<figure class="align-default" id="id12">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-line-formulas.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-line-formulas.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-line-formulas.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.26 </span><span class="caption-text">Final least squares formulas expressed in terms of sample covariance and variance</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Our final least squares estimates can be expressed in several equivalent forms:</p>
<p><strong>Slope Estimate</strong>:</p>
<div class="math notranslate nohighlight">
\[b_1 = \frac{s_{xy}}{s_x^2} = \frac{S_{xy}}{S_{xx}}\]</div>
<p>where:
- <span class="math notranslate nohighlight">\(s_{xy} = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})\)</span> (sample covariance)
- <span class="math notranslate nohighlight">\(s_x^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2\)</span> (sample variance of X)
- <span class="math notranslate nohighlight">\(S_{xy} = \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})\)</span> (sum of cross-products)
- <span class="math notranslate nohighlight">\(S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2\)</span> (sum of squares for X)</p>
<p><strong>Intercept Estimate</strong>:</p>
<div class="math notranslate nohighlight">
\[b_0 = \bar{y} - b_1\bar{x}\]</div>
<p><strong>Fitted Regression Line</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = b_0 + b_1 x\]</div>
</section>
</section>
<section id="properties-of-the-regression-line">
<h2><span class="section-number">13.2.4. </span>Properties of the Regression Line<a class="headerlink" href="#properties-of-the-regression-line" title="Link to this heading"></a></h2>
<figure class="align-default" id="id13">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-line-properties.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-line-properties.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-regression-line-properties.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.27 </span><span class="caption-text">Important properties of the fitted regression line</span><a class="headerlink" href="#id13" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The least squares regression line has several important properties:</p>
<p><strong>Property 1: Interpretation of the Slope</strong></p>
<p>The slope <span class="math notranslate nohighlight">\(b_1\)</span> represents the average change in the response variable for every one-unit change in the explanatory variable. The sign of <span class="math notranslate nohighlight">\(b_1\)</span> indicates the direction of the association.</p>
<p><strong>Property 2: Interpretation of the Intercept</strong></p>
<p>The intercept <span class="math notranslate nohighlight">\(b_0\)</span> represents the average value of the response variable when the explanatory variable equals zero. However, this may not have practical meaning if <span class="math notranslate nohighlight">\(X = 0\)</span> is outside the range of the data or not physically meaningful.</p>
<p><strong>Property 3: The Line Passes Through</strong> <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span></p>
<p>If we substitute <span class="math notranslate nohighlight">\(x = \bar{x}\)</span> into our regression equation:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = b_0 + b_1\bar{x} = (\bar{y} - b_1\bar{x}) + b_1\bar{x} = \bar{y}\]</div>
<p><strong>Property 4: Non-invertibility</strong></p>
<p>If we swap the explanatory and response variables and refit the regression, we do not get the algebraic inverse of the original line. The least squares method considers which variable is treated as fixed and which as random, so the roles cannot simply be interchanged.</p>
</section>
<section id="a-complete-example-blood-pressure-study">
<h2><span class="section-number">13.2.5. </span>A Complete Example: Blood Pressure Study<a class="headerlink" href="#a-complete-example-blood-pressure-study" title="Link to this heading"></a></h2>
<figure class="align-default" id="id14">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-example.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-example.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-blood-pressure-example.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.28 </span><span class="caption-text">Blood pressure treatment study data showing age and change in blood pressure</span><a class="headerlink" href="#id14" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Let’s work through a comprehensive example to illustrate all the concepts.</p>
<p><strong>Research Context</strong>: A new treatment for high blood pressure is being assessed for feasibility. In an early trial, 11 subjects have their blood pressure measured before and after treatment. Researchers want to determine if there’s an association between patient age and the change in systolic blood pressure after 24 hours.</p>
<p><strong>Variable Definitions</strong>:
- <strong>Explanatory variable (X)</strong>: Age of patient (years)
- <strong>Response variable (Y)</strong>: Change in blood pressure = (After treatment) - (Before treatment)</p>
<p><strong>Expected Results</strong>: If the treatment is effective, we expect mostly negative values (blood pressure decreases). If age affects treatment effectiveness, we might see a relationship between age and the magnitude of change.</p>
<section id="computing-the-least-squares-estimates">
<h3>Computing the Least Squares Estimates<a class="headerlink" href="#computing-the-least-squares-estimates" title="Link to this heading"></a></h3>
<p>Using our data with <span class="math notranslate nohighlight">\(n = 11\)</span> subjects:</p>
<p><strong>Summary Statistics</strong>:
- <span class="math notranslate nohighlight">\(\bar{x} = 52.73\)</span> years
- <span class="math notranslate nohighlight">\(\bar{y} = -8.09\)</span> mm Hg
- <span class="math notranslate nohighlight">\(\sum x_i y_i = -5700\)</span>
- <span class="math notranslate nohighlight">\(\sum x_i^2 = 32056\)</span></p>
<p><strong>Slope Calculation</strong>:</p>
<div class="math notranslate nohighlight">
\[S_{xy} = \sum x_i y_i - n\bar{x}\bar{y} = -5700 - 11(52.73)(-8.09) = -1056\]</div>
<div class="math notranslate nohighlight">
\[S_{xx} = \sum x_i^2 - n\bar{x}^2 = 32056 - 11(52.73)^2 = 2008\]</div>
<div class="math notranslate nohighlight">
\[b_1 = \frac{S_{xy}}{S_{xx}} = \frac{-1056}{2008} = -0.526\]</div>
<p><strong>Intercept Calculation</strong>:</p>
<div class="math notranslate nohighlight">
\[b_0 = \bar{y} - b_1\bar{x} = -8.09 - (-0.526)(52.73) = 20.11\]</div>
<p><strong>Fitted Regression Line</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = 20.11 - 0.526x\]</div>
<p><strong>Interpretation</strong>:</p>
<ul class="simple">
<li><p>For each additional year of age, the change in blood pressure decreases by an average of 0.526 mm Hg (treatment becomes more effective)</p></li>
<li><p>The intercept (20.11) has no practical meaning since we don’t study newborns for blood pressure treatment</p></li>
<li><p>The negative slope suggests older patients benefit more from the treatment</p></li>
</ul>
</section>
</section>
<section id="residual-analysis-and-variance-estimation">
<h2><span class="section-number">13.2.6. </span>Residual Analysis and Variance Estimation<a class="headerlink" href="#residual-analysis-and-variance-estimation" title="Link to this heading"></a></h2>
<figure class="align-default" id="id15">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-variance-estimation.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-variance-estimation.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-variance-estimation.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.29 </span><span class="caption-text">Estimating the common variance using residuals from the fitted model</span><a class="headerlink" href="#id15" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Once we have our fitted line, we can compute residuals and estimate the error variance.</p>
<p><strong>Residuals (Observable Errors)</strong>:</p>
<div class="math notranslate nohighlight">
\[e_i = y_i - \hat{y}_i = y_i - (b_0 + b_1 x_i)\]</div>
<p><strong>Example Calculation</strong>: For a 65-year-old patient:</p>
<ul class="simple">
<li><p><strong>Predicted change</strong>: <span class="math notranslate nohighlight">\(\hat{y} = 20.11 - 0.526(65) = -14.0\)</span> mm Hg</p></li>
<li><p><strong>Observed change</strong>: <span class="math notranslate nohighlight">\(y = -8\)</span> mm Hg (from data)</p></li>
<li><p><strong>Residual</strong>: <span class="math notranslate nohighlight">\(e = -8 - (-14) = 6\)</span> mm Hg</p></li>
</ul>
<p><strong>Variance Estimate</strong>:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2 = \frac{\text{SSE}}{n-2}\]</div>
<p>We use <span class="math notranslate nohighlight">\(n-2\)</span> degrees of freedom because we estimated two parameters (<span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span>) from the data.</p>
</section>
<section id="the-anova-table-for-regression">
<h2><span class="section-number">13.2.7. </span>The ANOVA Table for Regression<a class="headerlink" href="#the-anova-table-for-regression" title="Link to this heading"></a></h2>
<div class="video-placeholder" role="group" aria-labelledby="video-ch13-4">
   <iframe
      id="video-ch13-4"
      title="STAT 350 – Chapter 13.4 Simple Linear Regression ANOVA Table and Coefficient of Determination Video"
      src="https://www.youtube.com/embed/nD9hKHIaUIQ?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
      allowfullscreen>
   </iframe>
</div><p>Just as in ANOVA, we can decompose the total variability in our response variable into meaningful components. This decomposition helps us assess how well our regression model explains the variation in the data.</p>
<section id="variance-decomposition-in-regression">
<h3>Variance Decomposition in Regression<a class="headerlink" href="#variance-decomposition-in-regression" title="Link to this heading"></a></h3>
<p>The key insight is that total variability can be split into two parts:</p>
<div class="math notranslate nohighlight">
\[\text{Total Variability} = \text{Explained by Regression} + \text{Unexplained (Error)}\]</div>
<p><strong>Sum of Squares Total (SST)</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SST} = \sum_{i=1}^n (y_i - \bar{y})^2\]</div>
<p>This measures how much the response values deviate from their overall mean, ignoring the explanatory variable.</p>
<p><strong>Sum of Squares Regression (SSR)</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SSR} = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2\]</div>
<p>This measures how much the fitted values deviate from the overall mean—the variability explained by the linear relationship.</p>
<p><strong>Sum of Squares Error (SSE)</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SSE} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n e_i^2\]</div>
<p>This measures how much the observed values deviate from the fitted line—the unexplained variability.</p>
<p><strong>The Fundamental Identity</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SST} = \text{SSR} + \text{SSE}\]</div>
<p>This can be proven algebraically by adding and subtracting <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> in the expression for SST and using properties of least squares.</p>
</section>
<section id="computational-shortcuts-for-sum-of-squares">
<h3>Computational Shortcuts for Sum of Squares<a class="headerlink" href="#computational-shortcuts-for-sum-of-squares" title="Link to this heading"></a></h3>
<p>While we could compute each sum of squares directly, there are more efficient computational formulas:</p>
<p><strong>For SSR</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SSR} = b_1 \cdot S_{xy}\]</div>
<p>This elegant result shows that <span class="math notranslate nohighlight">\(\text{SSR} = b_1 \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})\)</span>.</p>
<p><strong>For SST</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SST} = \sum_{i=1}^n y_i^2 - n\bar{y}^2\]</div>
<p><strong>For SSE</strong>:</p>
</section>
<section id="the-complete-anova-table">
<h3>The Complete ANOVA Table<a class="headerlink" href="#the-complete-anova-table" title="Link to this heading"></a></h3>
<table class="docutils align-default" id="id16">
<caption><span class="caption-number">Table 13.2 </span><span class="caption-text">ANOVA Table for Simple Linear Regression</span><a class="headerlink" href="#id16" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 19.0%" />
<col style="width: 14.3%" />
<col style="width: 23.8%" />
<col style="width: 19.0%" />
<col style="width: 14.3%" />
<col style="width: 9.5%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Source</p></th>
<th class="head"><p>df</p></th>
<th class="head"><p>Sum of Squares</p></th>
<th class="head"><p>Mean Square</p></th>
<th class="head"><p>F-statistic</p></th>
<th class="head"><p>p-value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Regression</p></td>
<td><p>1</p></td>
<td><p><span class="math notranslate nohighlight">\(\text{SSR} = b_1 \cdot S_{xy}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{MSR} = \text{SSR}/1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{MSR}/\text{MSE}\)</span></p></td>
<td><p>p-value</p></td>
</tr>
<tr class="row-odd"><td><p>Error</p></td>
<td><p><span class="math notranslate nohighlight">\(n-2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{SSE} = \text{SST} - \text{SSR}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{MSE} = \text{SSE}/(n-2)\)</span></p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Total</p></td>
<td><p><span class="math notranslate nohighlight">\(n-1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{SST} = \sum y_i^2 - n\bar{y}^2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\text{MST} = \text{SST}/(n-1)\)</span></p></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Degrees of Freedom</strong>:</p>
<ul class="simple">
<li><p><strong>Regression</strong>: 1 (we have one explanatory variable)</p></li>
<li><p><strong>Error</strong>: <span class="math notranslate nohighlight">\(n-2\)</span> (total sample size minus two estimated parameters)</p></li>
<li><p><strong>Total</strong>: <span class="math notranslate nohighlight">\(n-1\)</span> (total sample size minus one estimated mean)</p></li>
</ul>
<p><strong>Mean Squares</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{MSE} = \text{SSE}/(n-2)\)</span> estimates <span class="math notranslate nohighlight">\(\sigma^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{MSR} = \text{SSR}/1\)</span> measures explained variation per degree of freedom</p></li>
</ul>
</section>
</section>
<section id="the-coefficient-of-determination-r-squared">
<h2><span class="section-number">13.2.8. </span>The Coefficient of Determination (R-squared)<a class="headerlink" href="#the-coefficient-of-determination-r-squared" title="Link to this heading"></a></h2>
<p>The coefficient of determination, denoted <span class="math notranslate nohighlight">\(R^2\)</span>, provides a single numerical measure of how well our regression line fits the data.</p>
<section id="definition-and-calculation">
<h3>Definition and Calculation<a class="headerlink" href="#definition-and-calculation" title="Link to this heading"></a></h3>
<div class="math notranslate nohighlight">
\[R^2 = \frac{\text{SSR}}{\text{SST}} = \frac{\text{Variation Explained by Regression}}{\text{Total Variation}}\]</div>
<p><strong>Interpretation</strong>: <span class="math notranslate nohighlight">\(R^2\)</span> represents the <strong>fraction</strong> (or <strong>percentage</strong> when multiplied by 100) of the variation in the response variable that is explained by the least squares regression line.</p>
<p><strong>Range</strong>: <span class="math notranslate nohighlight">\(0 \leq R^2 \leq 1\)</span>
- <span class="math notranslate nohighlight">\(R^2 = 0\)</span>: The regression line explains none of the variation (horizontal line at <span class="math notranslate nohighlight">\(\bar{y}\)</span>)
- <span class="math notranslate nohighlight">\(R^2 = 1\)</span>: The regression line explains all the variation (all points lie exactly on the line)</p>
<p><strong>Alternative Formula</strong>:</p>
<p>This shows that <span class="math notranslate nohighlight">\(R^2\)</span> measures the <strong>proportion of variation NOT explained</strong> subtracted from 1.</p>
</section>
<section id="understanding-r-squared-through-examples">
<h3>Understanding R-squared Through Examples<a class="headerlink" href="#understanding-r-squared-through-examples" title="Link to this heading"></a></h3>
<p><strong>When is R-squared large?</strong></p>
<p><span class="math notranslate nohighlight">\(R^2\)</span> approaches 1 when <span class="math notranslate nohighlight">\(\text{SSR}\)</span> is large relative to <span class="math notranslate nohighlight">\(\text{SST}\)</span>. This happens when:</p>
<ul class="simple">
<li><p>The fitted values <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> are close to the observed values <span class="math notranslate nohighlight">\(y_i\)</span></p></li>
<li><p>The regression line captures most of the variability in the response</p></li>
<li><p>Points cluster tightly around the fitted line</p></li>
</ul>
<p><strong>When is R-squared small?</strong></p>
<p><span class="math notranslate nohighlight">\(R^2\)</span> approaches 0 when <span class="math notranslate nohighlight">\(\text{SSE}\)</span> is large relative to <span class="math notranslate nohighlight">\(\text{SST}\)</span>. This happens when:</p>
<ul class="simple">
<li><p>The fitted values <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> are close to <span class="math notranslate nohighlight">\(\bar{y}\)</span> (horizontal line)</p></li>
<li><p>The explanatory variable provides little information about the response</p></li>
<li><p>Points scatter widely around the fitted line</p></li>
</ul>
</section>
<section id="blood-pressure-example-computing-r-squared">
<h3>Blood Pressure Example: Computing R-squared<a class="headerlink" href="#blood-pressure-example-computing-r-squared" title="Link to this heading"></a></h3>
<p>Using our blood pressure data:</p>
<p><strong>SSR Calculation</strong>:</p>
<p><strong>SST Calculation</strong>:</p>
<p><strong>SSE Calculation</strong>:</p>
<p><strong>R-squared</strong>:</p>
<p><strong>Interpretation</strong>: Approximately 59.2% of the variation in blood pressure change is explained by the linear relationship with patient age.</p>
</section>
</section>
<section id="important-limitations-of-r-squared">
<h2><span class="section-number">13.2.9. </span>Important Limitations of R-squared<a class="headerlink" href="#important-limitations-of-r-squared" title="Link to this heading"></a></h2>
<p>While <span class="math notranslate nohighlight">\(R^2\)</span> is a useful summary measure, it has important limitations that require careful interpretation.</p>
<section id="r-squared-does-not-guarantee-linearity">
<h3>R-squared Does Not Guarantee Linearity<a class="headerlink" href="#r-squared-does-not-guarantee-linearity" title="Link to this heading"></a></h3>
<p>A high <span class="math notranslate nohighlight">\(R^2\)</span> value does not necessarily mean the relationship is truly linear. Consider a sinusoidal relationship where a linear line might still capture the average trend and yield a high <span class="math notranslate nohighlight">\(R^2\)</span>, even though the true relationship is clearly non-linear.</p>
<p><strong>Example</strong>: A curved relationship might have <span class="math notranslate nohighlight">\(R^2 = 0.90\)</span>, suggesting the linear model explains 90% of the variation. However, the residuals would show systematic patterns indicating that a non-linear model would be more appropriate.</p>
<p><strong>Lesson</strong>: Always examine scatter plots and residual plots, not just <span class="math notranslate nohighlight">\(R^2\)</span>.</p>
</section>
<section id="r-squared-is-not-robust-to-outliers">
<h3>R-squared is Not Robust to Outliers<a class="headerlink" href="#r-squared-is-not-robust-to-outliers" title="Link to this heading"></a></h3>
<p>Outliers can dramatically affect <span class="math notranslate nohighlight">\(R^2\)</span> in unexpected ways:</p>
<p><strong>Case 1: Outlier reduces R-squared</strong></p>
<p>A single extreme outlier in the response direction can make <span class="math notranslate nohighlight">\(\text{SST}\)</span> very large while having less impact on <span class="math notranslate nohighlight">\(\text{SSR}\)</span>, resulting in a misleadingly low <span class="math notranslate nohighlight">\(R^2\)</span> even when most points follow a clear linear pattern.</p>
<p><strong>Case 2: Outlier inflates R-squared</strong></p>
<p>An outlier that happens to fall near the regression line might not dramatically affect the fit but could mask the strength of the relationship among the remaining points.</p>
<p><strong>Lesson</strong>: Always identify and investigate outliers before interpreting <span class="math notranslate nohighlight">\(R^2\)</span>.</p>
</section>
<section id="high-r-squared-good-predictions">
<h3>High R-squared ≠ Good Predictions<a class="headerlink" href="#high-r-squared-good-predictions" title="Link to this heading"></a></h3>
<p>A high <span class="math notranslate nohighlight">\(R^2\)</span> does not automatically guarantee good prediction performance:</p>
<p><strong>Scale Matters</strong>: Consider a scenario where <span class="math notranslate nohighlight">\(\text{SSR} = 9,000,000\)</span> and <span class="math notranslate nohighlight">\(\text{SST} = 10,000,000\)</span>, giving <span class="math notranslate nohighlight">\(R^2 = 0.90\)</span>. While 90% of variation is explained, <span class="math notranslate nohighlight">\(\text{SSE} = 1,000,000\)</span> indicates substantial absolute errors that might make predictions unreliable for practical purposes.</p>
<p><strong>Missing Variables</strong>: High <span class="math notranslate nohighlight">\(R^2\)</span> with one explanatory variable might improve dramatically with additional relevant variables, indicating the current model, while good, is incomplete.</p>
<p><strong>Lesson</strong>: Consider both the proportion of variation explained and the absolute magnitude of unexplained variation.</p>
</section>
<section id="other-important-limitations">
<h3>Other Important Limitations<a class="headerlink" href="#other-important-limitations" title="Link to this heading"></a></h3>
<p><strong>Sample Size Effects</strong>: With small sample sizes, you might obtain high <span class="math notranslate nohighlight">\(R^2\)</span> values even when the true population relationship is weak or non-linear, simply because you happened to sample points that align well with a linear pattern.</p>
<p><strong>Extrapolation Risks</strong>: <span class="math notranslate nohighlight">\(R^2\)</span> describes fit within the range of observed data but provides no information about model performance outside this range.</p>
<p><strong>Assumption Violations</strong>: If regression assumptions (normality, equal variance, independence) are violated, <span class="math notranslate nohighlight">\(R^2\)</span> may not provide reliable information about model quality.</p>
<p><strong>Correlation vs. Causation</strong>: A high <span class="math notranslate nohighlight">\(R^2\)</span> indicates association but never implies causation. The explanatory variable might be correlated with the true causal factor without itself being causal.</p>
</section>
</section>
<section id="best-practices-for-using-r-squared">
<h2><span class="section-number">13.2.10. </span>Best Practices for Using R-squared<a class="headerlink" href="#best-practices-for-using-r-squared" title="Link to this heading"></a></h2>
<p>To use <span class="math notranslate nohighlight">\(R^2\)</span> effectively:</p>
<ol class="arabic simple">
<li><p><strong>Always examine scatter plots first</strong> to verify linear relationships</p></li>
<li><p><strong>Check for outliers</strong> and assess their impact on the analysis</p></li>
<li><p><strong>Consider the practical significance</strong> of unexplained variation, not just the percentage explained</p></li>
<li><p><strong>Use R-squared as one component</strong> of model assessment, not the sole criterion</p></li>
<li><p><strong>Verify model assumptions</strong> through residual analysis</p></li>
<li><p><strong>Be cautious with small sample sizes</strong> where high <span class="math notranslate nohighlight">\(R^2\)</span> might be misleading</p></li>
<li><p><strong>Avoid extrapolation</strong> beyond the range of observed data</p></li>
<li><p><strong>Remember that association ≠ causation</strong> regardless of <span class="math notranslate nohighlight">\(R^2\)</span> value</p></li>
</ol>
</section>
<section id="sample-pearson-correlation-coefficient">
<h2><span class="section-number">13.2.11. </span>Sample Pearson Correlation Coefficient<a class="headerlink" href="#sample-pearson-correlation-coefficient" title="Link to this heading"></a></h2>
<div class="video-placeholder" role="group" aria-labelledby="video-ch13-5">
   <iframe
      id="video-ch13-5"
      title="STAT 350 – Chapter 13.5 Sample Pearson Correlation Coefficient Video"
      src="https://www.youtube.com/embed/qSG28mV6fx4?list=PLHKEwTHXfbagA3ybKLAcEJriGT-6k89c6"
      allowfullscreen>
   </iframe>
</div><p>Another numerical measure that proves useful in simple linear regression—and will become even more valuable in advanced regression techniques—is the sample Pearson correlation coefficient. This measure provides a standardized way to quantify the linear association between our explanatory and response variables.</p>
<p>The sample Pearson correlation coefficient is simply a statistical measure of the strength and direction of a linear relationship. Given that we’ve established a linear relationship between two quantitative variables, it tells us how strongly they’re associated and in which direction.</p>
<p>The formula for the sample correlation coefficient is:</p>
<div class="math notranslate nohighlight">
\[r = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2} \sqrt{\sum_{i=1}^n (y_i - \bar{y})^2}}\]</div>
<p>If we divide both the numerator and denominator by <span class="math notranslate nohighlight">\(n-1\)</span>, we can rewrite this more compactly as:</p>
<div class="math notranslate nohighlight">
\[r = \frac{s_{xy}}{s_x s_y}\]</div>
<p>where <span class="math notranslate nohighlight">\(s_{xy}\)</span> is the sample covariance and <span class="math notranslate nohighlight">\(s_x\)</span>, <span class="math notranslate nohighlight">\(s_y\)</span> are the sample standard deviations of X and Y respectively.</p>
<p>The sample covariance <span class="math notranslate nohighlight">\(s_{xy}\)</span> tells us about the relationship between X and Y. For observed pairs <span class="math notranslate nohighlight">\((x_i, y_i)\)</span>, when X tends to be above its mean, what does Y tend to do? Does it tend to be above its mean as well (indicating a positive association), or does it tend to be below its mean (indicating a negative association)? We divide by the standard deviations to get a standardized measure.</p>
<p>This quantity is estimating the population correlation <span class="math notranslate nohighlight">\(\rho\)</span>, which we saw earlier in the probability chapters:</p>
<div class="math notranslate nohighlight">
\[\rho = \frac{E[(X - \mu_X)(Y - \mu_Y)]}{\sigma_X \sigma_Y}\]</div>
<p>We’re estimating the population covariance with the sample covariance, and the population standard deviations with their sample versions.</p>
<p>Since the denominator consists only of constants, we can bring everything up into the numerator to give a different form:</p>
<div class="math notranslate nohighlight">
\[r = \frac{1}{n-1} \sum_{i=1}^n \left(\frac{x_i - \bar{x}}{s_x}\right)\left(\frac{y_i - \bar{y}}{s_y}\right)\]</div>
<p>This gives us insight into what the correlation coefficient is really doing. Notice that <span class="math notranslate nohighlight">\(\frac{x_i - \bar{x}}{s_x}\)</span> is just the standardized form of <span class="math notranslate nohighlight">\(x_i\)</span>—a z-score telling us how many standard deviations the observation <span class="math notranslate nohighlight">\(x_i\)</span> is away from <span class="math notranslate nohighlight">\(\bar{x}\)</span>. Similarly for the Y values. We’re looking at the average product of these standardized values across all observations.</p>
<p>Since standardized units are unitless measures, <span class="math notranslate nohighlight">\(r\)</span> is unitless, making it easy to compare the strength and direction of relationships between different quantitative variables regardless of their original units.</p>
<p>This correlation coefficient has a useful property: it always falls between -1 and +1. You can prove this using the Cauchy-Schwarz inequality, though we won’t cover that proof here. This bounded range makes it easy to compare associations across different variable pairs.</p>
<p>To interpret correlation strength, we use these rules of thumb:</p>
<ul class="simple">
<li><p>Strong negative association: -1 to -0.8</p></li>
<li><p>Moderate negative association: -0.8 to -0.5</p></li>
<li><p>Weak association: -0.5 to +0.5</p></li>
<li><p>Moderate positive association: +0.5 to +0.8</p></li>
<li><p>Strong positive association: +0.8 to +1</p></li>
</ul>
<p>The sign clearly indicates direction. Anything close to 0 indicates very little linear association between X and Y.</p>
<figure class="align-default" id="id17">
<a class="reference internal image-reference" href="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-correlation-visual-examples.png"><img alt="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-correlation-visual-examples.png" src="https://yjjpfnblgtrogqvcjaon.supabase.co/storage/v1/object/public/stat-350-assets/images/chapter13/slide-correlation-visual-examples.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13.30 </span><span class="caption-text">Visual representation of different correlation strengths showing downward trends for negative correlations and upward trends for positive correlations</span><a class="headerlink" href="#id17" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>When <span class="math notranslate nohighlight">\(r = 0\)</span>, it indicates no linear association, but this can happen in several ways. Sometimes there’s genuinely no relationship—the points look like a perfect circle scattered randomly. But zero correlation can also occur with strong nonlinear relationships that are symmetric around the means. For instance, a perfect circle, a U-shaped curve, or clustered patterns might all yield <span class="math notranslate nohighlight">\(r = 0\)</span> despite clear associations being present.</p>
<p>The key insight is that correlation specifically measures <strong>linear</strong> association. Symmetry tends to produce zero correlation because in some regions where X is above its mean, Y tends to be above its mean, while in other regions the opposite occurs, and these effects cancel out in the averaging process.</p>
<section id="connection-between-correlation-and-slope">
<h3>Connection Between Correlation and Slope<a class="headerlink" href="#connection-between-correlation-and-slope" title="Link to this heading"></a></h3>
<p>Since both the correlation coefficient and the regression slope capture information about the relationship between X and Y, they should be related—and indeed they are.</p>
<p>Recall that our slope formula is:</p>
<div class="math notranslate nohighlight">
\[b_1 = \frac{s_{xy}}{s_x^2}\]</div>
<p>From our correlation formula <span class="math notranslate nohighlight">\(r = \frac{s_{xy}}{s_x s_y}\)</span>, we can solve for the sample covariance:</p>
<p>Substituting this into our slope formula:</p>
<p>This elegant relationship shows that our slope can be expressed as the correlation coefficient rescaled by the ratio of standard deviations. The correlation provides the unitless measure of strength and direction, while the ratio <span class="math notranslate nohighlight">\(s_y/s_x\)</span> converts this into the actual units of our data.</p>
</section>
<section id="connection-to-r-squared">
<h3>Connection to R-squared<a class="headerlink" href="#connection-to-r-squared" title="Link to this heading"></a></h3>
<p>There’s also a direct mathematical relationship between the correlation coefficient and our coefficient of determination. Recall that:</p>
<p>and <span class="math notranslate nohighlight">\(\text{SST} = S_{yy}\)</span>. Therefore:</p>
<p>If we multiply by <span class="math notranslate nohighlight">\(\frac{1/(n-1)}{1/(n-1)}\)</span>, this becomes:</p>
<p>So in simple linear regression, <span class="math notranslate nohighlight">\(R^2 = r^2\)</span>. This means if we know one, we can determine the other (though we need the sign of the slope to determine whether <span class="math notranslate nohighlight">\(r\)</span> is positive or negative).</p>
<p>This relationship only holds for simple linear regression with one explanatory variable. In multiple regression, <span class="math notranslate nohighlight">\(R^2\)</span> assesses the overall model utility, while individual correlation coefficients measure pairwise associations between specific variables.</p>
</section>
<section id="limitations-of-correlation">
<h3>Limitations of Correlation<a class="headerlink" href="#limitations-of-correlation" title="Link to this heading"></a></h3>
<p>Just like <span class="math notranslate nohighlight">\(R^2\)</span>, the correlation coefficient has important limitations:</p>
<ol class="arabic simple">
<li><p><strong>Quantitative variables only</strong>: Correlation measures linear association between two quantitative variables exclusively.</p></li>
<li><p><strong>Linearity required</strong>: Correlation only measures linear relationships effectively. Strong nonlinear relationships can produce correlations near zero.</p></li>
<li><p><strong>Not robust to outliers</strong>: Extreme observations can significantly impact correlation values.</p></li>
<li><p><strong>Incomplete information</strong>: Correlation provides a summary number but doesn’t give complete information about the association. You must combine numerical assessment with visual assessment through scatter plots.</p></li>
</ol>
<p>These limitations mean we need to examine our data visually before trusting correlation values, just as we do with <span class="math notranslate nohighlight">\(R^2\)</span>. The correlation coefficient is useful for quantifying linear association strength and direction, but only after we’ve verified that a linear relationship is indeed appropriate.</p>
</section>
</section>
<section id="moving-forward-from-description-to-inference">
<h2><span class="section-number">13.2.12. </span>Moving Forward: From Description to Inference<a class="headerlink" href="#moving-forward-from-description-to-inference" title="Link to this heading"></a></h2>
<p>The tools developed in this chapter—the regression model, least squares estimation, ANOVA decomposition,
<span class="math notranslate nohighlight">\(R^2\)</span>, and correlation analysis—provide the foundation for statistical inference in regression. We can now:</p>
<ul class="simple">
<li><p><strong>Estimate relationships</strong> between quantitative variables</p></li>
<li><p><strong>Assess model fit</strong> using multiple criteria</p></li>
<li><p><strong>Decompose variation</strong> into explained and unexplained components</p></li>
<li><p><strong>Quantify strength of association</strong> through both <span class="math notranslate nohighlight">\(R^2\)</span> and <span class="math notranslate nohighlight">\(r\)</span></p></li>
<li><p><strong>Compare standardized and unstandardized measures</strong> of association</p></li>
</ul>
<p><strong>What’s Next</strong>: In subsequent sections, we’ll develop:</p>
<ul class="simple">
<li><p><strong>Model diagnostics</strong> for checking regression assumptions</p></li>
<li><p><strong>Hypothesis tests</strong> for slope and intercept parameters</p></li>
<li><p><strong>Confidence intervals</strong> for regression parameters and predictions</p></li>
<li><p><strong>Prediction intervals</strong> for new observations</p></li>
<li><p><strong>Robustness considerations</strong> for when assumptions are violated</p></li>
</ul>
<p>The progression follows our familiar pattern: establish the model, estimate parameters, assess fit,
quantify associations, then conduct formal inference with proper uncertainty quantification.</p>
<div class="important admonition">
<p class="admonition-title">Key Takeaways 📝</p>
<ol class="arabic simple">
<li><p><strong>The simple linear regression model</strong> <span class="math notranslate nohighlight">\(Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i\)</span> formalizes the linear relationship between quantitative variables with clear assumptions about error terms.</p></li>
<li><p><strong>Least squares estimation</strong> provides optimal estimates for slope and intercept by minimizing the sum of squared residuals, with formulas based on sample covariance and variance.</p></li>
<li><p><strong>The regression line always passes through</strong> <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span> and provides average changes in response per unit change in explanatory variable.</p></li>
<li><p><strong>ANOVA decomposition</strong> splits total variation into explained (SSR) and unexplained (SSE) components, with degrees of freedom that sum appropriately.</p></li>
<li><p><strong>The coefficient of determination</strong> <span class="math notranslate nohighlight">\(R^2 = \text{SSR}/\text{SST}\)</span> measures the proportion of variation explained by the regression, ranging from 0 to 1.</p></li>
<li><p><strong>R-squared has important limitations</strong>: it doesn’t guarantee linearity, isn’t robust to outliers, and doesn’t automatically ensure good predictions.</p></li>
<li><p><strong>Model assessment requires multiple tools</strong>: scatter plots, residual analysis, <span class="math notranslate nohighlight">\(R^2\)</span>, and assumption checking work together to evaluate model appropriateness.</p></li>
<li><p><strong>Residuals</strong> <span class="math notranslate nohighlight">\(e_i = y_i - \hat{y}_i\)</span> provide estimates of error terms and enable variance estimation through <span class="math notranslate nohighlight">\(s^2 = \text{SSE}/(n-2)\)</span>.</p></li>
<li><p><strong>Computational shortcuts</strong> like <span class="math notranslate nohighlight">\(\text{SSR} = b_1 \cdot S_{xy}\)</span> make ANOVA calculations efficient while maintaining conceptual clarity.</p></li>
<li><p><strong>Parameter interpretation requires context</strong>: the slope represents average change per unit increase in X, while the intercept may or may not have practical meaning depending on whether X = 0 is meaningful.</p></li>
</ol>
</div>
<section id="exercises">
<h3>Exercises<a class="headerlink" href="#exercises" title="Link to this heading"></a></h3>
<ol class="arabic">
<li><p><strong>Model Components and Assumptions</strong>: For the regression model <span class="math notranslate nohighlight">\(Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i\)</span>:</p>
<ol class="loweralpha simple">
<li><p>Explain what each component represents in the context of studying the relationship between hours studied and exam scores</p></li>
<li><p>List the four key assumptions and explain why each is important</p></li>
<li><p>What does it mean for the error terms to be “iid normal”?</p></li>
<li><p>How would you check each assumption using real data?</p></li>
</ol>
</li>
<li><p><strong>Least Squares Calculation</strong>: Given the following data on house size (X, in 1000 sq ft) and price (Y, in <a href="#id2"><span class="problematic" id="id3">`</span></a>1000s):</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>House</p></th>
<th class="head"><p>Size (X)</p></th>
<th class="head"><p>Price (Y)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>1.2</p></td>
<td><p>180</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>1.8</p></td>
<td><p>220</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>2.1</p></td>
<td><p>280</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>1.5</p></td>
<td><p>200</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>2.4</p></td>
<td><p>320</p></td>
</tr>
</tbody>
</table>
<ol class="loweralpha simple">
<li><p>Calculate <span class="math notranslate nohighlight">\(\bar{x}\)</span>, <span class="math notranslate nohighlight">\(\bar{y}\)</span>, <span class="math notranslate nohighlight">\(\sum x_i y_i\)</span>, and <span class="math notranslate nohighlight">\(\sum x_i^2\)</span></p></li>
<li><p>Find the least squares estimates <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span></p></li>
<li><p>Write the fitted regression equation</p></li>
<li><p>Interpret the slope and intercept in context</p></li>
<li><p>Verify that the line passes through <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span></p></li>
</ol>
</li>
<li><p><strong>ANOVA Table Construction</strong>: Using the house price data from Exercise 2:</p>
<ol class="loweralpha simple">
<li><p>Calculate SST, SSR, and SSE</p></li>
<li><p>Complete the ANOVA table with degrees of freedom and mean squares</p></li>
<li><p>Calculate <span class="math notranslate nohighlight">\(R^2\)</span> and interpret its meaning</p></li>
<li><p>Estimate the error standard deviation <span class="math notranslate nohighlight">\(s\)</span></p></li>
</ol>
</li>
<li><p><strong>Residual Analysis</strong>: For the house price regression:</p>
<ol class="loweralpha simple">
<li><p>Calculate the fitted value and residual for House 3</p></li>
<li><p>What does this residual tell you about the model’s performance for this house?</p></li>
<li><p>If all residuals were positive, what would this suggest about the model?</p></li>
</ol>
</li>
<li><p><strong>R-squared Interpretation</strong>: Consider three different regression analyses:</p>
<p><strong>Scenario A</strong>: Predicting height from shoe size with <span class="math notranslate nohighlight">\(R^2 = 0.85\)</span>, <span class="math notranslate nohighlight">\(\text{SSE} = 25\)</span></p>
<p><strong>Scenario B</strong>: Predicting income from education with <span class="math notranslate nohighlight">\(R^2 = 0.65\)</span>, <span class="math notranslate nohighlight">\(\text{SSE} = 10,000,000\)</span></p>
<p><strong>Scenario C</strong>: Predicting test scores from study hours with <span class="math notranslate nohighlight">\(R^2 = 0.40\)</span>, <span class="math notranslate nohighlight">\(\text{SSE} = 100\)</span></p>
<ol class="loweralpha simple">
<li><p>Which scenario shows the strongest linear relationship?</p></li>
<li><p>Which scenario might be most useful for prediction? Explain your reasoning</p></li>
<li><p>What additional information would help you better evaluate these models?</p></li>
</ol>
</li>
<li><p><strong>Computational Formulas</strong>: Show algebraically that:</p>
<ol class="loweralpha simple">
<li><p><span class="math notranslate nohighlight">\(S_{xy} = \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) = \sum_{i=1}^n x_i y_i - n\bar{x}\bar{y}\)</span></p></li>
<li><p>The regression line always passes through <span class="math notranslate nohighlight">\((\bar{x}, \bar{y})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{SST} = \text{SSR} + \text{SSE}\)</span> (hint: add and subtract <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> in the SST expression)</p></li>
</ol>
</li>
<li><p><strong>Model Comparison</strong>: Two students fit different models to the same dataset:</p>
<p><strong>Student A</strong>: <span class="math notranslate nohighlight">\(\hat{y} = 12 + 3.5x\)</span> with <span class="math notranslate nohighlight">\(R^2 = 0.78\)</span>, <span class="math notranslate nohighlight">\(s = 4.2\)</span></p>
<p><strong>Student B</strong>: <span class="math notranslate nohighlight">\(\hat{y} = 8 + 4.1x\)</span> with <span class="math notranslate nohighlight">\(R^2 = 0.81\)</span>, <span class="math notranslate nohighlight">\(s = 3.9\)</span></p>
<ol class="loweralpha simple">
<li><p>If both used the same data and method, why might they get different results?</p></li>
<li><p>Which model appears better based on the given information?</p></li>
<li><p>What could cause such different results from the same dataset?</p></li>
</ol>
</li>
<li><p><strong>Assumption Violations</strong>: For each scenario, identify which regression assumption might be violated and explain the potential consequences:</p>
<ol class="loweralpha simple">
<li><p>Studying crop yield vs. rainfall, but some fields received fertilizer and others didn’t</p></li>
<li><p>Predicting salary from years of experience, but the data includes both part-time and full-time workers</p></li>
<li><p>Analyzing test scores vs. study time, but students were allowed to collaborate</p></li>
<li><p>Examining house prices vs. size, but the data spans both urban and rural areas</p></li>
</ol>
</li>
<li><p><strong>Practical Applications</strong>: A researcher studying the relationship between advertising spend (X, in <cite>1000s) and sales (Y, in `1000s) obtains:
- :math:</cite>hat{y} = 42 + 3.2x`
- <span class="math notranslate nohighlight">\(R^2 = 0.67\)</span>
- <span class="math notranslate nohighlight">\(s = 15\)</span>
- <span class="math notranslate nohighlight">\(n = 25\)</span></p>
<ol class="loweralpha simple">
<li><p>Interpret the slope and intercept in business terms</p></li>
<li><p>What does the <span class="math notranslate nohighlight">\(R^2\)</span> value tell management about the advertising-sales relationship?</p></li>
<li><p>If the company spends $8,000 on advertising, what sales level does the model predict?</p></li>
<li><p>Given <span class="math notranslate nohighlight">\(s = 15\)</span>, how much confidence should management have in predictions?</p></li>
</ol>
</li>
<li><p><strong>Critical Thinking</strong>: A news article claims: “Study finds strong relationship between ice cream sales and drowning deaths (R² = 0.89). Ice cream consumption causes drowning!”</p>
<ol class="loweralpha simple">
<li><p>What statistical concept is being misunderstood in this claim?</p></li>
<li><p>Explain what the high <span class="math notranslate nohighlight">\(R^2\)</span> actually tells us</p></li>
<li><p>What might be a more plausible explanation for this relationship?</p></li>
<li><p>How could you design a study to investigate causation rather than just association?</p></li>
</ol>
</li>
<li><p><strong>Advanced Calculation</strong>: Using the computational formula <span class="math notranslate nohighlight">\(\text{SSR} = b_1 \cdot S_{xy}\)</span>:</p>
<ol class="loweralpha simple">
<li><p>Prove that <span class="math notranslate nohighlight">\(\text{SSR}\)</span> is always non-negative</p></li>
<li><p>Explain why <span class="math notranslate nohighlight">\(b_1\)</span> and <span class="math notranslate nohighlight">\(S_{xy}\)</span> always have the same sign</p></li>
<li><p>Under what conditions would <span class="math notranslate nohighlight">\(\text{SSR} = 0\)</span>?</p></li>
</ol>
</li>
<li><p><strong>Real Data Analysis</strong>: Collect data on a topic of interest (e.g., study hours vs. GPA, car age vs. price, etc.) with at least 10 observations:</p>
<ol class="loweralpha simple">
<li><p>Create a scatter plot and assess whether linear regression is appropriate</p></li>
<li><p>Calculate the least squares regression line by hand</p></li>
<li><p>Construct the ANOVA table and calculate <span class="math notranslate nohighlight">\(R^2\)</span></p></li>
<li><p>Interpret your results in context</p></li>
<li><p>Identify any potential outliers or assumption violations</p></li>
<li><p>Discuss the practical implications of your findings</p></li>
</ol>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="13-1-intro-to-lr-correlation-scatter-plots.html" class="btn btn-neutral float-left" title="13.1. Introduction to Linear Regression: Correlation and Scatter Plots" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="13-3-diagnostics-inference.html" class="btn btn-neutral float-right" title="13.3. Model Diagnostics and Statistical Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>