

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images &mdash; Fitting SMPL to IMU Optimization</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/VirtualHumans/lecture_06_2SMPL_model_fitting.html" />
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=f2a433a1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html" />
    <link rel="prev" title="Lecture 06.1 - Fitting the SMPL Model to Images via Optimization" href="lecture_06_1_SMPL_optimization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Virtual Humans Lecture
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_historical_body_models.html">Lecture 01.1 – Historical Body Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#early-origins-simplified-primitives-and-kinematic-skeletons-1970s1980s">Early Origins: Simplified Primitives and Kinematic Skeletons (1970s–1980s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#advances-in-the-1990s-superquadrics-differentiable-fitting-and-physical-models">Advances in the 1990s: Superquadrics, Differentiable Fitting, and Physical Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#the-impact-of-3d-scanning-and-data-from-anthropometry-to-statistical-models-1990s2000s">The Impact of 3D Scanning and Data: From Anthropometry to Statistical Models (1990s–2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#scape-and-the-emergence-of-pose-aware-models-mid-2000s">SCAPE and the Emergence of Pose-Aware Models (Mid-2000s)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#consolidation-in-the-2010s-smpl-and-integration-with-learning-based-methods">Consolidation in the 2010s: SMPL and Integration with Learning-Based Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#deep-learning-and-neural-implicit-models-late-2010spresent">Deep Learning and Neural Implicit Models (Late 2010s–Present)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#timeline-summary-of-milestones">Timeline Summary of Milestones</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_historical_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html">Lecture 01.2 – Introduction to Human Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#historical-context-of-human-body-modeling">1. Historical Context of Human Body Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#early-developments">Early Developments</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mid-20th-century-approaches">Mid-20th Century Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#data-driven-revolution-1990s-2000s">Data-Driven Revolution (1990s-2000s)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#mathematical-foundations-of-human-body-models">2. Mathematical Foundations of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#the-smpl-model">The SMPL Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#pca-based-statistical-shape-modeling">PCA-Based Statistical Shape Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#applications-of-human-body-models">3. Applications of Human Body Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-animation-and-visual-effects">Computer Animation and Visual Effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#virtual-humans-and-avatars">Virtual Humans and Avatars</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#biomechanics-and-ergonomics">Biomechanics and Ergonomics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#human-computer-interaction-hci">Human-Computer Interaction (HCI)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computer-vision-and-ai">Computer Vision and AI</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#education-and-training">Education and Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#challenges-and-future-directions">4. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#computational-efficiency">Computational Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#accuracy-and-detail">Accuracy and Detail</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#generalization">Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#clothing-and-accessories">Clothing and Accessories</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#emerging-approaches">Emerging Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_introduction_to_human_models.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html">Lecture 01.3 – Introduction to Human Models (Overview)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#historical-context">1. Historical Context</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#early-scientific-studies">Early Scientific Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mid-20th-century-to-digital-era">Mid-20th Century to Digital Era</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#st-century-advances">21st Century Advances</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#mathematical-foundations">2. Mathematical Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#parametric-body-models">Parametric Body Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-surface-representations">Implicit Surface Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#kinematic-modeling">Kinematic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#image-formation-and-rendering">3. Image Formation and Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#camera-models">Camera Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#shading-and-visibility">Shading and Visibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#differentiable-rendering">Differentiable Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#surface-representation-methods">4. Surface Representation Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#explicit-mesh-models">Explicit Mesh Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-function-models">Implicit Function Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#motion-capture-and-behavior-synthesis">5. Motion Capture and Behavior Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#capturing-human-motion">Capturing Human Motion</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#behavior-synthesis">Behavior Synthesis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#clothing-modeling">6. Clothing Modeling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physically-based-simulation">Physically-Based Simulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-driven-approaches">Data-Driven Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#implicit-clothing-models">Implicit Clothing Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-object-interaction">7. Human-Object Interaction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-based-methods">Physics-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#learning-based-approaches">Learning-Based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#hybrid-systems">Hybrid Systems</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#applications">8. Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#entertainment-and-media">Entertainment and Media</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#healthcare-and-biomechanics">Healthcare and Biomechanics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#engineering-and-design">Engineering and Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#human-computer-interaction">Human-Computer Interaction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scientific-research">Scientific Research</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#challenges-and-future-directions">9. Challenges and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#scalability-and-generalization">Scalability and Generalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#higher-fidelity-dynamics">Higher-Fidelity Dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#data-and-labeling-constraints">Data and Labeling Constraints</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#physics-and-learning-integration">Physics and Learning Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#semantic-and-cognitive-aspects">Semantic and Cognitive Aspects</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_01_3_introduction_to_human_models_continued.html#realism-vs-controllability">Realism vs. Controllability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_image_formation.html">Lecture 02.1 – Image Formation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#historical-developments-in-image-formation">1. Historical Developments in Image Formation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#ancient-and-medieval-optics-camera-obscura">Ancient and Medieval Optics – Camera Obscura</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#renaissance-perspective-and-geometry">Renaissance Perspective and Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#early-cameras-and-photographic-imaging">Early Cameras and Photographic Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#modern-developments">Modern Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#the-pinhole-camera-model">2. The Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#coordinate-setup">Coordinate Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#proof-by-similar-triangles">Proof by Similar Triangles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#numerical-example">Numerical Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#inadequacy-of-a-simple-pinhole">Inadequacy of a Simple Pinhole</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#camera-intrinsics-and-the-projection-matrix">3. Camera Intrinsics and the Projection Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#extrinsic-parameters">Extrinsic Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#full-projection-example">Full Projection Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#image-distortions-correction">4. Image Distortions &amp; Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#properties-of-perspective-projection">5. Properties of Perspective Projection</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#advanced-theoretical-extensions">6. Advanced Theoretical Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#light-field-imaging-and-plenoptic-cameras">Light Field Imaging and Plenoptic Cameras</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#non-conventional-imaging-techniques">Non-Conventional Imaging Techniques</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#applications-in-modern-vision-and-graphics">7. Applications in Modern Vision and Graphics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#computer-vision-and-3d-reconstruction">Computer Vision and 3D Reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#medical-imaging">Medical Imaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_1_image_formation.html#photorealistic-rendering-in-computer-graphics">Photorealistic Rendering in Computer Graphics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_image_formation.html#python-example-simulating-image-formation">8. Python Example: Simulating Image Formation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html">Lecture 02.2 – Rotations and Kinematic Chains</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#representations-of-3d-rotations">1. Representations of 3D Rotations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#a-rotation-matrices">A) Rotation Matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#b-euler-angles">B) Euler Angles</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#c-quaternions">C) Quaternions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#lie-algebra-so-3-and-exponential-map">2. Lie Algebra <span class="math notranslate nohighlight">\(so(3)\)</span> and Exponential Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#rodrigues-rotation-formula">3. Rodrigues’ Rotation Formula</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#kinematic-chains-forward-inverse-kinematics">4. Kinematic Chains: Forward &amp; Inverse Kinematics</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_rotations_kinematic_chains.html#comparison-of-rotation-representations">Comparison of Rotation Representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_1_surface_representations.html">Lecture 03.1 – Surface Representations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#mathematical-foundations-of-surface-representations">1. Mathematical Foundations of Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-parametric-surfaces">A) Parametric Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surfaces">B) Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-explicit-surfaces">C) Explicit Surfaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#surface-differential-properties">2. Surface Differential Properties</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-surface-normals">A) Surface Normals</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-fundamental-forms-and-curvature">B) Fundamental Forms and Curvature</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-geodesics">C) Geodesics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#discrete-surface-representations">3. Discrete Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-polygon-meshes">A) Polygon Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-point-clouds">B) Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-signed-distance-fields-sdf">C) Signed Distance Fields (SDF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-surface-representations">4. Advanced Surface Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-bezier-curves-and-surfaces">A) Bézier Curves and Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-b-splines-and-nurbs">B) B-Splines and NURBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-subdivision-surfaces">C) Subdivision Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-level-sets">D) Level Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#e-neural-implicit-representations">E) Neural Implicit Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#comparative-analysis-and-applications">5. Comparative Analysis and Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-computational-efficiency-and-storage">A) Computational Efficiency and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-practical-applications">B) Practical Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-operations-complexity">C) Operations Complexity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#implementation-examples">6. Implementation Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-basic-mesh-processing-python">A) Basic Mesh Processing (Python)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-implicit-surface-utilities">B) Implicit Surface Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-bezier-curve-implementation">C) Bézier Curve Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-curvature-estimation-on-meshes">D) Curvature Estimation on Meshes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_1_surface_representations.html#advanced-topics-and-future-directions">7. Advanced Topics and Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#a-multi-resolution-representations">A) Multi-Resolution Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#b-machine-learning-for-geometry">B) Machine Learning for Geometry</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#c-dynamic-surfaces">C) Dynamic Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_1_surface_representations.html#d-non-manifold-geometries">D) Non-Manifold Geometries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html">Lecture 03.2 – Procrustes Alignment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#goal-learning-a-model-of-pose-and-shape">Goal: Learning a Model of Pose and Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-challenge-of-registration">The Challenge of Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#surface-representation-mesh">Surface Representation: Mesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#the-procrustes-alignment-problem-mathematical-formulation">The Procrustes Alignment Problem: Mathematical Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rigid-transformations">Rigid Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#procrustes-alignment-solution">Procrustes Alignment Solution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#decoupling-translation-by-centroid-alignment">Decoupling Translation by Centroid Alignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-rotation-via-svd">Optimal Rotation via SVD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#reflection-adjustment">Reflection Adjustment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#optimal-scale-optional">Optimal Scale (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#complete-mathematical-derivation">Complete Mathematical Derivation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#translation-derivation">Translation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#rotation-derivation">Rotation Derivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#scale-derivation">Scale Derivation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#summary-of-procrustes-alignment-algorithm">Summary of Procrustes Alignment Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#python-implementation-example">Python Implementation Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_03_2_procrustes_alignment.html#interactive-visualization-ideas">Interactive Visualization Ideas</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_1_icp.html">Lecture 4.1: Iterative Closest Point</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#introduction-to-shape-alignment-and-registration">Introduction to Shape Alignment and Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#the-registration-problem">The Registration Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#review-procrustes-analysis">Review: Procrustes Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#problem-unknown-correspondences">Problem: Unknown Correspondences</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#basic-icp-algorithm">Basic ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computational-considerations">Computational Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#closest-point-computation">Closest Point Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="lecture_04_1_icp.html#convergence-and-local-minima">Convergence and Local Minima</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-vs-point-to-plane-icp">Point-to-Point vs. Point-to-Plane ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-point-icp">Point-to-Point ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#point-to-plane-icp">Point-to-Plane ICP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-for-non-rigid-registration">Gradient-based ICP for Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#gradient-based-icp-algorithm">Gradient-based ICP Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#advantages-of-gradient-based-icp">Advantages of Gradient-based ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#computing-gradients">Computing Gradients</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#improving-icp-s-robustness">Improving ICP’s Robustness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#data-association-direction">Data Association Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#robust-cost-functions">Robust Cost Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#trimmed-icp">Trimmed ICP</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#ransac-based-approaches">RANSAC-based Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#additional-information">Additional Information</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#icp-variants-and-extensions">ICP Variants and Extensions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#generalized-icp-gicp">Generalized ICP (GICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#em-icp-and-probabilistic-approaches">EM-ICP and Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#multi-scale-approaches">Multi-Scale Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#applications-of-icp">Applications of ICP</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#implementing-icp">Implementing ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#efficient-python-implementation">Efficient Python Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_1_icp.html#practical-tips">Practical Tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_1_icp.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_04_2_body_models.html">Lecture 04.2 - Body Models: Vertex-Based Models and SMPL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#body-models-as-parameterized-functions">1. Body Models as Parameterized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#rotations-articulation-and-pose-representation">2. Rotations, Articulation, and Pose Representation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#rotation-representation">2.1 Rotation Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#kinematic-chain">2.2 Kinematic Chain</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning-and-its-limitations">3. Linear Blend Skinning and its Limitations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#linear-blend-skinning">3.1 Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#problems-with-standard-lbs">3.2 Problems with Standard LBS</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#blend-shapes-for-correcting-lbs-artifacts">3.3 Blend Shapes for Correcting LBS Artifacts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#the-smpl-body-model">4. The SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-philosophy">4.1 SMPL Philosophy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#smpl-model-architecture">4.2 SMPL Model Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#shape-blend-shapes">4.2.1 Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#pose-blend-shapes">4.2.2 Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#joint-regression">4.2.3 Joint Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#model-training">4.3 Model Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#comparison-with-scape">5. Comparison with SCAPE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-scape-model">5.1 The SCAPE Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#different-approaches-to-deformation">5.2 Different Approaches to Deformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#performance-comparison">5.3 Performance Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#other-advantages">5.4 Other Advantages</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#alignment-techniques-procrustes-analysis-and-icp">6. Alignment Techniques: Procrustes Analysis and ICP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#procrustes-analysis">6.1 Procrustes Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#iterative-closest-point-icp">6.2 Iterative Closest Point (ICP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#fitting-smpl-to-scans">6.3 Fitting SMPL to Scans</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#image-formation-and-the-pinhole-camera-model">7. Image Formation and the Pinhole Camera Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#the-pinhole-camera-model">7.1 The Pinhole Camera Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#lens-distortion">7.2 Lens Distortion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#extensions-and-advanced-applications">8. Extensions and Advanced Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#dynamic-soft-tissue-modeling">8.1 Dynamic Soft Tissue Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#specialized-extensions">8.2 Specialized Extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#deep-learning-for-model-fitting">8.3 Deep Learning for Model Fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#probabilistic-approaches">8.4 Probabilistic Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_04_2_body_models.html#hybrid-models">8.5 Hybrid Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_04_2_body_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_1_body_model_training.html">Lecture 5.1 - Training a Body Model and Fitting SMPL to Scans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#body-models-based-on-triangle-deformations">Body Models Based on Triangle Deformations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#scape-and-blendscape-models">SCAPE and BlendSCAPE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#triangle-deformation-process">Triangle Deformation Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#comparison-smpl-vs-scape-blendscape">Comparison: SMPL vs. SCAPE/BlendSCAPE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-a-body-model-from-registrations">Training a Body Model from Registrations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#the-challenge-of-raw-scan-data">The Challenge of Raw Scan Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#training-from-registrations">Training from Registrations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#obtaining-registrations-fitting-smpl-to-scans">Obtaining Registrations: Fitting SMPL to Scans</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#non-rigid-registration-process">Non-Rigid Registration Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#iterative-closest-point-icp-review">Iterative Closest Point (ICP) Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#registration-objective-formulation">Registration Objective Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#point-to-surface-distance">Point-to-Surface Distance</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#multi-stage-optimization-strategy">Multi-Stage Optimization Strategy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#joint-registration-and-model-training">Joint Registration and Model Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_1_body_model_training.html#co-registration-approach">Co-Registration Approach</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_1_body_model_training.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_05_2_3d_registration.html">Lecture 05.2 - 3D Registration: From Classical ICP to Modern Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#rigid-registration-and-the-icp-algorithm">1. Rigid Registration and the ICP Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-iterative-closest-point-icp-algorithm">The Iterative Closest Point (ICP) Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#convergence-analysis-and-failure-modes">Convergence Analysis and Failure Modes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#classical-non-rigid-registration">2. Classical Non-Rigid Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#thin-plate-spline-robust-point-matching-tps-rpm">Thin Plate Spline Robust Point Matching (TPS-RPM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#coherent-point-drift-cpd">Coherent Point Drift (CPD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#other-non-rigid-methods">Other Non-Rigid Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#parametric-models-and-the-smpl-body-model">3. Parametric Models and the SMPL Body Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#model-structure">Model Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#shape-blend-shapes-identity-variation">Shape Blend Shapes (Identity Variation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#pose-blend-shapes-pose-dependent-deformation">Pose Blend Shapes (Pose-Dependent Deformation)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#linear-blend-skinning-lbs-for-articulation">Linear Blend Skinning (LBS) for Articulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#learning-smpl">Learning SMPL</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#using-smpl-for-registration">Using SMPL for Registration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#modeling-clothing-and-fine-details-smpl-d">4. Modeling Clothing and Fine Details: SMPL+D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#why-smpl-d">Why SMPL+D?</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#how-displacements-are-applied">How Displacements Are Applied</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#applications">Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#survey-of-3d-registration-methods-from-icp-to-deep-learning">5. Survey of 3D Registration Methods: From ICP to Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#early-pioneering-works-1990s-foundational-rigid-registration">5.1 Early Pioneering Works (1990s) – Foundational Rigid Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#the-2000s-robust-and-non-rigid-registration-emerges">5.2 The 2000s – Robust and Non-Rigid Registration Emerges</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-template-based-and-parametric-model-registration">5.3 2010s – Template-based and Parametric Model Registration</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_05_2_3d_registration.html#s-learning-based-parametric-registration-and-hybrid-approaches">5.4 2020s – Learning-Based Parametric Registration and Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05_2_3d_registration.html#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html">Lecture 06.1 - Fitting the SMPL Model to Images via Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#mathematical-background-pinhole-camera-and-projections">Mathematical Background: Pinhole Camera and Projections</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#perspective-projection">Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#weak-perspective-projection">Weak-Perspective Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#camera-extrinsics-vs-model-pose">Camera Extrinsics vs. Model Pose</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#d-keypoints-and-projection">2D Keypoints and Projection</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#the-smpl-model-as-a-differentiable-function-of-shape-and-pose">The SMPL Model as a Differentiable Function of Shape and Pose</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#shape-blend-shapes">Shape Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#pose-blend-shapes">Pose Blend Shapes</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#joint-positions">Joint Positions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#linear-blend-skinning">Linear Blend Skinning</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#differentiability-of-smpl">Differentiability of SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#fitting-smpl-to-images-via-optimization-smplify">Fitting SMPL to Images via Optimization (SMPLify)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#objective-function">Objective Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#combined-objective">Combined Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-strategy">Optimization Strategy</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#optimization-algorithms">Optimization Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#automatic-differentiation-and-jacobians">Automatic Differentiation and Jacobians</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#result-of-smplify">Result of SMPLify</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#historical-progression-and-method-comparisons">Historical Progression and Method Comparisons</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-bogo-et-al-2016">SMPLify (Bogo et al. 2016)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_06_1_SMPL_optimization.html#smplify-x-2019">SMPLify-X (2019)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#foundations-of-learning-based-smpl-estimation">Foundations of Learning-Based SMPL Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#integrating-smpl-into-neural-networks">Integrating SMPL into Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#projection-functions">Projection Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loss-functions">Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#statistical-priors-and-adversarial-losses">Statistical Priors and Adversarial Losses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#early-regression-approaches-hmr-and-nbf">Early Regression Approaches: HMR and NBF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#human-mesh-recovery-hmr">Human Mesh Recovery (HMR)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#neural-body-fitting-nbf">Neural Body Fitting (NBF)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evolving-architectures-hybrid-and-improved-regression-methods">Evolving Architectures: Hybrid and Improved Regression Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#spin-optimization-in-the-training-loop">SPIN: Optimization in the Training Loop</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pymaf-pyramidal-mesh-alignment-feedback">PyMAF: Pyramidal Mesh Alignment Feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cliff-using-full-frame-context-for-camera-orientation">CLIFF: Using Full-Frame Context for Camera Orientation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pixie-whole-body-regression-with-part-experts">PIXIE: Whole-Body Regression with Part Experts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#temporal-methods-from-single-images-to-video-sequences">Temporal Methods: From Single Images to Video Sequences</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vibe-adversarial-motion-prior-with-grus">VIBE: Adversarial Motion Prior with GRUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tcmr-temporally-consistent-mesh-recovery">TCMR: Temporally Consistent Mesh Recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="#motionbert-transformer-based-motion-representations">MotionBERT: Transformer-Based Motion Representations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#comparison-of-learning-based-methods">Comparison of Learning-Based Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#supervision-and-data">Supervision and Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#network-architecture">Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="#objective-functions">Objective Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pose-and-shape-priors">Pose and Shape Priors</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#runtime">Runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="#strengths-and-weaknesses">Strengths and Weaknesses</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html">Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#classical-imu-based-pose-estimation-a-historical-perspective">Classical IMU-Based Pose Estimation: A Historical Perspective</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#attitude-and-heading-reference-systems">Attitude and Heading Reference Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kalman-filter-approaches">Kalman Filter Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#early-sparse-sensor-approaches">Early Sparse-Sensor Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#model-based-optimization-methods">Model-Based Optimization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#inertial-sensor-fundamentals-and-orientation-representations">Inertial Sensor Fundamentals and Orientation Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gravity-alignment-and-drift-correction">Gravity Alignment and Drift Correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#orientation-representations">Orientation Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#sensor-calibration">Sensor Calibration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#optimization-based-smpl-fitting-with-imu-data">Optimization-Based SMPL Fitting with IMU Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#kinematic-model-and-sensor-prediction">Kinematic Model and Sensor Prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#regularization-and-prior-terms">Regularization and Prior Terms</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#gradient-and-jacobian-computation">Gradient and Jacobian Computation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#pseudocode-smpl-pose-estimation-from-imu-sequence">Pseudocode: SMPL Pose Estimation from IMU Sequence</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#imu-based-human-pose-datasets-and-resources">IMU-Based Human Pose Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html">Lecture 07.2: Fitting SMPL to IMU Data Using Learning-Based Methods</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#optimization-based-vs-learning-based-approaches">Optimization-Based vs. Learning-Based Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-based-imu-to-pose-estimation-historical-overview-of-key-models">Learning-Based IMU-to-Pose Estimation: Historical Overview of Key Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#deep-inertial-poser-dip-2018">Deep Inertial Poser (DIP, 2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transpose-2021">TransPose (2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#transformer-inertial-poser-tip-2022">Transformer Inertial Poser (TIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#physics-physical-inertial-poser-pip-2022">Physics/Physical Inertial Poser (PIP, 2022)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-notable-models-and-developments">Other Notable Models and Developments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#problem-formulation-and-learning-task-definition">Problem Formulation and Learning Task Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#input-and-output-representations">Input and Output Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#learning-objective-and-loss-functions">Learning Objective and Loss Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#temporal-modeling-approaches">Temporal Modeling Approaches</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#supervised-vs-semi-supervised-training-synthetic-data">Supervised vs. Semi-Supervised Training; Synthetic Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#model-architectures-and-design-considerations">Model Architectures and Design Considerations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#encoding-imu-measurements">Encoding IMU Measurements</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#network-structures">Network Structures</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#training-pipeline-and-pseudocode">Training Pipeline and Pseudocode</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#datasets-benchmarks-and-resources">Datasets, Benchmarks, and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#dip-imu-dataset-2018">DIP-IMU Dataset (2018)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#totalcapture-2017">TotalCapture (2017)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#amass-2019">AMASS (2019)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#other-datasets-and-resources">Other Datasets and Resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#challenges-and-outlook">Challenges and Outlook</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_07_2_fitting_SMPL_to_IMU_learning.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html">Lecture 08.1: Vertex-Based Clothing Modeling for Virtual Humans</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#clothing-representation-as-vertex-displacements">Clothing Representation as Vertex Displacements</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#registration-of-clothed-human-scans">Registration of Clothed Human Scans</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#estimating-body-shape-under-clothing-the-buff-method">Estimating Body Shape Under Clothing: The BUFF Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#multi-layer-clothing-capture-the-clothcap-method">Multi-Layer Clothing Capture: The ClothCap Method</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#data-term">Data Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-term">Boundary Term</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#boundary-smoothness">Boundary Smoothness</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#laplacian-smoothness">Laplacian Smoothness</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#applications-of-multi-layer-registration">Applications of Multi-Layer Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#advantages-and-limitations">Advantages and Limitations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_08_1_vertex_based_clothing_modeling.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html">Lecture 09.1: Neural Implicit and Point-Based Representations for Clothed Human Modeling</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#background-explicit-vs-implicit-vs-point-based-representations">Background: Explicit vs. Implicit vs. Point-Based Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#mesh-based-models">Mesh-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-implicit-representations">Neural Implicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-radiance-fields-nerfs-for-humans">Neural Radiance Fields (NeRFs) for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#hybrid-approaches">Hybrid Approaches</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#expressiveness-and-topology">Expressiveness and Topology</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#differentiability-and-learning">Differentiability and Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#data-efficiency-and-performance">Data Efficiency and Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-implicit-function-foundations">Neural Implicit Function Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#signed-distance-field-sdf">Signed Distance Field (SDF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#occupancy-field-indicator-function">Occupancy Field (Indicator Function)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#volume-radiance-field">Volume Radiance Field</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#articulated-deformation-fields-for-implicit-models">Articulated Deformation Fields for Implicit Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#backward-warping-inverse-skinning-field">Backward Warping (Inverse Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#forward-warping-forward-skinning-field">Forward Warping (Forward Skinning Field)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pose-dependent-deformation-secondary-motion">Pose-Dependent Deformation (Secondary Motion)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#generative-implicit-models-for-clothed-bodies">Generative Implicit Models for Clothed Bodies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#smplicit-topology-aware-clothed-human-model">SMPLicit: Topology-Aware Clothed Human Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#imghum-implicit-generative-human-model">imGHUM: Implicit Generative Human Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pose-dependent-implicit-models-and-animatable-avatars">Pose-Dependent Implicit Models and Animatable Avatars</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#nasa-neural-articulated-shape-approximation-eccv-2020">NASA: Neural Articulated Shape Approximation (ECCV 2020)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#scanimate-weakly-supervised-skinned-avatar-networks-cvpr-2021">SCANimate: Weakly-Supervised Skinned Avatar Networks (CVPR 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#neural-gif-neural-generalized-implicit-functions-iccv-2021">Neural-GIF: Neural Generalized Implicit Functions (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#snarf-skinned-neural-articulated-implicit-shapes-iccv-2021">SNARF: Skinned Neural Articulated Implicit Shapes (ICCV 2021)</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pop-the-power-of-points-iccv-2021-point-based-modeling">POP: The Power of Points (ICCV 2021) – Point-Based Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#other-noteworthy-methods">Other Noteworthy Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#blueprint-algorithms-for-key-methods">Blueprint Algorithms for Key Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#snarf-training-procedure-differentiable-forward-skinning-for-implicit-surfaces">SNARF (Training Procedure): Differentiable Forward Skinning for Implicit Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#pop-training-fitting-pipeline-point-based-model-for-pose-dependent-clothing">POP (Training &amp; Fitting Pipeline): Point-Based Model for Pose-Dependent Clothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#smplicit-inference-fitting-workflow-generative-implicit-garment-model-conditioned-on-smpl">SMPLicit (Inference/Fitting Workflow): Generative Implicit Garment Model conditioned on SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#historical-perspective-and-future-outlook">Historical Perspective and Future Outlook</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#challenges-and-future-directions">Challenges and Future Directions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_09_1_neual_implicit_and_point_based_clothing_models.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extended_materials_neural_radiance_fields.html">Neural Radiance Fields: A Historical and Theoretical Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#foundations-3d-scene-representation-and-reconstruction-techniques">Foundations: 3D Scene Representation and Reconstruction Techniques</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#voxel-grids">Voxel Grids</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#point-clouds">Point Clouds</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#mesh-based-surfaces">Mesh-Based Surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#light-fields-and-volumetric-rendering">Light Fields and Volumetric Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#emergence-of-neural-radiance-fields-nerf">Emergence of Neural Radiance Fields (NeRF)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#core-idea">Core Idea</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#training-procedure">Training Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#nerf-architecture">NeRF Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#hierarchical-sampling">Hierarchical Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#original-results">Original Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#theoretical-and-mathematical-analysis-of-nerf">Theoretical and Mathematical Analysis of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#volume-rendering-formulation-in-nerf">Volume Rendering Formulation in NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#positional-encoding-and-neural-network-architecture">Positional Encoding and Neural Network Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#loss-function-and-optimization">Loss Function and Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#major-advancements-and-extensions-of-nerf">Major Advancements and Extensions of NeRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#anti-aliasing-and-unbounded-scenes-mip-nerf-and-nerf">Anti-Aliasing and Unbounded Scenes: mip-NeRF and NeRF++</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#efficiency-improvements-instant-nerf-and-plenoctrees">Efficiency Improvements: Instant NeRF and PlenOctrees</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dynamic-and-deformable-nerfs-d-nerf-nerfies-nsff-etc">Dynamic and Deformable NeRFs (D-NeRF, Nerfies, NSFF, etc.)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#neural-radiance-fields-for-human-modeling-with-smpl-and-body-models">Neural Radiance Fields for Human Modeling (with SMPL and Body Models)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-notable-extensions">Other Notable Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#comparison-with-other-3d-representations">Comparison with Other 3D Representations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-polygonal-meshes">Vs. Polygonal Meshes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-point-clouds-3d-splatting">Vs. Point Clouds / 3D Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-voxel-grids-and-volumetric-methods">Vs. Voxel Grids and Volumetric Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#vs-multi-plane-images-mpis-light-fields">Vs. Multi-Plane Images (MPIs) / Light Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#accuracy-and-fidelity">Accuracy and Fidelity</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#applicability">Applicability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#datasets-for-nerf-training-and-evaluation">Datasets for NeRF Training and Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#blender-synthetic-nerf-dataset">Blender Synthetic NeRF Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#local-light-field-fusion-llff-real-forward-facing-dataset">Local Light Field Fusion (LLFF) Real Forward-Facing Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#dtu-dataset">DTU Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#human3-6m">Human3.6M</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#zju-mocap-dataset">ZJU-MoCap Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#people-snapshot">People-Snapshot</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#synthetic-dynamic-scenes">Synthetic dynamic scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#other-datasets">Other datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extended_materials_neural_radiance_fields.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html">3D Gaussian Splatting: A Basic Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#foundations">Foundations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#what-is-a-3d-scene">What is a 3D Scene?</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-scene-representation">3D Scene Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#computer-graphics-fundamentals">Computer Graphics Fundamentals</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-and-ray-tracing">Rasterization and Ray Tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-blending-and-compositing">Alpha Blending and Compositing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-evolution-of-novel-view-synthesis">The Evolution of Novel View Synthesis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#image-based-rendering">Image-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#structure-from-motion-and-multi-view-stereo">Structure-from-Motion and Multi-View Stereo</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-based-rendering">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#neural-rendering">Neural Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accelerated-neural-fields">Accelerated Neural Fields</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-a-convergence-of-approaches">3D Gaussian Splatting: A Convergence of Approaches</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#id1">Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#point-clouds-and-their-challenges">Point Clouds and Their Challenges</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-concept-of-splatting">The Concept of Splatting</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#elliptical-weighted-average-ewa-filtering">Elliptical Weighted Average (EWA) Filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-point-based-rendering">Differentiable Point-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-core-principles">3D Gaussian Splatting: Core Principles</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#key-insight-unifying-points-and-volumes">Key Insight: Unifying Points and Volumes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussians-as-scene-primitives">3D Gaussians as Scene Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#the-volumetric-rendering-equation">The Volumetric Rendering Equation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#alpha-compositing-with-gaussians">Alpha Compositing with Gaussians</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#mathematical-formulation-of-3d-gaussian-splatting">Mathematical Formulation of 3D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#projecting-3d-gaussians-to-2d">Projecting 3D Gaussians to 2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#parameterization-of-3d-gaussians">Parameterization of 3D Gaussians</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#view-dependent-appearance">View-Dependent Appearance</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-equations">Differentiable Rendering Equations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-and-optimization">Training and Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#photometric-loss">Photometric Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#initial-point-cloud">Initial Point Cloud</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#optimization-process">Optimization Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-splatting-pipeline">Differentiable Splatting Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#adaptive-density-control">Adaptive Density Control</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-and-real-time-rendering">Implementation and Real-Time Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tile-based-rendering">Tile-Based Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#fast-sorting-strategies">Fast Sorting Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-accelerated-rasterization">GPU-Accelerated Rasterization</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#memory-considerations">Memory Considerations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparison-with-other-methods">Comparison with Other Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#nerf-vs-3d-gaussian-splatting">NeRF vs. 3D Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#voxel-based-representations-vs-gaussians">Voxel-Based Representations vs. Gaussians</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#traditional-point-based-rendering-vs-gaussian-splatting">Traditional Point-Based Rendering vs. Gaussian Splatting</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-and-extensions">Applications and Extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-reconstruction">Static Scene Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-capture">Dynamic Scene Capture</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#avatar-creation-and-animation">Avatar Creation and Animation</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-neural-rendering">Integration with Neural Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#large-scale-scene-rendering">Large-Scale Scene Rendering</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#bezier-gaussian-triangles-bg-triangle-for-sharper-rendering">Bézier Gaussian Triangles (BG-Triangle) for Sharper Rendering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#representation">Representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#performance">Performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-reconstruction-with-gaussian-splatting-and-priors">Human Reconstruction with Gaussian Splatting and Priors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#eg-humannerf-efficient-generalizable-human-nerf">EG-HumanNeRF: Efficient Generalizable Human NeRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gps-gaussian-pixel-wise-gaussian-splatting-for-humans">GPS-Gaussian: Pixel-Wise Gaussian Splatting for Humans</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#generalizable-human-gaussians-ghg-with-smpl">Generalizable Human Gaussians (GHG) with SMPL</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-reconstruction-with-4d-gaussian-splatting">Dynamic Scene Reconstruction with 4D Gaussian Splatting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#d-gaussian-splatting-4dgs">4D Gaussian Splatting (4DGS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#speed-and-memory-enhancements-4dgs-1k-and-mega">Speed and Memory Enhancements (4DGS-1K and MEGA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#applications-to-mocap-and-4d-human-rendering">Applications to MoCap and 4D Human Rendering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#implementation-details-and-real-time-performance">Implementation Details and Real-Time Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#data-structures">Data Structures</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#rasterization-shaders">Rasterization &amp; Shaders</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#gpu-memory-and-throughput">GPU Memory and Throughput</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#differentiable-rendering-implementation">Differentiable Rendering Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#training-vs-inference-compute">Training vs. Inference Compute</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#accuracy-vs-speed-trade-offs">Accuracy vs. Speed trade-offs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#benchmarks-and-comparative-evaluation">Benchmarks and Comparative Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#static-scene-comparison">Static Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#human-novel-view-comparison">Human Novel-View Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#dynamic-scene-comparison">Dynamic Scene Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#comparative-summary-table">Comparative Summary Table</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#datasets-and-resources">Datasets and Resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#synthetic-nerf-dataset-blender-scenes">Synthetic NeRF Dataset (Blender Scenes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#llff-local-light-field-fusion">LLFF (Local Light Field Fusion)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#tanks-and-temples">Tanks and Temples</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#multi-object-360-co3d-common-objects-in-3d">Multi-Object 360 (CO3D - Common Objects in 3D)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#amass-archive-of-motion-capture-as-surface-shapes">AMASS (Archive of Motion Capture as Surface Shapes)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#cape-clothed-auto-person-encoding">CAPE (Clothed Auto Person Encoding)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#thuman-thuman2-0">THuman / THuman2.0</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#renderpeople">RenderPeople</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#open-source-implementations">Open-Source Implementations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#future-directions">Future Directions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#improved-compression-techniques-for-memory-efficiency">Improved Compression Techniques for Memory Efficiency</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#handling-dynamic-and-deformable-scenes">Handling Dynamic and Deformable Scenes</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#advanced-material-modeling-for-realistic-rendering">Advanced Material Modeling for Realistic Rendering</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#hybrid-approaches-integrating-neural-fields-and-explicit-representations">Hybrid Approaches Integrating Neural Fields and Explicit Representations</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#scalability-for-large-scale-scenes-city-level-and-beyond">Scalability for Large-Scale Scenes (City-Level and Beyond)</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#real-time-applications-in-ar-vr-and-gaming">Real-Time Applications in AR/VR and Gaming</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#integration-with-existing-graphics-pipelines">Integration with Existing Graphics Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#learning-from-limited-data">Learning from Limited Data</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="extendeed_materials_gaussian_splatting.html#glossary">Glossary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a><ul>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-1-historical-body-models">Lecture 01.1 (Historical Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-2-introduction-to-human-models">Lecture 01.2 (Introduction to Human Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-01-3-introduction-to-human-models-continued">Lecture 01.3 (Introduction to Human Models Continued)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-1-image-formation">Lecture 02.1 (Image Formation)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-02-2-rotations-kinematic-chains">Lecture 02.2 (Rotations &amp; Kinematic Chains)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-1-surface-representations">Lecture 03.1 (Surface Representations)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-03-2-procrustes-alignment">Lecture 03.2 (Procrustes Alignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-1-iterative-closest-points">Lecture 04.1 (Iterative Closest Points)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-04-2-body-models">Lecture 04.2 (Body Models)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-1-body-model-training">Lecture 05.1 (Body Model Training)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-05-2-3d-registration">Lecture 05.2 (3D Registration)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-fitting-smpl-to-images">Lecture 06.1 (Fitting SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-1-optimization-based-fitting-of-smpl-to-images">Lecture 06.1 (Optimization-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-06-2-learning-based-fitting-of-smpl-to-images">Lecture 06.2 (Learning-Based Fitting of SMPL to Images)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-1-fitting-smpl-to-imu-optimization">Lecture 07.1 (Fitting SMPL to IMU Optimization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-07-2-fitting-smpl-to-imu-learning">Lecture 07.2 (Fitting SMPL to IMU Learning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="references.html#classic-and-optimization-based-methods">Classic and Optimization-Based Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="references.html#learning-based-methods">Learning-Based Methods</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets-and-resources">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#relevant-software-and-libraries">Relevant Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-08-1-references-for-vertex-based-clothing-modeling-for-virtual-humans">Lecture 08.1: References for Vertex-Based Clothing Modeling for Virtual Humans</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#body-models-and-shape-estimation">Body Models and Shape Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#alternative-representations">Alternative Representations</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#software-and-libraries">Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#lecture-09-1-references-for-neural-implicit-and-point-based-representations-for-clothed-human-modeling">Lecture 09.1: References for Neural Implicit and Point-Based Representations for Clothed Human Modeling</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#point-based-models">Point-Based Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#id50">Datasets and Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#related-software-and-libraries">Related Software and Libraries</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#review-papers-and-tutorials">Review Papers and Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#neural-radiance-fields-nerf">Neural Radiance Fields (NERF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="references.html#gaussian-splatting">Gaussian Splatting</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Virtual Humans Lecture</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lecture_06_2SMPL_model_fitting.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lecture-06-2-learning-based-fitting-of-the-smpl-model-to-images">
<span id="lecture-06-2-learning-based-fitting"></span><h1>Lecture 06.2 - Learning-Based Fitting of the SMPL Model to Images<a class="headerlink" href="#lecture-06-2-learning-based-fitting-of-the-smpl-model-to-images" title="Link to this heading"></a></h1>
<iframe width="600" height="400" src="https://www.youtube.com/embed/y8p9X-0T3-c"
title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><a class="reference external" href="https://virtualhumans.mpi-inf.mpg.de/VH23/slides/pdf/Lecture_06_2-Fitting_SMPL_to_Images_with_Learning.pdf">Lecture Slides: Learning-Based Fitting of the SMPL Model to Images</a></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>In the previous lecture, we explored optimization-based methods for fitting the SMPL body model to images (e.g., the SMPLify algorithm). While these approaches are effective without requiring large training datasets, they come with certain limitations: they can be slow (taking several seconds per image), are sensitive to initialization, and may get stuck in local minima.</p>
<p>This lecture shifts our focus to learning-based methods for SMPL model fitting. These approaches leverage deep neural networks to regress SMPL parameters directly from images, often achieving:</p>
<ol class="arabic simple">
<li><p>Real-time performance (milliseconds vs. seconds per image)</p></li>
<li><p>Greater robustness to initialization and image variations</p></li>
<li><p>Improved ability to handle ambiguous or partial observations</p></li>
</ol>
<p>We will explore pure regression approaches, hybrid methods that integrate optimization in-the-loop, and advanced techniques for video sequences. We’ll derive the mathematical foundations and highlight key architectures, from simple encoders to adversarial and temporal models.</p>
</section>
<section id="foundations-of-learning-based-smpl-estimation">
<h2>Foundations of Learning-Based SMPL Estimation<a class="headerlink" href="#foundations-of-learning-based-smpl-estimation" title="Link to this heading"></a></h2>
<section id="integrating-smpl-into-neural-networks">
<h3>Integrating SMPL into Neural Networks<a class="headerlink" href="#integrating-smpl-into-neural-networks" title="Link to this heading"></a></h3>
<p>A core idea in learning-based methods is to make the parametric SMPL model part of a neural network’s forward pass. Recall that SMPL is a differentiable function <span class="math notranslate nohighlight">\(M(\boldsymbol{\theta}, \boldsymbol{\beta})\)</span> that outputs a posed mesh (vertices <span class="math notranslate nohighlight">\(V \in \mathbb{R}^{3\times N}\)</span>) given pose parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> (joint rotations) and shape parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> (body shape PCA coefficients):</p>
<div class="math notranslate nohighlight">
\[V = M(\boldsymbol{\theta}, \boldsymbol{\beta})\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{10}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \mathbb{R}^{3K}\)</span> (typically with <span class="math notranslate nohighlight">\(K=23\)</span> body joints, parameterized in axis-angle or rotation matrices).</p>
<p>The model’s differentiability means we can backpropagate through it: small changes in <span class="math notranslate nohighlight">\(\boldsymbol{\theta}, \boldsymbol{\beta}\)</span> yield smooth changes in vertices and hence in any differentiable error measured on the output. Learning-based approaches implement SMPL layers in frameworks like TensorFlow or PyTorch so that predicted pose/shape parameters can be refined via gradient descent on task losses.</p>
</section>
<section id="projection-functions">
<h3>Projection Functions<a class="headerlink" href="#projection-functions" title="Link to this heading"></a></h3>
<p>To train on 2D images, we need to compare the 3D model output with 2D annotations (e.g., keypoints, silhouettes). This requires a projection function <span class="math notranslate nohighlight">\(\Pi\)</span> to map 3D points to the image plane. Two projection models are commonly used:</p>
<p><strong>Full Perspective Projection</strong></p>
<p>Given camera intrinsics (focal length <span class="math notranslate nohighlight">\(f\)</span>, principal point <span class="math notranslate nohighlight">\((c_x, c_y)\)</span>) and extrinsic rotation <span class="math notranslate nohighlight">\(R \in SO(3)\)</span>, translation <span class="math notranslate nohighlight">\(t = (t_x, t_y, t_z)\)</span>, a 3D point <span class="math notranslate nohighlight">\(X = (X_X, X_Y, X_Z)\)</span> in the world projects to pixel coordinates <span class="math notranslate nohighlight">\(x = (u, v)\)</span> via:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X' &amp;= R X + t \\
u &amp;= f \frac{X'_x}{X'_z} + c_x \\
v &amp;= f \frac{X'_y}{X'_z} + c_y\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\((X'_x, X'_y, X'_z)\)</span> are the coordinates of <span class="math notranslate nohighlight">\(X\)</span> in the camera frame. This perspective model accounts for foreshortening (apparent size changes with depth).</p>
<p><strong>Weak-Perspective (Scaled Orthographic) Projection</strong></p>
<p>Many methods assume the person occupies a small field of view, allowing a simplified model:</p>
<div class="math notranslate nohighlight">
\[x = s \cdot (R X)_{xy} + t_{xy}\]</div>
<p>Here <span class="math notranslate nohighlight">\(s\)</span> is an overall scale (related to focal length and average depth), <span class="math notranslate nohighlight">\((R X)_{xy}\)</span> are the first two components of <span class="math notranslate nohighlight">\(R X\)</span>, and <span class="math notranslate nohighlight">\(t_{xy} = (t_x, t_y)\)</span> is an in-plane translation. Effectively, all points are scaled by the same factor <span class="math notranslate nohighlight">\(s\)</span> and translated, ignoring <span class="math notranslate nohighlight">\(z\)</span>-depth variation.</p>
<p>For instance, in HMR (Human Mesh Recovery), the projection of the 3D joint positions <span class="math notranslate nohighlight">\(X_i(\boldsymbol{\theta}, \boldsymbol{\beta})\)</span> is modeled as:</p>
<div class="math notranslate nohighlight">
\[\hat{x}^i = s \Pi_{ortho}(R X_i) + t\]</div>
<p>where <span class="math notranslate nohighlight">\(\Pi_{\text{ortho}}(X) = (X_x, X_y)\)</span> simply drops <span class="math notranslate nohighlight">\(z\)</span>. Weak perspective is convenient since <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(t\)</span> can be learned as part of the regression. However, it introduces ambiguity between <span class="math notranslate nohighlight">\(s\)</span> and <span class="math notranslate nohighlight">\(t_z\)</span> (depth), which some methods later address by incorporating full-frame information.</p>
</section>
<section id="loss-functions">
<h3>Loss Functions<a class="headerlink" href="#loss-functions" title="Link to this heading"></a></h3>
<p><strong>2D Reprojection Loss</strong></p>
<p>During training, 2D reprojection loss is typically used: if <span class="math notranslate nohighlight">\(x_i\)</span> is the ground-truth 2D position (e.g., from keypoint annotators) of joint <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(\hat{x}_i = \Pi(X_i(\boldsymbol{\theta}, \boldsymbol{\beta}))\)</span> is the projected model joint, one minimizes:</p>
<div class="math notranslate nohighlight">
\[L_{2D} = \sum_i w_i \|\hat{x}_i - x_i\|^2\]</div>
<p>summing over visible joints with weights <span class="math notranslate nohighlight">\(w_i\)</span> (to handle confidence or importance). This encourages the network to output pose parameters that explain the observed 2D pose.</p>
<p><strong>3D Losses</strong></p>
<p>Some works also include 3D losses when ground-truth 3D joint positions <span class="math notranslate nohighlight">\(\tilde{X}_i\)</span> or SMPL parameters are available for training data:</p>
<div class="math notranslate nohighlight">
\[L_{3D} = \sum_i \|\hat{X}_i - \tilde{X}_i\|^2\]</div>
<p>or a direct parameter regression loss:</p>
<div class="math notranslate nohighlight">
\[L_{param} = \|\hat{\boldsymbol{\theta}} - \tilde{\boldsymbol{\theta}}\|_1 + \|\hat{\boldsymbol{\beta}} - \tilde{\boldsymbol{\beta}}\|_1\]</div>
<p>Many modern approaches train in a mixed supervised manner: some images have 3D labels (allowing <span class="math notranslate nohighlight">\(L_{3D}\)</span>) and a vast number have only 2D labels (using <span class="math notranslate nohighlight">\(L_{2D}\)</span>). The total objective is a weighted sum, with losses zeroed out if not applicable to a given sample.</p>
<p><strong>Silhouette and Segmentation Losses</strong></p>
<p>Beyond keypoints, another 2D cue is the person’s silhouette or part segmentation. If one renders the SMPL mesh (using the predicted parameters and some camera guess) into a binary mask or a part index image, it can be compared against a ground-truth silhouette/segmentation.</p>
<p>A common formulation is a silhouette overlap (IoU) loss or binary cross-entropy per pixel. These losses enforce that the projected 3D mesh aligns with the person’s outline in the image. For instance, Pavlakos et al. (2018) incorporated a silhouette consistency term in addition to keypoints.</p>
<p>Silhouette losses are less commonly used than keypoints (since obtaining accurate silhouettes for training is harder), but they provide information about body shape and can correct pose ambiguities not evident from sparse joints.</p>
</section>
<section id="statistical-priors-and-adversarial-losses">
<h3>Statistical Priors and Adversarial Losses<a class="headerlink" href="#statistical-priors-and-adversarial-losses" title="Link to this heading"></a></h3>
<p>A recurring challenge is that predicting 3D pose from 2D is an ill-posed problem – many 3D configurations project to the same 2D points. Without regularization, a network trained purely on <span class="math notranslate nohighlight">\(L_{2D}\)</span> may output implausible bodies that nevertheless yield low reprojection error (e.g., bent limbs folded behind the camera, or unnatural poses).</p>
<p>To address this, learning-based methods incorporate priors on pose and shape:</p>
<p><strong>Explicit Priors</strong></p>
<p>One approach is to use explicit priors: for example, penalize improbable shapes via a Gaussian prior on <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> (e.g., <span class="math notranslate nohighlight">\(L_{shape} = \|\boldsymbol{\beta}\|^2\)</span> if the shape PCA is zero-mean, ensuring shapes stay near average) or on joint angles (as in SMPLify’s pose prior).</p>
<p><strong>Adversarial Priors</strong></p>
<p>Another very successful approach is to learn a latent pose prior via adversarial training. Adversarial approaches introduce a discriminator <span class="math notranslate nohighlight">\(D\)</span> that tries to distinguish “real” human model parameters from “fake” ones predicted by the network.</p>
<p>Kanazawa et al. (2018) pioneered this in HMR, training <span class="math notranslate nohighlight">\(D(\boldsymbol{\theta}, \boldsymbol{\beta})\)</span> to output 1 for samples coming from a large motion capture dataset and 0 for the network’s outputs. This discriminator acts as a data-driven prior: the regression network (the generator) gets an additional loss <span class="math notranslate nohighlight">\(L_{adv}\)</span> if its predicted pose parameters lie outside the distribution of plausible humans.</p>
<p>The adversarial loss can be formulated (using least-squares GAN formulation) as:</p>
<div class="math notranslate nohighlight">
\[L_{adv}(E) = \sum_i \mathbb{E}_{\Theta \sim E(I)}[(D_i(\Theta) - 1)^2]\]</div>
<p>where <span class="math notranslate nohighlight">\(E(I)\)</span> are the predicted parameters from image <span class="math notranslate nohighlight">\(I\)</span>, and <span class="math notranslate nohighlight">\(D_i\)</span> ranges over multiple discriminators if the adversary is factorized. The discriminator(s) are simultaneously trained to minimize:</p>
<div class="math notranslate nohighlight">
\[\sum_i \mathbb{E}_{\Theta \sim p_{real}}[(D_i(\Theta) - 1)^2] + \mathbb{E}_{\Theta \sim E(I)}[(D_i(\Theta))^2]\]</div>
<p>Intuitively, the regressor is pushed to generate outputs that fool the discriminator, i.e., that look like real human poses in the SMPL parameter space. This strategy implicitly learns joint-angle limits, anthropometric feasibility (limb lengths, etc.), and typical pose combinations from data.</p>
<p>Kanazawa et al. further factorized the adversarial prior by breaking the SMPL parameter vector into parts: one discriminator for shape (<span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>), one for each joint’s rotation, and one for the whole pose together. Because each input to <span class="math notranslate nohighlight">\(D\)</span> is low-dimensional (e.g., a 9D rotation representation per joint), these discriminators are small and easier to train than one large discriminator on the full pose vector.</p>
<p>The joint-wise <span class="math notranslate nohighlight">\(D\)</span> effectively learns the acceptable range for each joint (learning to detect out-of-range angles), while the full-pose <span class="math notranslate nohighlight">\(D\)</span> captures inter-joint correlations (e.g., you can’t raise your left foot without bending your left knee). This factorized adversarial loss proved crucial to regularize the network when no ground-truth 3D labels are available.</p>
<p>In summary, learning-based SMPL fitting frameworks build a differentiable pipeline: Image <span class="math notranslate nohighlight">\(\to\)</span> CNN features <span class="math notranslate nohighlight">\(\to\)</span> SMPL parameters <span class="math notranslate nohighlight">\(\to\)</span> Projected 2D outputs, trained end-to-end. They combine reprojection losses for alignment, 3D losses when possible for accuracy, and priors (explicit or learned adversarial) to constrain the solution space.</p>
</section>
</section>
<section id="early-regression-approaches-hmr-and-nbf">
<h2>Early Regression Approaches: HMR and NBF<a class="headerlink" href="#early-regression-approaches-hmr-and-nbf" title="Link to this heading"></a></h2>
<section id="human-mesh-recovery-hmr">
<h3>Human Mesh Recovery (HMR)<a class="headerlink" href="#human-mesh-recovery-hmr" title="Link to this heading"></a></h3>
<p>One of the first end-to-end learning methods for human mesh recovery was HMR (Human Mesh Recovery) by Kanazawa et al. (CVPR 2018). HMR demonstrated that a deep network could directly regress the 85-dimensional SMPL parameter vector (<span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \mathbb{R}^{69}\)</span> for pose, <span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{10}\)</span> for shape, and optionally global orientation + camera <span class="math notranslate nohighlight">\((R, s, t)\)</span>) from a single RGB image.</p>
<p>This represented a paradigm shift: instead of per-image optimization at inference time, a neural network “learns” to do the fitting.</p>
<p><strong>Architecture</strong></p>
<p>At its core, HMR uses a convolutional encoder (based on ResNet) feeding into a pose parameter regressor that is implemented as an Iterative Error Feedback (IEF) loop. IEF is a technique where an initial guess of parameters is progressively refined by the network in a fixed number of iterations (implemented by a recurrent or unrolled loop).</p>
<p>The network predicts a series of parameter updates <span class="math notranslate nohighlight">\(\Delta \Theta\)</span> to add to the current estimate, gradually improving the fit. This helps the network handle large output dimensions by breaking down the regression task into smaller corrective steps.</p>
<p>In practice, HMR’s regressor outputs <span class="math notranslate nohighlight">\(\Theta = \{\boldsymbol{\theta}, \boldsymbol{\beta}, \mathbf{c}\}\)</span> (where <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> are weak-perspective camera params <span class="math notranslate nohighlight">\((s, t_x, t_y)\)</span>) and does so in 3 iterative refinements.</p>
<p>The final output’s 3D joints <span class="math notranslate nohighlight">\(\hat{X}(\Theta)\)</span> are projected with the weak-perspective model and compared to ground-truth 2D keypoints for a reprojection loss. If any 3D pose labels are available (e.g., from motion capture datasets), an additional 3D loss on joint positions or parameters can be applied.</p>
<p><strong>Adversarial Pose Prior</strong></p>
<p>The key innovation of HMR was introducing an adversarial loss to cope with the lack of direct 3D supervision on in-the-wild images. A discriminator <span class="math notranslate nohighlight">\(D\)</span> was trained on a large corpus of SMPL parameters obtained from mocap (the CMU MoCap dataset processed via MoSh, yielding a distribution of plausible human poses and shapes).</p>
<p>The discriminator learns to output “real” for any parameter vector coming from this dataset and “fake” for the network’s output. The HMR regressor then receives an adversarial loss term that pushes its outputs to be indistinguishable from real poses.</p>
<p>This adversarial prior ensures that even if an image’s 2D keypoints are sparse or ambiguous, the estimated 3D pose will lie on the manifold of realistic human poses. Notably, HMR’s adversarial prior captured a variety of constraints implicitly – from joint angle limits to natural pose transitions – outperforming simpler priors like SMPLify’s Gaussian Mixture Model on joint angles.</p>
<p>The overall HMR loss is:</p>
<div class="math notranslate nohighlight">
\[L_{HMR} = L_{2D\_joints} + \lambda_{3D} L_{3D} + \lambda_{adv} L_{adv}\]</div>
<p>with <span class="math notranslate nohighlight">\(\lambda\)</span> weights and <span class="math notranslate nohighlight">\(L_{3D}\)</span> used only for those images (e.g., from Human3.6M) where 3D ground truth is available.</p>
<p>HMR was trained on a mix of datasets (several 2D pose datasets for <span class="math notranslate nohighlight">\(L_{2D}\)</span> and some 3D mocap data for <span class="math notranslate nohighlight">\(L_{adv}\)</span> and optional <span class="math notranslate nohighlight">\(L_{3D}\)</span>) and was the first to demonstrate end-to-end learning of pose and shape with an inference speed of ~30 FPS – a huge advantage over multi-second optimization. Its accuracy on 3D pose benchmarks was on par with or better than optimization methods of the time, and qualitatively it produced reasonable body shapes.</p>
</section>
<section id="neural-body-fitting-nbf">
<h3>Neural Body Fitting (NBF)<a class="headerlink" href="#neural-body-fitting-nbf" title="Link to this heading"></a></h3>
<p>In parallel to HMR, Omran et al. (3DV 2018) proposed Neural Body Fitting (NBF), which took a slightly different approach. NBF is a hybrid CNN + model-based method that explicitly incorporates body part segmentation as an intermediate representation.</p>
<p>The pipeline consists of two stages:
1. A 2D part segmentation network processes the input image into a segmentation map with labels for torso, limbs, head, etc.
2. An encoder network takes this color-coded part segmentation and regresses the SMPL parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}, \boldsymbol{\beta}\)</span>.</p>
<p>The motivation is that part segments provide a richer and less ambiguous input than raw pixels for inferring 3D pose – they roughly capture the 2D shape and orientation of limbs. By first solving an easier 2D vision problem (segmentation), the burden on the 3D regressor is reduced.</p>
<p>Mathematically, if <span class="math notranslate nohighlight">\(S(I)\)</span> is the segmentation probability map produced from image <span class="math notranslate nohighlight">\(I\)</span>, NBF learns a function <span class="math notranslate nohighlight">\(f: S(I) \mapsto (\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\beta}})\)</span>. The SMPL model is then applied to get a mesh <span class="math notranslate nohighlight">\(M(\hat{\boldsymbol{\theta}}, \hat{\boldsymbol{\beta}})\)</span>, from which 3D joints <span class="math notranslate nohighlight">\(\hat{X}\)</span> are obtained (via a fixed regressor matrix <span class="math notranslate nohighlight">\(J\)</span> that maps vertices to joints).</p>
<p>These joints are projected to 2D with a known camera model (NBF assumed a given camera or estimated a simple orthographic projection). The entire chain <span class="math notranslate nohighlight">\(I \to S(I) \to f(\cdot) \to \hat{X} \to \hat{x}\)</span> is differentiable, so NBF can be trained end-to-end with a combination of 2D joint losses and 3D losses (when available).</p>
<p>One notable design choice: NBF applied the loss on 3D rotations in matrix form rather than on the raw axis-angle parameters. They found that regressing a rotation in axis-angle space and comparing to ground truth can be problematic (due to angle wrapping and the fact that a small Euclidean error in axis-angle might not correspond to a small rotation difference).</p>
<p>Instead, they convert predicted and GT rotations to rotation matrices and compute an <span class="math notranslate nohighlight">\(L_1\)</span> or <span class="math notranslate nohighlight">\(L_2\)</span> loss on the matrix entries (or equivalently on axis-angle after mapping to nearest rotation). This ensures the loss directly penalizes orientation discrepancy in a smooth way.</p>
<p><strong>Comparison of HMR and NBF</strong></p>
<p>Both are single-image, fully differentiable models that output SMPL parameters. The key differences are:</p>
<ul class="simple">
<li><p><strong>Intermediate representation</strong>: HMR is direct regression from raw pixels, which is elegant and requires minimal preprocessing but relies heavily on the network learning internal representations for body parts and geometry. NBF explicitly gives the network a structured input (the part segmentation), which can make learning easier and also provides a degree of interpretability.</p></li>
<li><p><strong>Training paradigm</strong>: NBF did not use an adversarial prior; instead, it relied on some 3D labels (from a dataset called UP-3D which provides fitted SMPL parameters to real images) and implicit regularization from the segmentation input to keep outputs realistic.</p></li>
</ul>
<p>In practice, NBF achieved competitive results to HMR on benchmarks, confirming that 2D proxy tasks (like segmentation or keypoint detection) can usefully guide 3D regression. Later methods would combine both strategies – using rich intermediate representations and still maintaining an end-to-end trainable system.</p>
</section>
</section>
<section id="evolving-architectures-hybrid-and-improved-regression-methods">
<h2>Evolving Architectures: Hybrid and Improved Regression Methods<a class="headerlink" href="#evolving-architectures-hybrid-and-improved-regression-methods" title="Link to this heading"></a></h2>
<p>After HMR and NBF, a flurry of works in 2019-2021 proposed improvements. Here we highlight major developments: integrating model-fitting into training (SPIN), refining regressors with feedback (PyMAF), leveraging full-image information (CLIFF), and extending to whole-body models (PIXIE).</p>
<section id="spin-optimization-in-the-training-loop">
<h3>SPIN: Optimization in the Training Loop<a class="headerlink" href="#spin-optimization-in-the-training-loop" title="Link to this heading"></a></h3>
<p>While purely regression-based methods are fast, they can suffer if the training data is limited or if the 2D-to-3D mapping is too ambiguous. On the other hand, classical optimization (like SMPLify) can often find a very precise fit for a given image by directly minimizing reprojection error – but it’s slow at test time and requires a good initialization.</p>
<p>SPIN (SMPLify-In-the-Loop) by Kolotouros et al. (ICCV 2019) attempts to get the best of both worlds. The idea is to use optimization during training to supervise the network, but at test time the network alone produces the result (no slow optimization needed).</p>
<p><strong>Self-Improving Loop</strong></p>
<p>In each training iteration, for a given image <span class="math notranslate nohighlight">\(I\)</span>, the CNN first predicts an initial estimate of SMPL parameters <span class="math notranslate nohighlight">\(\Theta_{reg} = (\boldsymbol{\theta}, \boldsymbol{\beta}, \mathbf{c})\)</span>.</p>
<p>Instead of immediately computing a loss on <span class="math notranslate nohighlight">\(\Theta_{reg}\)</span>, SPIN uses this as a starting point for SMPLify (the optimization-based fitter) to refine the parameters to better match the 2D keypoints of that image. In other words, it runs a few iterations of analysis-by-synthesis model fitting, initialized from the network’s output.</p>
<p>This yields a new set <span class="math notranslate nohighlight">\(\Theta_{opt}\)</span> which (ideally) has lower reprojection error than <span class="math notranslate nohighlight">\(\Theta_{reg}\)</span>. Now, <span class="math notranslate nohighlight">\(\Theta_{opt}\)</span> is treated as pseudo-ground-truth for the network, and a loss <span class="math notranslate nohighlight">\(L = \|\Theta_{reg} - \Theta_{opt}\|^2\)</span> is applied to train the CNN to predict closer to this optimally fitted solution.</p>
<p>In effect, the optimization module “corrects” the network on each training example, and the network is explicitly trained to mimic the optimizer’s result. This process is self-improving:
- As the network gets better, its initial guesses <span class="math notranslate nohighlight">\(\Theta_{reg}\)</span> are closer to the optimum, making the optimizer’s job easier and more likely to succeed
- Conversely, the optimizer provides increasingly accurate training targets which make the network even better</p>
<p>Crucially, this in-the-loop fitting provides a form of 3D supervision even for images that only have 2D annotations. Normally, without 3D ground truth, a purely regression method would have to rely on weak <span class="math notranslate nohighlight">\(L_{2D}\)</span> losses or an adversarial prior (as HMR did) to infer 3D structure.</p>
<p>SPIN bypasses this by using the optimizer to create a plausible 3D solution that explains the 2D points. That solution serves as a training signal. It was found that this “privileged” supervision (the optimizer has effectively done a mini model-fitting for that image) is more informative than a binary real/fake signal from a discriminator.</p>
<p>Indeed, Kolotouros et al. note that in a setting with only 2D keypoints, SPIN’s loop outperformed a HMR-style adversarial training.</p>
<p><strong>Robust Training and Performance</strong></p>
<p>Implementing SPIN requires care: sometimes SMPLify can fail or produce implausible fits (especially if the network’s initialization is poor in early training). The authors addressed this by:</p>
<ul class="simple">
<li><p>Rejecting bad fits (if the reprojection error after fitting is above a threshold, they do not use <span class="math notranslate nohighlight">\(\Theta_{opt}\)</span>, and instead fall back to a standard 2D loss for that image)</p></li>
<li><p>Clamping extreme shape values (avoiding out-of-bound <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>)</p></li>
<li><p>Keeping a dictionary of best fits per training image: if a later epoch’s SMPLify produces a worse result than a previous attempt, they retain the previous best, ensuring the supervision only gets better over time</p></li>
<li><p>Initializing this dictionary by running SMPLify offline on all training images (to give the network a reasonable starting target)</p></li>
</ul>
<p>These tactics stabilized training.</p>
<p>SPIN was trained on a mix of datasets (similar to HMR: e.g., COCO and MPII for 2D, H3.6M and MPI-INF-3DHP for 3D, and additionally the 2D keypoints from LSP, etc.), and importantly no adversarial prior was used – the network leans on the optimized fits to stay realistic.</p>
<p>The results were impressive: SPIN significantly outperformed HMR and other contemporaries on benchmarks like 3DPW (a challenging dataset of outdoor videos). For instance, on 3DPW SPIN achieved a reconstruction error of ~59.2 mm, whereas HMR was around 81.3 mm. Even a baseline “SPIN without in-loop fitting” (training on static pseudo-labels) got ~66 mm, highlighting that the in-loop update gave a further boost.</p>
<p>Qualitatively, SPIN results were visually closer to the image evidence, since the network had essentially learned to perform a few iterations of keypoint alignment internally. SPIN’s approach underscored the value of combining optimization and learning: optimization can enhance training data, and learning makes optimization robust at runtime.</p>
</section>
<section id="pymaf-pyramidal-mesh-alignment-feedback">
<h3>PyMAF: Pyramidal Mesh Alignment Feedback<a class="headerlink" href="#pymaf-pyramidal-mesh-alignment-feedback" title="Link to this heading"></a></h3>
<p>Despite the advances of HMR, SPIN, and similar methods, a common issue remained: the precision of the final alignment. A network might predict pose parameters that give correct 2D joint projections, but the limbs or torso of the 3D mesh might still be slightly misaligned relative to the image (especially if evaluated by overlap or detailed correspondences).</p>
<p>Minor errors in joint angles compound along the kinematic chain, leading to visible misplacements of elbows or knees in the image frame. Additionally, regression networks often emphasize global pose (to get keypoints right) at the expense of fine local pose adjustments or shape adjustments that would improve pixel-level alignment.</p>
<p>PyMAF by Zhang et al. (ICCV 2021) addresses this by introducing an iterative refinement loop inside the network that explicitly checks the mesh-image alignment and corrects the parameters. “PyMAF” stands for Pyramidal Mesh Alignment Feedback. The pipeline is as follows:</p>
<ol class="arabic simple">
<li><p>The image is passed through a backbone to produce a pyramid of feature maps (high-level, low-resolution features and lower-level, higher-resolution features).</p></li>
<li><p>An initial prediction of SMPL parameters <span class="math notranslate nohighlight">\((\boldsymbol{\theta}_0, \boldsymbol{\beta}_0)\)</span> is made from the global feature (this is analogous to previous regressors).</p></li>
<li><p>The current predicted mesh (at iteration <span class="math notranslate nohighlight">\(t\)</span>) is then used to sample “mesh-aligned” evidence from the feature pyramid. Concretely, they project each vertex (or a selection of key vertices/landmarks) onto one of the higher-resolution feature maps and extract the feature values at those locations. These features tell the network how well the mesh aligns with image details. For example, if the hand vertices project onto a region of the image feature map that has background-like features instead of hand-like features, that indicates a misalignment.</p></li>
<li><p>These mesh-aligned features are fed into a refinement module (e.g., an MLP or small CNN) that predicts an update to the parameters: <span class="math notranslate nohighlight">\((\Delta \boldsymbol{\theta}, \Delta \boldsymbol{\beta})\)</span>. The network then produces a corrected estimate <span class="math notranslate nohighlight">\((\boldsymbol{\theta}_1, \boldsymbol{\beta}_1) = (\boldsymbol{\theta}_0 + \Delta \boldsymbol{\theta}, \boldsymbol{\beta}_0 + \Delta \boldsymbol{\beta})\)</span>.</p></li>
<li><p>This process can repeat for a couple of iterations (hence “feedback loop”), analogous to how IEF was used in HMR but here the feedback is guided by spatial feature information at projected mesh locations.</p></li>
</ol>
<p>By leveraging higher-resolution feature maps in later refinement steps, PyMAF ensures that local image evidence (like the contour of an arm or the shape of a leg) can adjust the global prediction. This is important because a CNN’s deepest features (used for the initial prediction) are low-resolution and might not preserve precise spatial details.</p>
<p>The use of a feature pyramid (common in detection/segmentation networks) means even small misalignments can be detected at the appropriate scale. The outcome is a tighter mesh-image alignment, which the authors demonstrate qualitatively (the projected mesh outlines align better with the person’s silhouette).</p>
<p><strong>Auxiliary Supervision</strong></p>
<p>To train this system, PyMAF employs an auxiliary pixel-wise supervision signal. Essentially, they guide the feature extractor to be sensitive to mesh correspondence. While the details are technical, one way to implement this is to use a ground-truth correspondence map (if available, e.g., DensePose or segmentation) and ensure that the feature at a pixel encodes the identity of the body part or even the specific vertex it corresponds to.</p>
<p>This encourages the network that, when a predicted mesh is overlaid on the image, the features at mesh locations carry information about the true underlying body. Such dense supervision can be obtained from synthetic data or fitting-based annotations (the UP-3D dataset provided ground-truth part segments which could be used).</p>
<p>The authors note that this auxiliary loss makes the extracted “mesh-aligned evidence” more reliable, since the features are trained to represent body part information. In the absence of it, features might be distracted by textures or clothing, adding noise to the feedback loop.</p>
<p>PyMAF’s results showed improved per-pixel alignment and competitive pose accuracy. On 3DPW and Human3.6M, it improved error metrics slightly over previous SOTA, but more notably, it produced nicer visual alignment.</p>
<p>The approach is a bridge between pure regression and iterative fitting: it doesn’t run an external optimizer, but internally it iteratively samples the image at locations suggested by the current mesh and corrects the mesh – conceptually similar to how an analysis-by-synthesis optimizer would tweak parameters to better match image evidence, but here learned and done in a fixed small number of steps at inference.</p>
</section>
<section id="cliff-using-full-frame-context-for-camera-orientation">
<h3>CLIFF: Using Full-Frame Context for Camera Orientation<a class="headerlink" href="#cliff-using-full-frame-context-for-camera-orientation" title="Link to this heading"></a></h3>
<p>Most learning-based methods follow a top-down approach: they take a cropped image of a person (often cropped tightly around the person using a detector) and estimate the body relative to that crop. This has a limitation: once the image is cropped, the absolute position of the person in the original image and the true camera view are lost.</p>
<p>The network typically predicts the person’s pose in a crop-relative coordinate system, and uses a weak-perspective camera that is also relative to the crop. As a result, determining the global orientation of the person (e.g., facing north vs east in world coordinates) is tricky – rotating the camera or the person by 180° yields the same crop.</p>
<p>Methods like HMR or SPIN thus can only predict relative rotation (they often assume the person’s root joint rotation around vertical axis is zero in the crop, since any azimuth rotation is “absorbed” by the camera). In practice, this yields poor estimates of the global heading of the person and requires post-hoc adjustments if one needs the result in a global frame.</p>
<p>CLIFF (Carrying Location Information in Full Frames) by Li et al. (ECCV 2022) specifically tackles this issue. The key idea is to preserve the location information of the person in the full image both in the input and in the supervision. There are two main modifications in CLIFF:</p>
<p><strong>Input Encoding of Location</strong></p>
<p>Instead of only feeding the cropped RGB patch to the network, CLIFF also feeds in the person’s normalized location within the full image. In practice, one can concatenate extra channels or features that encode the bounding box position (e.g., the normalized center coordinates and scale of the crop relative to image size).</p>
<p>CLIFF obtains “holistic features” by combining the appearance features of the crop with this global location cue. By doing so, the network can learn correlations between a person’s position in the image and the likely perspective distortion or camera rotation.</p>
<p>For example, a person at the left edge of the image might be more likely turned sideways due to camera view, etc. This is akin to giving the network a sense of where the camera is relative to the person.</p>
<p><strong>Full-Frame Reprojection Loss</strong></p>
<p>Instead of computing the 2D keypoint loss in the crop coordinates, CLIFF computes it in the original image coordinates. They take the predicted 3D body (which now includes a global rotation parameter) and project it onto the full image, comparing to the ground-truth full-image keypoints.</p>
<p>Because the network knows it will be penalized for misaligning in the full frame, it must learn to predict the correct global rotation (otherwise, even if the pose looks right in the crop, the joints might fall at wrong full-image positions after undoing the crop transform).</p>
<p>In earlier methods, if a person is facing left vs right, the cropped image might look identical (just mirrored), and both would yield the same 2D loss; but in CLIFF, facing left vs right leads to different projections in the full frame (imagine the left-facing person’s joints are located more to the left in the full image vs the right-facing person’s joints). Thus the ambiguity is reduced.</p>
<p>These two changes allow CLIFF to directly predict the global orientation of the body (pelvis rotation in world coordinates) along with the pose and shape.</p>
<p><strong>Training Data and Results</strong></p>
<p>Training CLIFF required having some ground-truth global annotations. The authors leveraged the AGORA dataset (which provides ground-truth SMPL parameters in a global coordinate system) and some pseudo-labels.</p>
<p>They also built a pseudo-GT annotator based on CLIFF itself: after training an initial model, they used it with the full-frame loss to annotate in-the-wild images with global rotations, improving the training data.</p>
<p>In results, CLIFF significantly outperformed previous methods on metrics that depend on global orientation and position. For instance, on the 3DPW dataset CLIFF improved MPJPE by 5-6 mm over prior art and achieved state-of-the-art, and on the challenging AGORA dataset it ranked first on the public leaderboard at the time.</p>
<p>The benefit was most pronounced in global pose accuracy, while maintaining or improving local pose estimation.</p>
<p>In summary, CLIFF demonstrated the importance of using the full-frame context for 3D human pose: by “carrying location information” from the beginning, the network’s camera prediction no longer has to guess the depth or global rotation arbitrarily. This idea can be combined with any regression backbone (CLIFF’s architecture was built on a ResNet and an MLP head similar to SPIN’s) and is now a common consideration in extending single-person mesh recovery to multi-person scenes or images with camera movement.</p>
</section>
<section id="pixie-whole-body-regression-with-part-experts">
<h3>PIXIE: Whole-Body Regression with Part Experts<a class="headerlink" href="#pixie-whole-body-regression-with-part-experts" title="Link to this heading"></a></h3>
<p>All the methods discussed so far focus on the body pose and shape. However, the SMPL model has been extended to SMPL-X, which includes the face and hands (for a total of 104 shape parameters and 54 pose parameters including facial expression and finger joints).</p>
<p>Reconstructing a full human avatar from an image involves not only the body but also facial details and hand poses, which are challenging due to their fine-scale nature and often small pixel size in the image. Traditionally, computer vision has tackled body, face, and hands with separate specialized models:
- Face shape from a headshot using 3D Morphable Models
- Hand pose from a cropped image using a hand model
- Body pose and shape from a full-body image</p>
<p>These specialized methods can capture details (like facial expressions or finger bending) better than a generic body model regressor. But they operate independently and might produce an inconsistent overall human (e.g., the face might not match the body shape, or the pose might be inconsistent at the wrist where body and hand meet).</p>
<p>PIXIE by Feng et al. (2021) is a collaborative regression method that combines experts for body, face, and hands to produce a single coherent SMPL-X fit. The name stands for “Pixel-Aligned Whole-body Human Pose and Shape Regression with Expression” (or “Parted X (SMPL-X) regression with Moderation” in the paper’s full title), reflecting its architecture with part experts and a moderator.</p>
<p><strong>Architecture and Approach</strong></p>
<p>Here’s how PIXIE works:</p>
<ol class="arabic">
<li><p>It has three expert regression networks:
- One trained for whole-body (SMPL-X) pose and shape (much like an HMR or SPIN but for SMPL-X parameters)
- One specialized for face (which predicts detailed facial shape and expression)
- One for hands (predicting finger poses)</p>
<p>These experts are trained on their domain-specific data: e.g., the face expert on face datasets with 3D face scans or landmarks, the body expert on body pose datasets, etc.</p>
</li>
<li><p>All experts predict parameters that reside in SMPL-X’s shared space. Notably, SMPL-X uses a common shape vector for the body and face, meaning that if the face expert predicts a certain face shape, that should correspond to a certain body shape as well. PIXIE leverages this by ensuring the shape parameters are shared across the networks.</p></li>
<li><p>A moderator network takes the features or intermediate results of all experts and learns to weight and merge them into a final prediction. The moderator essentially decides how much to trust the face expert vs. the body expert for the head pose and shape, how much the hand expert vs. body expert for the wrist/hand, etc., on a per-instance basis.</p>
<p>For example, if the face is clearly visible (frontal, high-res), the face expert’s prediction of head shape should be given high weight; if the face is occluded or blurry, the body expert’s rough guess (which might be based on demographics or overall body shape) might be more reliable to avoid artifacts.</p>
</li>
<li><p>PIXIE also introduces a “gendered” shape loss. Human body shape is highly correlated with gender, and SMPL-X shape space implicitly represents gender (male vs. female body types).</p>
<p>They explicitly classify the subject’s gender from the image (or use annotation if available) and encourage the predicted shape <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> to lie in the subspace for that gender. Concretely, they trained separate shape PCA for male and female from training data, and have PIXIE predict a gender label along with the shape; a loss then penalizes shape parameters that contradict the inferred gender.</p>
<p>This leads to more accurate body shapes, as prior methods that ignore gender might predict an unrealistic average shape when given ambiguous cues.</p>
</li>
</ol>
<p><strong>Training and Results</strong></p>
<p>Training PIXIE required assembling a variety of datasets: 3D face scans for facial shape, body datasets for pose, hand datasets for finger pose, etc. During inference, PIXIE runs all experts and the moderator to output the final SMPL-X parameters (including face expression, jaw pose, hand pose, body pose, shape).</p>
<p>The result is an animatable full-body avatar with realistic face detail, something previous body-only methods could not provide. An illustration in the paper shows that a conventional SMPL-X regression (like ExPose or SPIN-X) yields a very generic face (since it’s not focusing on face shape), whereas PIXIE produces a face shape that matches the person (even capturing smile lines or jaw structure).</p>
<p>PIXIE achieved state-of-the-art accuracy on benchmarks for full-body capture, outperforming separate approaches that fit body and then refine face/hands independently. It also demonstrated the value of paying attention to demographic attributes (gender in this case) when estimating shape.</p>
<p>In practice, the authors released PIXIE’s code, enabling others to produce high-fidelity whole-body reconstructions from a single image. One limitation of PIXIE is that it still assumes the person is mostly visible (especially the face), and it doesn’t model clothing – it provides a nude body mesh with perhaps some offsets for facial detail.</p>
</section>
</section>
<section id="temporal-methods-from-single-images-to-video-sequences">
<h2>Temporal Methods: From Single Images to Video Sequences<a class="headerlink" href="#temporal-methods-from-single-images-to-video-sequences" title="Link to this heading"></a></h2>
<p>So far, we’ve considered single-image estimation. Now we’ll discuss extensions to video, where temporal consistency and motion realism become priorities. Estimating SMPL parameters for each frame independently often yields jittery results – the pose might flicker due to detector noise or minor network inconsistencies.</p>
<p>Moreover, without temporal context, an algorithm can’t enforce physically plausible transitions (e.g., limbs shouldn’t teleport from one position to another between adjacent frames). Several approaches have incorporated temporal models or priors to handle video input.</p>
<p>We’ll detail three notable methods: VIBE, TCMR, and MotionBERT, which represent the evolution from recurrent networks with adversarial motion priors to transformers with large-scale pretraining.</p>
<section id="vibe-adversarial-motion-prior-with-grus">
<h3>VIBE: Adversarial Motion Prior with GRUs<a class="headerlink" href="#vibe-adversarial-motion-prior-with-grus" title="Link to this heading"></a></h3>
<p>VIBE (Video Inference for Body Estimation) by Kocabas et al. (CVPR 2020) was one of the first frameworks to truly leverage video data to improve human mesh recovery. The authors observed that naive application of HMR on each frame yields shaky, unnatural motion, partly because the model has no knowledge of physics or typical motion patterns. To address this, VIBE introduced a temporal network and a motion discriminator.</p>
<p><strong>Architecture</strong></p>
<p>VIBE uses a two-stage approach:</p>
<ol class="arabic simple">
<li><p>First, each video frame is passed through a CNN (e.g., ResNet) to extract features. This is similar to HMR’s image encoder (and indeed they initialized it from SPIN’s pretrained model).</p></li>
<li><p>Then, these features are fed into a temporal encoder – specifically, a gated recurrent unit (GRU) – which processes the sequence of frame features and outputs SMPL parameters for each frame sequentially.</p></li>
</ol>
<p>The GRU has an internal state that carries information from past frames, enabling it to produce smoother and context-aware predictions. Intuitively, if in frame <span class="math notranslate nohighlight">\(t-1\)</span> the person had arms raised and in frame <span class="math notranslate nohighlight">\(t\)</span> the CNN feature is ambiguous, the GRU might continue to predict arms raised because it “remembers” the previous state, rather than jittering to a random new pose.</p>
<p><strong>Adversarial Motion Discriminator</strong></p>
<p>Beyond just smoothing, VIBE wanted to ensure the sequence of poses is realistic as a whole. They leveraged the AMASS dataset – a large collection of motion capture sequences – to train a discriminator <span class="math notranslate nohighlight">\(D_{motion}\)</span> that looks at a window of predicted poses (e.g., 16 frames) and determines if this motion came from a real human or from the model.</p>
<p>This discriminator considers the temporal progression of joints, effectively learning what real motion dynamics look like (smooth acceleration, no jitter, plausible gait cycles, etc.). The regression network (the GRU + image encoder) is then adversarially trained to produce pose sequences that fool the motion discriminator.</p>
<p>In practice, the discriminator architecture in VIBE was also a GRU (or similar recurrent network) that outputs a probability of the sequence being real. The loss from this GAN-like setup complements the per-frame supervised losses: the network still gets 2D keypoint losses on each frame (to ensure accuracy per frame), and if available, some 3D supervision on some dataset, but additionally the adversarial loss pushes it to maintain realistic transitions.</p>
<p>By combining these, VIBE achieved both high per-frame accuracy and much improved temporal smoothness. In quantitative terms, on 3DPW (a dataset with ground-truth pose for videos), VIBE improved the PA-MPJPE and MPJPE by a noticeable margin over SPIN (which was state-of-the-art single-frame).</p>
<p>More importantly, it drastically reduced the acceleration error – a metric of jitter which measures frame-to-frame differences in pose accelerations. The outputs “looked” more like plausible motion; the paper’s Figure 1 illustrates how a previous method’s output had awkward inter-frame inconsistency while VIBE’s output was natural.</p>
<p><strong>Training with Unpaired Data</strong></p>
<p>One technical aspect: training VIBE required dealing with unpaired data – the mocap sequences in AMASS have no associated images (they are just 3D pose sequences). So the motion discriminator sees two kinds of input:
1. Poses produced by the regressor from video frames (these are “fake”)
2. Poses coming from AMASS (these are “real”)</p>
<p>The discriminator tries to distinguish them, while the regressor tries to fool it. This way, VIBE could train on real videos without 3D labels (for the adversarial part) and still benefit from the vast AMASS dataset (which provides examples of how limbs move, without needing to see the video).</p>
<p>This approach is analogous to HMR’s use of an adversarial prior for static poses, but extended to sequences as a temporal prior. As the authors note, this was critical because large-scale 3D motion annotations in the wild are scarce, but AMASS provides a rich repository of motions that can be used to regularize the network.</p>
<p>The final VIBE model was released with code and became a popular off-the-shelf solution for video pose estimation. Its limitations included occasional failure on very fast motions (where a GRU might lag) and the need for a pretrained keypoint detector (VIBE uses detected 2D keypoints during training as input as well, to better focus on the person). Nonetheless, VIBE set a new standard by demonstrating that temporal adversarial learning can significantly improve 3D human pose and shape estimation in videos.</p>
</section>
<section id="tcmr-temporally-consistent-mesh-recovery">
<h3>TCMR: Temporally Consistent Mesh Recovery<a class="headerlink" href="#tcmr-temporally-consistent-mesh-recovery" title="Link to this heading"></a></h3>
<p>While VIBE improved smoothness, it still operated mainly causally (forward in time with a GRU) and one could observe a trade-off: aggressive adversarial smoothing can sometimes lag the motion or oversmooth, sacrificing some accuracy for stability. In 2021, Choi et al. proposed TCMR (Temporally Consistent Mesh Recovery), which further addressed temporal consistency by architectural means rather than adversarial training.</p>
<p><strong>Key Idea</strong></p>
<p>TCMR introduces a two-stream temporal network:
- One stream (let’s call it PoseForecast) focuses on predicting the current pose from past and future frames
- The other (the main temporal encoder) processes the whole sequence including the current frame</p>
<p>By explicitly forecasting the current pose without using the current frame’s features, the network obtains a “second opinion” on what the pose should be, based purely on motion continuity. This forecast is then integrated with the actual current-frame-based prediction to yield the final output.</p>
<p>In doing so, they ensure that the current frame’s visual features (which might be noisy or temporarily confusing) do not solely dictate the estimation – the temporal prior can override or pull the solution toward a smooth trajectory.</p>
<p><strong>Implementation</strong></p>
<p>Concretely, TCMR uses a bidirectional recurrent network. One part of the network looks at a window of frames excluding the current one (or down-weights the current) to infer a residual pose update that would make the sequence smoother. This is added to or combined with the standard sequence encoding that does include the current frame.</p>
<p>At inference, they effectively remove the dependency on the current frame in one branch, so the final predictor is less dominated by instantaneous glitches. As the authors put it, they “remove the strong dependency on the current static feature” so the model “can focus on past and future frames without being dominated by the current frame”.</p>
<p>TCMR’s training still uses standard losses (2D keypoints, possibly 3D if available) but no adversarial component. By architecture design, it achieves smoothness.</p>
<p>The results showed that TCMR not only improved temporal consistency (as measured by acceleration error or similar metrics) by a large margin, but also slightly increased per-frame accuracy compared to VIBE.</p>
<p>Qualitatively, TCMR outputs are very stable – e.g., if a person’s arm is moving, the motion is fluid and without jitter, even better than VIBE which, under some circumstances, might wobble due to its GRU memory not perfectly capturing long-term context.</p>
<p>Because TCMR can leverage future frames (thanks to a bidirectional or non-causal design), it achieves a more stable pose at frame <span class="math notranslate nohighlight">\(t\)</span> knowing what happens at frame <span class="math notranslate nohighlight">\(t+1, t+2, ...\)</span> (of course, this means TCMR as described is an offline method, not suitable for real-time streaming use cases where future frames aren’t available; but one could run a delayed causal version in practice).</p>
<p>In summary, TCMR represents the refinement of temporal modeling: instead of adding another loss (like VIBE’s <span class="math notranslate nohighlight">\(L_{adv}\)</span>) to coerce the network into smooth behavior, TCMR builds the notion of forecasting and smoothing into the network’s structure. It validates that sequence modeling techniques (like bi-directional GRUs or sequence-to-sequence networks) can be very effective for human mesh recovery, yielding results that are both accurate and temporally coherent.</p>
</section>
<section id="motionbert-transformer-based-motion-representations">
<h3>MotionBERT: Transformer-Based Motion Representations<a class="headerlink" href="#motionbert-transformer-based-motion-representations" title="Link to this heading"></a></h3>
<p>The latest generation of video-based pose estimators employs transformers, which have shown great success in sequence modeling tasks across vision and NLP. MotionBERT by Zhu et al. (ICCV 2023) is a representative modern approach that offers a unified perspective on learning human motion representations.</p>
<p>MotionBERT is a bit different from previous methods in that it aims to pretrain a motion model that can be adapted to multiple tasks: 3D pose estimation from video, skeleton-based action recognition, etc. However, it also directly contributes to SMPL fitting by providing a powerful backbone for temporal pose estimation.</p>
<p><strong>Architecture</strong></p>
<p>MotionBERT uses a transformer-based encoder (specifically, a dual-stream transformer called DSTFormer in the paper) to process spatio-temporal pose data. It actually decouples the spatial and temporal attention:
- One stream handles relationships between different joints in a single frame (the spatial configuration)
- Another stream handles the temporal relationships of each joint over time</p>
<p>By alternating or combining these, the model captures complex motion patterns. This is in contrast to a GRU which might have trouble with long sequences or capturing global patterns (like periodic motion across many frames).</p>
<p><strong>Pretraining on Heterogeneous Data</strong></p>
<p>One of MotionBERT’s innovations is to leverage both motion capture data and large 2D keypoint datasets in a pretraining-finetuning paradigm.</p>
<p>In the pretraining stage, they generate sequences of 2D keypoints (some from real videos, some synthetically “masked” or noised) and train the transformer to lift them to 3D (essentially a 2D-to-3D pose reconstruction task). They corrupt the input in various ways (mask certain frames or joints) so that the model learns to fill in gaps, akin to BERT’s masking strategy but for motion.</p>
<p>This self-supervised or weakly-supervised training teaches the transformer a general notion of human motion continuity, even without explicit SMPL parameter output yet. Then, in fine-tuning, they specialize the model to specific tasks, one of which is 3D human mesh recovery from video: they attach a regression head that outputs SMPL pose and shape, and train on annotated datasets.</p>
<p>Because MotionBERT’s pretraining can use diverse data sources (2D pose sequences from YouTube, motion capture from AMASS, etc.), the resulting model has a strong prior for human motion. Fine-tuned on 3DPW or Human3.6M, it achieved state-of-the-art results. For instance, the paper reports a mean MPJPE of about 39.2 mm on Human3.6M, notably better than previous methods in the high-40s or 50s.</p>
<p>It also shows significant improvement in action recognition accuracy when using the learned representations, underlining the generality of the features. In the context of SMPL fitting, MotionBERT can be viewed as providing a very informed motion prior (even more implicitly than VIBE’s discriminator) – it’s “seen” a lot of motion during pretraining and thus will output pose sequences that both match the image evidence and make sense globally.</p>
<p><strong>Practical Considerations</strong></p>
<p>One difference in MotionBERT is that it often operates on 2D skeleton input rather than raw pixels (the version described in the paper lifts 2D keypoints to 3D). In a complete system, one could combine MotionBERT with an image-based pose detector: first extract 2D keypoints per frame, then use the transformer to produce the 3D pose and shape.</p>
<p>This decoupling can leverage the accuracy of 2D pose estimators (which are very mature) and the learned motion model for 3D, at the expense of not using the image fully (e.g., shape estimation might suffer if only skeletons are used).</p>
<p>However, nothing precludes using image features directly in a transformer for 3D human mesh – some recent works do exactly that, mixing CNN and transformer (a CNN extracts per-frame features, a transformer handles temporal aggregation, similar to how VIBE used a GRU). The trend is that transformers with long-range attention can better model motions that span longer times (like a full walking cycle or a dance move) than RNNs with limited memory.</p>
<p>In summary, MotionBERT pushes the frontier by applying modern sequence learning strategies and large-scale pretraining to human motion. It achieves a unified model that can be fine-tuned for 3D pose, shape, and even action understanding. For the task of human mesh recovery, it means we can get highly accurate and temporally stable predictions, leveraging both rich unlabeled video data and sophisticated architectures.</p>
</section>
</section>
<section id="comparison-of-learning-based-methods">
<h2>Comparison of Learning-Based Methods<a class="headerlink" href="#comparison-of-learning-based-methods" title="Link to this heading"></a></h2>
<p>Having surveyed major learning-based SMPL fitting techniques, we now compare them along key dimensions:</p>
<section id="supervision-and-data">
<h3>Supervision and Data<a class="headerlink" href="#supervision-and-data" title="Link to this heading"></a></h3>
<p>Early methods like HMR and NBF used primarily 2D annotated images and a bit of 3D mocap for adversarial training. They demonstrated that in-the-wild images with only 2D keypoints can be used to train a 3D regressor when combined with a strong prior (HMR’s discriminator or NBF’s part segmentation).</p>
<p>SPIN increased the use of mixed supervision: it exploited 2D keypoints from several datasets and 3D labels from controlled datasets, plus SMPLify fits as additional pseudo-labels. This resulted in a significant accuracy jump, showing the value of using all available data (the so-called “dataset fusion” training).</p>
<p>PyMAF and CLIFF continued this trend – for example, CLIFF used pseudo-3D annotations from its own annotator to augment training images without ground truth. Temporal methods similarly mix 2D keypoints from video (often obtained via an off-the-shelf 2D tracker) with datasets like 3DPW that have 3D sequences, and large mocap collections (for adversarial or pretraining).</p>
<p>In summary, modern approaches train on a melting pot of data: COCO, MPII, H36M, 3DHP, 3DPW, AMASS, etc., each providing different supervision signals. This has been crucial to reaching ~40-50 mm MPJPE levels.</p>
</section>
<section id="network-architecture">
<h3>Network Architecture<a class="headerlink" href="#network-architecture" title="Link to this heading"></a></h3>
<p>The progression has been from simple encoders (ResNet + MLP) to more complex ones with feedback or multi-stream designs:</p>
<ul class="simple">
<li><p><strong>HMR</strong>: Relatively simple (ResNet + IEF loop + discriminator)</p></li>
<li><p><strong>NBF</strong>: Two-stage (segmentation + regression) modular design</p></li>
<li><p><strong>SPIN</strong>: Simple architecture (ResNet + MLP head) but changed the training loop</p></li>
<li><p><strong>PyMAF</strong>: Internal loop with a feature pyramid and attention to spatial detail</p></li>
<li><p><strong>CLIFF</strong>: Extra inputs (full-frame info) but otherwise similar to HMR/SPIN</p></li>
<li><p><strong>PIXIE</strong>: Multiple networks with a moderator</p></li>
<li><p><strong>Video methods</strong>: Added RNNs (VIBE’s GRU, TCMR’s bi-GRU) or Transformers (MotionBERT)</p></li>
</ul>
<p>Notably, as architectures evolved, the number of parameters increased modestly but not drastically – even MotionBERT remains in the few dozens of million parameters range, which is feasible. It is the training data and strategy that often make the bigger difference.</p>
<p>One can view many of these methods as different network topologies exploring the design space: e.g., iterative vs. direct regression, single vs. multi-head (NBF could be seen as a multi-head network: one head for segmentation, one for pose).</p>
</section>
<section id="objective-functions">
<h3>Objective Functions<a class="headerlink" href="#objective-functions" title="Link to this heading"></a></h3>
<p>All methods rely on reprojection loss on keypoints as the primary driver when ground truth 3D is scarce. But they differ in auxiliary losses:</p>
<ul class="simple">
<li><p><strong>HMR</strong>: Adversarial pose prior loss</p></li>
<li><p><strong>NBF</strong>: Segmentation network has its cross-entropy loss; 3D parameter loss when possible</p></li>
<li><p><strong>SPIN</strong>: Uses the SMPLify energy as being minimized to provide labels</p></li>
<li><p><strong>PyMAF</strong>: Alignment loss (possibly silhouette or correspondence) to guide the pixel-aligned features</p></li>
<li><p><strong>CLIFF</strong>: Same losses as SPIN (2D/3D keypoints), but computed in a different coordinate frame</p></li>
<li><p><strong>PIXIE</strong>: Multi-part losses – face landmarks, body joints, hand joints, gender classification, and shape regularizer</p></li>
<li><p><strong>VIBE</strong>: Per-frame keypoint losses + adversarial sequence loss</p></li>
<li><p><strong>TCMR</strong>: Per-frame losses, but implicitly optimizes sequence smoothness through architecture</p></li>
<li><p><strong>MotionBERT</strong>: Pretraining uses masked reconstruction loss on motion, fine-tuning uses joint losses</p></li>
</ul>
</section>
<section id="pose-and-shape-priors">
<h3>Pose and Shape Priors<a class="headerlink" href="#pose-and-shape-priors" title="Link to this heading"></a></h3>
<p>Methods differ in how they ensure plausible outputs:</p>
<ul class="simple">
<li><p><strong>Optimization methods</strong>: Used explicit priors (like SMPLify’s pose prior GMM)</p></li>
<li><p><strong>HMR</strong>: Replaced that with a learned adversary</p></li>
<li><p><strong>NBF and others</strong>: Without adversaries sometimes include a mild <span class="math notranslate nohighlight">\(\ell_2\)</span> prior on shape (to keep <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> near zero)</p></li>
<li><p><strong>SPIN</strong>: Effectively used the prior baked into SMPLify’s results</p></li>
<li><p><strong>PyMAF and CLIFF</strong>: Rely on training with SPIN/HMR outputs and datasets</p></li>
<li><p><strong>PIXIE</strong>: Used gendered shape and the fact that face and body networks were trained on real data</p></li>
<li><p><strong>VIBE</strong>: Motion prior is the adversarial discriminator on sequences</p></li>
<li><p><strong>TCMR</strong>: Prior is its forecasting mechanism (trained on real sequences)</p></li>
<li><p><strong>MotionBERT</strong>: Prior comes from pretraining on massive motion data and implicitly learning physics</p></li>
</ul>
</section>
<section id="performance">
<h3>Performance<a class="headerlink" href="#performance" title="Link to this heading"></a></h3>
<p>It is difficult to cite exact numbers for all methods on all benchmarks (as they are often evaluated on different sets), but a general trend on the popular 3DPW dataset (which tests in-the-wild video, evaluated on MPJPE (mm) for pose accuracy) is as follows:</p>
<ul class="simple">
<li><p>SMPLify (optimization): ~100+ mm</p></li>
<li><p>HMR (2018): ~81 mm</p></li>
<li><p>NBF (2018): Similar to HMR (not reported in the same way, but competitive)</p></li>
<li><p>SPIN (2019): ~59 mm</p></li>
<li><p>GraphCMR (2019): ~70 mm</p></li>
<li><p>PyMAF (2021): ~58 mm (slight improvement over SPIN)</p></li>
<li><p>CLIFF (2022): ~52 mm (significant jump by solving global orientation)</p></li>
<li><p>PIXIE (2021): On body metrics similar to PyMAF</p></li>
<li><p>VIBE (2020): ~51 mm on 3DPW</p></li>
<li><p>TCMR (2021): ~46-47 mm on 3DPW</p></li>
<li><p>MotionBERT (2023): ~45 mm or lower on 3DPW</p></li>
</ul>
<p>These numbers are indicative (and often quoted after Procrustes alignment, etc., so one must be careful). The bottom line is that accuracy (in terms of joint error) has roughly halved from 2018 to 2023, thanks to learning-based approaches.</p>
<p>Qualitatively, the field has gone from wobbly, coarse reconstructions to quite realistic and stable reconstructions of human motion in everyday videos.</p>
</section>
<section id="runtime">
<h3>Runtime<a class="headerlink" href="#runtime" title="Link to this heading"></a></h3>
<p>Learning methods are generally fast. An HMR or SPIN model runs in ~30-60 FPS on a GPU (ResNet-50 based). VIBE, with a GRU over a 16-frame batch, also achieves near real-time performance (still on the order of 10-20 FPS).</p>
<p>TCMR and MotionBERT, if run non-causally on full sequences, are more offline and could be slower, but they can be chunked. In contrast, optimization like SMPLify took several seconds per image. Thus, learning-based methods enable applications like live motion capture from a webcam or processing large video datasets for analytics.</p>
</section>
<section id="strengths-and-weaknesses">
<h3>Strengths and Weaknesses<a class="headerlink" href="#strengths-and-weaknesses" title="Link to this heading"></a></h3>
<p><strong>Regression methods (HMR, SPIN, etc.)</strong>:
- <strong>Strengths</strong>: Very fast, can leverage big data to avoid local minima, straightforward to integrate with modern frameworks
- <strong>Weaknesses</strong>: May fail for poses unseen in training, can be thrown off by occlusions or unseen camera viewpoints, and without careful priors can output odd poses</p>
<p><strong>Hybrid methods (SPIN, EFT)</strong>:
- <strong>Strengths</strong>: Get accuracy boost from optimization, can train with less 3D GT
- <strong>Weaknesses</strong>: More complex training, still requires good 2D detections</p>
<p><strong>Feedback methods (PyMAF)</strong>:
- <strong>Strengths</strong>: Better alignment, addresses some shortcomings of one-shot regression
- <strong>Weaknesses</strong>: Slightly more computation, still ultimately limited by training data distribution</p>
<p><strong>Full-frame methods (CLIFF)</strong>:
- <strong>Strengths</strong>: Resolves global orientation and position, needed for multi-person or camera-aware scenarios
- <strong>Weaknesses</strong>: Requires known intrinsics or at least consistent bounding box references, and multi-person extension requires detecting and tracking multiple people</p>
<p><strong>Whole-body (PIXIE)</strong>:
- <strong>Strengths</strong>: Combines best of specialized methods, achieves detailed face/hand reconstruction
- <strong>Weaknesses</strong>: More network components, needs multi-domain data, might not generalize if one part is in extreme conditions</p>
<p><strong>Temporal (VIBE, TCMR)</strong>:
- <strong>Strengths</strong>: Smooth, physically plausible output, can handle momentary occlusions or drop in keypoint detection by using temporal context
- <strong>Weaknesses</strong>: Often need a buffering of frames (lag or offline processing), and might not resolve inherently ambiguous poses</p>
<p><strong>Transformer (MotionBERT)</strong>:
- <strong>Strengths</strong>: Can capture long-term dependencies and be pretrained on huge data, thus very robust
- <strong>Weaknesses</strong>: Typically requires a lot of training data and computing power, might rely on external 2D detections that have their own errors (if not end-to-end)</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<p>Learning-based methods for fitting SMPL to images have dramatically advanced the state of the art. By combining deep networks with model knowledge (the SMPL parametric space), they achieve fast and accurate 3D human pose and shape estimation.</p>
<p>The field has progressed from basic CNN regressors with weak priors to sophisticated systems that integrate optimization, multi-scale reasoning, and temporal modeling. These methods form the foundation for downstream tasks in vision (human action recognition, AR/VR avatar creation, biomechanical analysis, etc.), and many are available as open-source projects.</p>
<p>In future lectures, we will explore how these pose/shape estimators can be extended to handle clothing, hair, and interaction with objects, moving closer to capturing the full complexity of humans in the real world.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_06_1_SMPL_optimization.html" class="btn btn-neutral float-left" title="Lecture 06.1 - Fitting the SMPL Model to Images via Optimization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lecture_07_1_fitting_SMPL_to_IMU_optimization.html" class="btn btn-neutral float-right" title="Lecture 07.1: Fitting SMPL to IMU Data Using Optimization-Based Methods" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>