

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lecture 02.4 - Conversational AI Survey (BFITraitTalk_AI Tutorial) &mdash; Generative AI in Psychological Research 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/documentation_options.js?v=f2a433a1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Lecture 02.2 – Privacy Considerations" href="lecture_02_2_privacy_considerations.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Generative AI in Psychological Research
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="lecture_overview.html">Generative AI in Psychological Research: Course Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Module 1: Foundations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html">Lecture 01.1 – Introduction to Generative AI in Psychology</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html">Lecture 01.2 – Survey Design with Large Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_synthetic_respondents.html">Lecture 01.3 – Synthetic Respondents: Simulation and Supplementation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Module 2: Data Collection</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html">Lecture 02.1 – Interactive AI Surveys</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_privacy_considerations.html">Lecture 02.2 – Privacy Considerations</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lecture 02.4 - Conversational AI Survey (BFITraitTalk_AI Tutorial)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#setup">Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#installation-steps">Installation Steps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#codebase-walkthrough">Codebase Walkthrough</a></li>
<li class="toctree-l2"><a class="reference internal" href="#frontend-design-user-interface">Frontend Design (User Interface)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#survey-logic-and-adaptive-interview-flow">Survey Logic and Adaptive Interview Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#backend-structure-and-data-flow">Backend Structure and Data Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#psychological-design-considerations">Psychological Design Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ethical-and-methodological-considerations">Ethical and Methodological Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#customization-and-extension-ideas">Customization and Extension Ideas</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Generative AI in Psychological Research</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Lecture 02.4 - Conversational AI Survey (BFITraitTalk_AI Tutorial)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lecture_02_3_bfitraittalk_ai_tutorial.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lecture-02-4-conversational-ai-survey-bfitraittalk-ai-tutorial">
<h1>Lecture 02.4 - Conversational AI Survey (BFITraitTalk_AI Tutorial)<a class="headerlink" href="#lecture-02-4-conversational-ai-survey-bfitraittalk-ai-tutorial" title="Link to this heading">¶</a></h1>
<p><strong>Lecture Objectives:</strong> In this tutorial, we walk through a working prototype app that delivers a standard psychology survey (the Big Five Inventory) via a conversational AI interviewer. This is the practical follow-up to Lecture 01.2 (survey design) and Lecture 02.1 (interactive AI interviewing). By the end, you should understand how the app’s code integrates a large language model into a survey, how it handles the dialog and data, and how this approach aligns with (and challenges) traditional survey methodology.</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">¶</a></h2>
<p>BFITraitTalk_AI is a Flask-based web application that administers the <strong>Big Five Inventory (BFI)</strong> personality questionnaire in a chat format. Instead of filling out a form with checkboxes, the user interacts with an AI persona (named “Kaya”) through a chat interface. <strong>Kaya</strong> asks each BFI question in a conversational manner, understands the user’s free-text answers, and then suggests a numeric score (1 through 5) for that item based on the answer. The user can confirm or correct this suggested score before the AI moves on. The interface dynamically updates a survey form on the side with the confirmed answers and, once all questions are answered, shows a personalized Big Five profile summary.</p>
<p><strong>How this fits into AI-augmented survey research:</strong> Traditionally, surveys present fixed statements and ask respondents to select predefined options (e.g. “Strongly Agree” to “Strongly Disagree”). This ensures standardized data but can feel impersonal and may miss nuances of why a respondent chose an answer. Conversely, human-led interviews allow open-ended responses and clarification but are costly and hard to scale. Lecture 02.1 introduced the idea that large language models (LLMs) can act as <em>adaptive interviewers</em>, potentially offering the best of both: the <strong>scalability and structure of surveys</strong> with the <strong>rapport and adaptiveness of interviews</strong>. BFITraitTalk_AI is a proof-of-concept of this idea. It uses <strong>Google’s Gemma 3</strong> LLM running locally to conduct the interview. All conversation occurs on the user’s machine, ensuring privacy (no data leaves to an external API). The primary goal is to explore whether an AI interviewer can make the survey experience more engaging and insightful while still collecting quantifiable, standardized responses.</p>
<p><strong>Key Features:</strong></p>
<ul class="simple">
<li><p><em>Conversational Delivery:</em> The AI asks BFI statements one by one in a natural, dialogic way (e.g. <em>“Would you say you see yourself as someone who is talkative?”</em> instead of a dry prompt). Users respond in their own words.</p></li>
<li><p><em>Dynamic Interpretation:</em> The AI interprets each free-text answer and <strong>proposes a Likert score</strong> (1=Strongly Disagree to 5=Strongly Agree). For example, if a user responds <em>“I’m only talkative around close friends”</em>, Kaya might say <em>“Sounds like you’re moderately talkative. I’d score that around 3 out of 5. Does that feel right?”</em>.</p></li>
<li><p><em>Adaptive Flow:</em> If the user confirms, the AI records the score and <strong>seamlessly proceeds to the next question</strong>. If the user disagrees or is unsure, the AI can ask for clarification or let the user provide the correct number. The AI can also handle follow-up questions from the user (e.g., “What does ‘talkative’ mean in this context?”) by offering an explanation and then re-asking the item.</p></li>
<li><p><em>Local and Privacy-Preserving:</em> The app runs the Gemma 3 model via Hugging Face on the local machine, so all personal responses stay local. This design was chosen to address ethical concerns about data privacy (discussed later).</p></li>
<li><p><em>Result Summary:</em> After all 44 BFI items are answered, the app computes the Big Five trait scores and displays a summary profile (e.g., <em>“You scored high on Extraversion”</em> along with a brief interpretation).</p></li>
</ul>
<p><strong>A Sample Dialog:</strong> The following excerpt illustrates a typical interaction:</p>
<p><strong>Kaya (AI):</strong> <em>“Let’s begin! First, thinking about yourself, how much do you agree with the statement: ‘I see myself as someone who is talkative’? You can answer in your own words.”</em></p>
<p><strong>User:</strong> <em>“Hmm, I’m actually pretty quiet around new people, but very chatty once I get comfortable.”</em></p>
<p><strong>Kaya (AI):</strong> <em>“Got it. So you’re talkative in some situations but not others. It sounds like you **somewhat agree*</em> with the statement. I’d tentatively put that at a <strong>3 out of 5</strong>. Does that feel right to you?”*</p>
<p><strong>User:</strong> <em>“Yeah, that sounds about right.”</em></p>
<p><em>(The app records a score of 3 for “is talkative” and Kaya continues to the next item.)</em></p>
<p>This approach keeps the user engaged and allows them to explain their answers. Meanwhile, the system still obtains a structured numeric score for each question, maintaining compatibility with traditional survey analysis.</p>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">¶</a></h2>
<p>In this section, we cover how to install and run BFITraitTalk_AI on your local machine. The app has a few heavy dependencies (like the LLM model itself), so be prepared for a large download and the need for a decent GPU for best performance.</p>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Python 3.9+</strong> – The code is tested on Python 3.10.</p></li>
<li><p><strong>Git</strong> – To clone the repository.</p></li>
<li><p><strong>PyTorch and GPU drivers</strong> – An NVIDIA CUDA-compatible GPU is <strong>highly recommended</strong>. The Gemma-3 model has billions of parameters, so running on CPU will be extremely slow (though possible). Ensure you have appropriate NVIDIA CUDA drivers installed for your GPU.</p></li>
<li><p><strong>Disk Space</strong> – Several gigabytes free for the model files.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Gemma-3 models come in different sizes (4B, 12B, 27B parameters). The larger the model, the better the AI’s responses <em>and</em> the more VRAM required. For example, the 4B model can run in ~5 GB GPU memory (8-bit quantized), whereas the 12B and 27B models need substantially more. In this tutorial, we’ll assume use of the 4B or 12B model on a typical modern GPU.</p>
</div>
</section>
<section id="installation-steps">
<h3>Installation Steps<a class="headerlink" href="#installation-steps" title="Link to this heading">¶</a></h3>
<ol class="arabic">
<li><p><strong>Clone the Repository:</strong> Get the code from GitHub.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/treese41528/BFITraitTalk_AI.git
<span class="gp">$ </span><span class="nb">cd</span><span class="w"> </span>BFITraitTalk_AI
</pre></div>
</div>
</li>
<li><p><strong>Create a Virtual Environment:</strong> (optional, but good practice) Create an isolated environment for the project and activate it.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>venv
<span class="gp">$ </span><span class="nb">source</span><span class="w"> </span>venv/bin/activate<span class="w">   </span><span class="c1"># on Linux/Mac</span>
<span class="gp">$ </span>.<span class="se">\\</span>venv<span class="se">\\</span>Scripts<span class="se">\\</span>activate<span class="w">  </span><span class="c1"># on Windows</span>
</pre></div>
</div>
<p><em>(After activation, your console prompt should show `(venv)`.)</em></p>
</li>
<li><p><strong>Install Python Dependencies:</strong> Use pip to install required libraries.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp gp-VirtualEnv">(venv)</span><span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>pip
<span class="gp gp-VirtualEnv">(venv)</span><span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
<p>This will install Flask, Transformers (for the Hugging Face LLM integration), <strong>BitsAndBytes</strong> (for 4-bit quantization), and other necessary packages.</p>
</li>
<li><p><strong>Download the Gemma 3 Model:</strong> The LLM weights are not shipped with the repository due to size. You have two options:</p>
<ul>
<li><p><strong>Automated download script (recommended):</strong> The repo provides <code class="docutils literal notranslate"><span class="pre">utils/gemma_downloader.py</span></code> which uses Hugging Face Hub. You must have git LFS installed or be logged in to Hugging Face if the model requires it. For example, to download the 12B instruction-tuned Gemma-3 model:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp gp-VirtualEnv">(venv)</span><span class="gp">$ </span>python<span class="w"> </span>utils/gemma_downloader.py<span class="w"> </span>--model_size<span class="w"> </span>12b<span class="w"> </span>--variant<span class="w"> </span>it
</pre></div>
</div>
<p>This will fetch the model files (several GB) and place them under <code class="docutils literal notranslate"><span class="pre">data/hf_models/gemma-3-12b-it/</span></code>. You can similarly download <code class="docutils literal notranslate"><span class="pre">4b</span></code> or <code class="docutils literal notranslate"><span class="pre">27b</span></code> by changing the argument.</p>
</li>
<li><p><strong>Manual download:</strong> Alternatively, download the model files for <em>``google/gemma-3-4b-it``</em>, <em>``gemma-3-12b-it``</em>, etc. from Hugging Face using your browser or <code class="docutils literal notranslate"><span class="pre">huggingface-cli</span></code>. Then place the files under <code class="docutils literal notranslate"><span class="pre">BFITraitTalk_AI/data/hf_models/&lt;model-folder&gt;</span></code> exactly as the script would (e.g., <code class="docutils literal notranslate"><span class="pre">data/hf_models/gemma-3-4b-it/</span></code>). Ensure the directory name matches one of the expected names (see next step).</p></li>
</ul>
</li>
<li><p><strong>Configure the App (optional):</strong> Open the <code class="docutils literal notranslate"><span class="pre">config.py</span></code> file in the project. Here you can set which model size and quantization to use, among other settings. For instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MODEL_SIZE</span> <span class="o">=</span> <span class="s2">&quot;4b&quot;</span>       <span class="c1"># Options: &quot;4b&quot;, &quot;12b&quot;, &quot;27b&quot;</span>
<span class="n">QUANTIZATION</span> <span class="o">=</span> <span class="s2">&quot;4bit&quot;</span>   <span class="c1"># Use 4-bit compression to save VRAM</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span>         <span class="c1"># &quot;cuda&quot; to force GPU, &quot;cpu&quot; to force CPU (auto picks GPU if available)</span>
</pre></div>
</div>
<p>By default, MODEL_SIZE is “12b” and QUANTIZATION is “4bit”. If you downloaded a 4B model, set this to “4b” or the app won’t find the model files. You can also leave these as-is and use environment variables to override (e.g., export GEMMA_MODEL_SIZE=4b before running, as described in the README).</p>
</li>
<li><p><strong>Run the App:</strong> With everything in place, start the Flask server:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp gp-VirtualEnv">(venv)</span><span class="gp">$ </span>python<span class="w"> </span>survey_app.py
</pre></div>
</div>
<p>The first time, the model will load into memory, which can take up to a few minutes. You’ll see log messages indicating the model is being loaded (and possibly that weights are being quantized to 4-bit). Once ready, the console will show a message like:</p>
<ul class="simple">
<li><p>Running on <a class="reference external" href="http://127.0.0.1:5000/">http://127.0.0.1:5000/</a> (Press CTRL+C to quit)</p></li>
</ul>
<p>Now open a web browser and go to <a class="reference external" href="http://localhost:5000">http://localhost:5000</a>. You should see the BFITraitTalk_AI interface. Click the “Start Interview” button to begin the chat.</p>
</li>
</ol>
</section>
</section>
<section id="codebase-walkthrough">
<h2>Codebase Walkthrough<a class="headerlink" href="#codebase-walkthrough" title="Link to this heading">¶</a></h2>
<p>Now, let’s dive into how the app is structured under the hood. The repository is organized into several modules:</p>
<ul class="simple">
<li><p><strong>Flask App (survey_app.py)</strong>: The main web application code, including route handlers and integration of all components.</p></li>
<li><p><strong>LLM Module (llm/ directory)</strong>: Code for loading the Gemma 3 model and handling LLM interactions (prompt formatting, generating responses, etc.).</p></li>
<li><p><strong>Utilities (utils/ directory)</strong>: Helper logic, including the session manager (tracking interview state), response parser, BFI scoring, and the model downloader.</p></li>
<li><p><strong>Frontend (HTML/JS/CSS in templates/ and static/)</strong>: The user interface files.</p></li>
</ul>
<p>We’ll examine each part to see how the conversational survey works end-to-end.</p>
</section>
<section id="frontend-design-user-interface">
<h2>Frontend Design (User Interface)<a class="headerlink" href="#frontend-design-user-interface" title="Link to this heading">¶</a></h2>
<p>BFITraitTalk_AI uses a classic web stack for the UI: HTML templates + CSS + JavaScript, served by Flask. There is no heavy front-end framework; the focus is on simplicity and transparency. The UI is split into two main panels:</p>
<ul class="simple">
<li><p><strong>Left Panel – Chat Interface</strong>: This panel shows the conversation between the user and Kaya (the AI). It has a scrollable chat history and an input box for the user to type their messages. When you click “Start Interview,” the first AI question appears here. As you continue, user messages and AI replies appear in this chat log.</p></li>
<li><p><strong>Right Panel – Questionnaire Form</strong>: This panel lists the BFI questions and records answers as they are confirmed. Each question displays a 5-point Likert scale plus a skip option. The current question being discussed is highlighted with a blue border. As answers are confirmed, they appear in the form with the selected score highlighted.</p></li>
</ul>
<p>The HTML template <code class="docutils literal notranslate"><span class="pre">templates/index.html</span></code> defines this two-panel structure, with a chat panel and a form panel:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;container&quot;</span><span class="p">&gt;</span>
    <span class="cm">&lt;!-- Chat Panel (Left Side) --&gt;</span>
    <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;panel chat-panel&quot;</span><span class="p">&gt;</span>
        <span class="cm">&lt;!-- Chat messages go here --&gt;</span>
        <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;chat-messages&quot;</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;chatMessages&quot;</span><span class="p">&gt;</span>
            <span class="cm">&lt;!-- Messages will be added here dynamically --&gt;</span>
            <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;typing-indicator&quot;</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;typingIndicator&quot;</span><span class="p">&gt;</span>
                <span class="p">&lt;</span><span class="nt">span</span><span class="p">&gt;&lt;/</span><span class="nt">span</span><span class="p">&gt;&lt;</span><span class="nt">span</span><span class="p">&gt;&lt;/</span><span class="nt">span</span><span class="p">&gt;&lt;</span><span class="nt">span</span><span class="p">&gt;&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
            <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
        <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
        <span class="cm">&lt;!-- Chat input form --&gt;</span>
        <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;chat-input&quot;</span><span class="p">&gt;</span>
            <span class="p">&lt;</span><span class="nt">form</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;chatForm&quot;</span><span class="p">&gt;</span>
                <span class="p">&lt;</span><span class="nt">input</span> <span class="na">type</span><span class="o">=</span><span class="s">&quot;text&quot;</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;userInput&quot;</span> <span class="na">placeholder</span><span class="o">=</span><span class="s">&quot;Type your response here...&quot;</span> <span class="na">disabled</span><span class="p">&gt;</span>
                <span class="p">&lt;</span><span class="nt">button</span> <span class="na">type</span><span class="o">=</span><span class="s">&quot;submit&quot;</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;sendButton&quot;</span> <span class="na">disabled</span><span class="p">&gt;</span>
                    <span class="cm">&lt;!-- Send icon SVG here --&gt;</span>
                <span class="p">&lt;/</span><span class="nt">button</span><span class="p">&gt;</span>
            <span class="p">&lt;/</span><span class="nt">form</span><span class="p">&gt;</span>
            <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;control-buttons&quot;</span><span class="p">&gt;</span>
                <span class="p">&lt;</span><span class="nt">button</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;startButton&quot;</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;primary-button&quot;</span><span class="p">&gt;</span>Start Interview<span class="p">&lt;/</span><span class="nt">button</span><span class="p">&gt;</span>
                <span class="p">&lt;</span><span class="nt">button</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;resetButton&quot;</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;secondary-button&quot;</span> <span class="na">style</span><span class="o">=</span><span class="s">&quot;display: none;&quot;</span><span class="p">&gt;</span>Reset<span class="p">&lt;/</span><span class="nt">button</span><span class="p">&gt;</span>
            <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
        <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>

    <span class="cm">&lt;!-- Form Panel (Right Side) --&gt;</span>
    <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;panel form-panel&quot;</span><span class="p">&gt;</span>
        <span class="cm">&lt;!-- Questions are rendered here with Likert scales --&gt;</span>
        <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;questions-container&quot;</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;questionsContainer&quot;</span><span class="p">&gt;</span>
            <span class="cm">&lt;!-- Each question is structured like this: --&gt;</span>
            <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;question-item&quot;</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;question-{{ question.id }}&quot;</span> <span class="na">data-question-id</span><span class="o">=</span><span class="s">&quot;{{ question.id }}&quot;</span> <span class="na">data-trait</span><span class="o">=</span><span class="s">&quot;{{ question.trait|lower }}&quot;</span><span class="p">&gt;</span>
                <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;question-text&quot;</span><span class="p">&gt;</span>
                    <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;question-number&quot;</span><span class="p">&gt;</span>{{ loop.index }}.<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
                    <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;statement&quot;</span><span class="p">&gt;</span>I see myself as someone who {{ question.text }}<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
                    {% if question.reverse %}
                    <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;reverse-indicator&quot;</span><span class="p">&gt;</span>(R)<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
                    {% endif %}
                    <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;trait-badge {{ question.trait|lower }}&quot;</span><span class="p">&gt;</span>{{ question.trait }}<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
                <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>

                <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;likert-scale&quot;</span><span class="p">&gt;</span>
                    <span class="cm">&lt;!-- 5-point scale plus skip option --&gt;</span>
                    {% for value in range(1, 6) %}
                    <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;scale-option&quot;</span> <span class="na">data-value</span><span class="o">=</span><span class="s">&quot;{{ value }}&quot;</span> <span class="na">data-question-id</span><span class="o">=</span><span class="s">&quot;{{ question.id }}&quot;</span><span class="p">&gt;</span>
                        <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;scale-number&quot;</span><span class="p">&gt;</span>{{ value }}<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
                    <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
                    {% endfor %}
                    <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;scale-option skip&quot;</span> <span class="na">data-value</span><span class="o">=</span><span class="s">&quot;skipped&quot;</span> <span class="na">data-question-id</span><span class="o">=</span><span class="s">&quot;{{ question.id }}&quot;</span><span class="p">&gt;</span>
                        <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;scale-number&quot;</span><span class="p">&gt;</span>S<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
                    <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
                <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>

                <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;answer-status&quot;</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;status-{{ question.id }}&quot;</span><span class="p">&gt;&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
            <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
            <span class="cm">&lt;!-- Results section (initially hidden) --&gt;</span>
            <span class="p">&lt;</span><span class="nt">div</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;resultsSection&quot;</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;results-section&quot;</span> <span class="na">style</span><span class="o">=</span><span class="s">&quot;display: none;&quot;</span><span class="p">&gt;</span>
                <span class="p">&lt;</span><span class="nt">h3</span><span class="p">&gt;</span>Your Big Five Personality Profile<span class="p">&lt;/</span><span class="nt">h3</span><span class="p">&gt;</span>
                <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;trait-results&quot;</span> <span class="na">id</span><span class="o">=</span><span class="s">&quot;traitResults&quot;</span><span class="p">&gt;</span>
                    <span class="cm">&lt;!-- Results will be added here dynamically --&gt;</span>
                <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
            <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
        <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>The styling is handled by <code class="docutils literal notranslate"><span class="pre">static/css/style.css</span></code>, which defines the colors, layouts, and interactive elements like the chat bubbles and Likert scale options.</p>
<p><strong>Interactive Behavior</strong>: The JavaScript (<code class="docutils literal notranslate"><span class="pre">static/js/interview.js</span></code>) manages all client-side interactivity. It handles:</p>
<ol class="arabic simple">
<li><p><strong>Starting the interview</strong>: When the user clicks the “Start Interview” button, it makes a POST request to <code class="docutils literal notranslate"><span class="pre">/api/start</span></code> and then displays the AI’s first message.</p></li>
<li><p><strong>Sending and receiving messages</strong>: The <code class="docutils literal notranslate"><span class="pre">handleChatSubmit()</span></code> function captures user input and sends it to <code class="docutils literal notranslate"><span class="pre">/api/chat</span></code>, then displays the AI’s response when it comes back.</p></li>
<li><p><strong>Updating the form</strong>: When the AI confirms a score, the <code class="docutils literal notranslate"><span class="pre">updateForm()</span></code> function highlights the selected option in the form panel and updates the answer status.</p></li>
<li><p><strong>Highlighting the current question</strong>: The <code class="docutils literal notranslate"><span class="pre">highlightQuestion()</span></code> function adds a CSS class to visually indicate which question is currently being discussed.</p></li>
<li><p><strong>Managing the progress bar</strong>: As questions are answered, the progress percentage is updated via the <code class="docutils literal notranslate"><span class="pre">updateProgress()</span></code> function.</p></li>
<li><p><strong>Displaying results</strong>: When all questions are complete, the <code class="docutils literal notranslate"><span class="pre">displayResults()</span></code> function shows the personality profile summary.</p></li>
</ol>
<p>Users can answer questions either by typing in the chat or by directly clicking the Likert scale options in the form panel. Direct scale clicks are handled by the <code class="docutils literal notranslate"><span class="pre">initializeScaleOptions()</span></code> function, which automatically sends the selected value to the chat.</p>
<p><strong>LLM Integration (Gemma 3 Model)</strong></p>
<p>The core “intelligence” of the app comes from the Gemma 3 large language model integrated via the Hugging Face Transformers library. Let’s examine how the actual code handles the integration, focusing particularly on the <cite>GemmaConversationHandler</cite> class from <cite>llm/gemma_handler.py</cite>.</p>
<p>The conversation handler is responsible for managing interactions with the Gemma 3 model. Here’s how the key parts are implemented:</p>
<ol class="arabic simple">
<li><p><strong>Initialization</strong>:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">default_generation_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the conversation handler.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>

    <span class="c1"># Store default generation params</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">default_generation_params</span> <span class="o">=</span> <span class="n">default_generation_params</span> <span class="k">if</span> <span class="n">default_generation_params</span> <span class="k">else</span> <span class="p">{}</span>
    <span class="c1"># Set some basic defaults if none provided at all</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_generation_params</span><span class="p">:</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">default_generation_params</span> <span class="o">=</span> <span class="p">{</span>
             <span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
             <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
             <span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
             <span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s2">&quot;repetition_penalty&quot;</span><span class="p">:</span> <span class="mf">1.1</span>
         <span class="p">}</span>
         <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No default generation params provided, using basic defaults: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">default_generation_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
         <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Conversation handler initialized with default generation params: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">default_generation_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Verify we have a valid model and tokenizer</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Model or tokenizer is None. Cannot initialize conversation handler.&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Model and Tokenizer must be provided to GemmaConversationHandler&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Conversation handler initialized successfully&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>Formatting Conversations</strong>:</p></li>
</ol>
<p>The handler formats the chat history into a prompt the model can understand using either the tokenizer’s built-in chat template or a manual formatting method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">format_conversation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Formats the chat history for Gemma 3 model.</span>

<span class="sd">    Args:</span>
<span class="sd">        chat_history: List of dictionaries with &#39;role&#39; and &#39;content&#39; keys</span>
<span class="sd">                      e.g., [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: &#39;...&#39;},</span>
<span class="sd">                            {&#39;role&#39;: &#39;model&#39;, &#39;content&#39;: &#39;...&#39;}]</span>

<span class="sd">    Returns:</span>
<span class="sd">        Optional[str]: Formatted prompt string for the model, or None if formatting fails</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">:</span>
         <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Tokenizer not available for formatting.&quot;</span><span class="p">)</span>
         <span class="k">return</span> <span class="kc">None</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Formatting chat history (length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">chat_history</span><span class="p">)</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">chat_history</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># Verify chat_history format</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chat_history</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
             <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;chat_history must be a list, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">chat_history</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">turn</span> <span class="ow">in</span> <span class="n">chat_history</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">turn</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">or</span> <span class="s1">&#39;role&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">turn</span> <span class="ow">or</span> <span class="s1">&#39;content&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">turn</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid turn format: </span><span class="si">{</span><span class="n">turn</span><span class="si">}</span><span class="s2">. Must be dict with &#39;role&#39; and &#39;content&#39;.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">turn</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid role: </span><span class="si">{</span><span class="n">turn</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">. Must be &#39;user&#39; or &#39;model&#39;.&quot;</span><span class="p">)</span>

        <span class="c1"># Check if tokenizer has chat template method</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;apply_chat_template&quot;</span><span class="p">):</span>
            <span class="c1"># Use built-in chat template (recommended approach)</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
                <span class="n">chat_history</span><span class="p">,</span>
                <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># Adds the final &#39;&lt;start_of_turn&gt;model\n&#39;</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using tokenizer&#39;s apply_chat_template method.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Manual formatting for Gemma 3 (fallback)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Tokenizer does not have apply_chat_template method, using manual formatting&quot;</span><span class="p">)</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_manually_format_conversation</span><span class="p">(</span><span class="n">chat_history</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">prompt</span>

    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error formatting conversation: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">exc_info</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
</pre></div>
</div>
<p>If the tokenizer doesn’t have an <cite>apply_chat_template</cite> method, the handler falls back to manual formatting:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_manually_format_conversation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Fallback manual formatting &quot;&quot;&quot;</span>
    <span class="c1"># Simple implementation, might need refinement based on exact model expectations</span>
    <span class="n">prompt_str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">turn</span> <span class="ow">in</span> <span class="n">chat_history</span><span class="p">:</span>
         <span class="n">prompt_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;&lt;start_of_turn&gt;</span><span class="si">{</span><span class="n">turn</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">turn</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&lt;end_of_turn&gt;</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">prompt_str</span> <span class="o">+=</span> <span class="s2">&quot;&lt;start_of_turn&gt;model</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="k">return</span> <span class="n">prompt_str</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p><strong>Generating Responses</strong>:</p></li>
</ol>
<p>The most critical method is <cite>generate_response()</cite>, which takes the chat history and generates the model’s next response:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_response</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">chat_history</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="n">max_new_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">repetition_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span> <span class="c1"># Allow passing other generate() args</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a response from the model based on chat history.</span>
<span class="sd">    Uses stored default generation parameters, allowing overrides.</span>

<span class="sd">    Args:</span>
<span class="sd">        chat_history: List of conversation turns</span>
<span class="sd">        max_new_tokens (Optional[int]): Override default max_new_tokens.</span>
<span class="sd">        temperature (Optional[float]): Override default temperature.</span>
<span class="sd">        top_p (Optional[float]): Override default top_p.</span>
<span class="sd">        do_sample (Optional[bool]): Override default do_sample.</span>
<span class="sd">        repetition_penalty (Optional[float]): Override default repetition_penalty.</span>
<span class="sd">        **kwargs: Additional keyword arguments passed directly to model.generate().</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: Generated response or error message</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">:</span>
         <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Cannot generate response: Model or Tokenizer not initialized.&quot;</span><span class="p">)</span>
         <span class="k">return</span> <span class="s2">&quot;Error: LLM components not ready.&quot;</span>

    <span class="c1"># Format the conversation into a prompt</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">format_conversation</span><span class="p">(</span><span class="n">chat_history</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;Error: Could not format conversation for the model.&quot;</span>

    <span class="c1"># Combine default and override parameters</span>
    <span class="n">gen_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_generation_params</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="c1"># Start with defaults</span>

    <span class="c1"># Apply overrides if provided</span>
    <span class="k">if</span> <span class="n">max_new_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">gen_params</span><span class="p">[</span><span class="s1">&#39;max_new_tokens&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_new_tokens</span>
    <span class="k">if</span> <span class="n">temperature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">gen_params</span><span class="p">[</span><span class="s1">&#39;temperature&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">temperature</span>
    <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">gen_params</span><span class="p">[</span><span class="s1">&#39;top_p&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">top_p</span>
    <span class="k">if</span> <span class="n">do_sample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">gen_params</span><span class="p">[</span><span class="s1">&#39;do_sample&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">do_sample</span>
    <span class="k">if</span> <span class="n">repetition_penalty</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">gen_params</span><span class="p">[</span><span class="s1">&#39;repetition_penalty&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">repetition_penalty</span>

    <span class="c1"># Add any other kwargs passed directly</span>
    <span class="n">gen_params</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Ensure essential params have some value</span>
    <span class="n">gen_params</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">&#39;max_new_tokens&#39;</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
    <span class="n">gen_params</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">&#39;pad_token_id&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">)</span>


    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Get the device from the model</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
             <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Model has no parameters loaded.&quot;</span><span class="p">)</span>
             <span class="k">return</span> <span class="s2">&quot;Error: Model parameters not loaded.&quot;</span>

        <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generating response using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generation parameters being used: </span><span class="si">{</span><span class="n">gen_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Tokenize the prompt</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span>
            <span class="n">prompt</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Generate response</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Generating response...&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
                <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span> <span class="c1"># Pass input_ids directly</span>
                <span class="o">**</span><span class="n">gen_params</span> <span class="c1"># Pass combined generation parameters</span>
            <span class="p">)</span>

        <span class="c1"># Extract only the newly generated tokens, not the prompt</span>
        <span class="n">input_length</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Handle potential edge case where output is shorter than input</span>
        <span class="k">if</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">input_length</span><span class="p">:</span>
             <span class="n">new_tokens</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">input_length</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
             <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Output sequence length is not greater than input length. Returning empty.&quot;</span><span class="p">)</span>
             <span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Decode the response</span>
        <span class="n">response_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">)</span><span class="si">}</span><span class="s2"> tokens&quot;</span><span class="p">)</span>

        <span class="c1"># Clean up response</span>
        <span class="n">response_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clean_response</span><span class="p">(</span><span class="n">response_text</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">response_text</span>

    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error generating response: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">exc_info</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;I&#39;m having difficulty generating a response. Let&#39;s try again.&quot;</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p><strong>Cleaning Responses</strong>:</p></li>
</ol>
<p>After generating a response, the handler cleans it up to remove any special tokens:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_clean_response</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Clean up the generated response. &quot;&quot;&quot;</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="c1"># Basic cleanup, may need refinement based on observed model outputs</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;|end_of_turn|&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;|start_of_turn|&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="c1"># Handle potential variations</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;end_of_turn&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;&lt;start_of_turn&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;(The AI seems to have generated an empty response. Let&#39;s try again.)&quot;</span>
    <span class="k">return</span> <span class="n">text</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p><strong>Detecting Questions in AI Responses</strong>:</p></li>
</ol>
<p>The handler includes a method to detect if the AI’s response includes a new question:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">detect_next_question</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Detects if the response includes a new question. &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">response_text</span><span class="p">:</span> <span class="k">return</span> <span class="kc">False</span>
    <span class="n">text_lower</span> <span class="o">=</span> <span class="n">response_text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

    <span class="c1"># Check for question marks (but not just clarifications like &quot;Does that sound right?&quot;)</span>
    <span class="k">if</span> <span class="s1">&#39;?&#39;</span> <span class="ow">in</span> <span class="n">text_lower</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;(?:sound right|is that correct|okay)\?$&#39;</span><span class="p">,</span> <span class="n">text_lower</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Detected &#39;?&#39; potentially indicating a next question.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="c1"># Look for common question starters, avoiding simple confirmations</span>
    <span class="n">question_starters</span> <span class="o">=</span> <span class="p">[</span> <span class="s2">&quot;how much do you&quot;</span><span class="p">,</span> <span class="s2">&quot;thinking about&quot;</span><span class="p">,</span> <span class="s2">&quot;next question&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;let&#39;s move on to&quot;</span><span class="p">,</span> <span class="s2">&quot;how about&quot;</span><span class="p">,</span> <span class="s2">&quot;do you see yourself&quot;</span> <span class="p">]</span>
    <span class="k">for</span> <span class="n">starter</span> <span class="ow">in</span> <span class="n">question_starters</span><span class="p">:</span>
         <span class="k">if</span> <span class="n">starter</span> <span class="ow">in</span> <span class="n">text_lower</span><span class="p">:</span>
              <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Detected potential question starter: &#39;</span><span class="si">{</span><span class="n">starter</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
              <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>
</pre></div>
</div>
<p>These methods form the core of the GemmaConversationHandler, which enables the BFITraitTalk_AI application to have intelligent, conversational interactions with users. The handler takes care of properly formatting the conversation history, generating appropriate responses, and processing those responses before they’re returned to the user.</p>
<p><strong>Initial System Prompt</strong></p>
<p>The AI’s behavior is guided by an initial system prompt defined in <code class="docutils literal notranslate"><span class="pre">survey_app.py</span></code>. This prompt establishes Kaya’s role and instructions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">INITIAL_INTERVIEW_PROMPT</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">You are &#39;Kaya&#39;, a friendly and professional AI interviewer conducting a personality assessment using the Big Five Inventory (BFI).</span>
<span class="s2">Your goal is to guide the user through the questionnaire conversationally.</span>

<span class="s2">Instructions:</span>
<span class="s2">1. Introduce yourself briefly and explain the process (conversational survey, answer in own words).</span>
<span class="s2">2. Ask one BFI question at a time. Phrase the questions naturally, starting with &quot;Thinking about yourself, how much do you agree with: &#39;I see myself as someone who... [statement text]?&#39;&quot; or similar conversational phrasing.</span>
<span class="s2">3. Wait for the user&#39;s response.</span>
<span class="s2">4. Practice active listening: Briefly acknowledge or summarize the user&#39;s answer (e.g., &quot;Okay, so you feel...&quot;, &quot;Got it, thanks for sharing.&quot;).</span>
<span class="s2">5. **Crucially**: After acknowledging, try to map their response to the 5-point Likert scale (1: Strongly Disagree, 2: Disagree, 3: Neutral, 4: Agree, 5: Strongly Agree). You **MUST state the score you are recording clearly using phrases like &#39;I will mark that as a [score]&#39; or &#39;Okay, recording a [score] for that one.&#39;** before asking if it feels right. For example: &quot;Based on what you said, **I will mark that as a 4** out of 5 for Agree. Does that feel right?&quot; or &quot;Okay, **recording a 1** for Strongly Disagree.&quot;</span>
<span class="s2">6. **Wait for confirmation**: After stating the score, explicitly ask the user if that score is correct (e.g., &quot;Does that feel right?&quot;, &quot;Is that accurate?&quot;). Do NOT ask the next BFI question until the user confirms the score for the current one (e.g., responds with &#39;yes&#39;, &#39;correct&#39;, etc.).</span>
<span class="s2">7. If the user corrects the score (e.g., &quot;no, make it a 3&quot;), acknowledge the correction and state the *new* score you are recording (e.g., &quot;My apologies, recording a 3 then.&quot;) and proceed to the next question.</span>
<span class="s2">8. If the user asks to skip a question, acknowledge it supportively (&quot;Okay, no problem, we can skip that one.&quot;) and record it as &#39;skipped&#39;, then ask the next question.</span>
<span class="s2">9. Maintain a warm, empathetic, and non-judgmental tone throughout.</span>
<span class="s2">10. After the user confirms a score/skip, seamlessly transition to the *next logical question* from the BFI sequence.</span>
<span class="s2">11. When all questions are done, provide a brief closing statement thanking the user.</span>

<span class="s2">Let&#39;s begin the interview now. Please start with your introduction and the first question from the inventory.</span>
<span class="s2">&quot;&quot;&quot;</span>
</pre></div>
</div>
<p>This prompt plays a crucial role in guiding the LLM’s behavior, ensuring it maintains the right tone and follows the specific protocol of asking questions, interpreting answers, and confirming scores.</p>
</section>
<section id="survey-logic-and-adaptive-interview-flow">
<h2>Survey Logic and Adaptive Interview Flow<a class="headerlink" href="#survey-logic-and-adaptive-interview-flow" title="Link to this heading">¶</a></h2>
<p>The heart of the application is the state machine that manages the interview flow. This is primarily handled by the <code class="docutils literal notranslate"><span class="pre">InterviewSessionManager</span></code> in <code class="docutils literal notranslate"><span class="pre">utils/session_manager.py</span></code> and the interview logic in <code class="docutils literal notranslate"><span class="pre">survey_app.py</span></code>.</p>
<p><strong>Session State Management</strong></p>
<p>Each interview session maintains a state dictionary with the following key components:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">session_state</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="n">session_id</span><span class="p">,</span>  <span class="c1"># Unique identifier</span>
    <span class="s1">&#39;state&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">STATES</span><span class="p">[</span><span class="s1">&#39;NOT_STARTED&#39;</span><span class="p">],</span>  <span class="c1"># Current state in the state machine</span>
    <span class="s1">&#39;started_at&#39;</span><span class="p">:</span> <span class="n">current_time</span><span class="p">,</span>  <span class="c1"># Timestamp</span>
    <span class="s1">&#39;last_updated&#39;</span><span class="p">:</span> <span class="n">current_time</span><span class="p">,</span>  <span class="c1"># Timestamp</span>
    <span class="s1">&#39;chat_history&#39;</span><span class="p">:</span> <span class="p">[],</span>  <span class="c1"># List of all messages exchanged</span>
    <span class="s1">&#39;current_question_id&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># ID of the question being asked</span>
    <span class="s1">&#39;last_question_id&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># ID of the most recently answered question</span>
    <span class="s1">&#39;answered_questions&#39;</span><span class="p">:</span> <span class="p">{},</span>  <span class="c1"># Dictionary mapping question IDs to answers</span>
    <span class="s1">&#39;remaining_question_ids&#39;</span><span class="p">:</span> <span class="n">question_ids</span><span class="p">,</span>  <span class="c1"># List of IDs still to be asked</span>
    <span class="s1">&#39;pending_answer&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># Temporary storage for proposed but unconfirmed answers</span>
    <span class="s1">&#39;completed_at&#39;</span><span class="p">:</span> <span class="kc">None</span>  <span class="c1"># Timestamp when completed</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The session state machine has five possible states:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">STATES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;NOT_STARTED&#39;</span><span class="p">:</span> <span class="s1">&#39;not_started&#39;</span><span class="p">,</span>
    <span class="s1">&#39;IN_PROGRESS&#39;</span><span class="p">:</span> <span class="s1">&#39;in_progress&#39;</span><span class="p">,</span>
    <span class="s1">&#39;AWAITING_CONFIRMATION&#39;</span><span class="p">:</span> <span class="s1">&#39;awaiting_confirmation&#39;</span><span class="p">,</span>
    <span class="s1">&#39;COMPLETED&#39;</span><span class="p">:</span> <span class="s1">&#39;completed&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ERROR&#39;</span><span class="p">:</span> <span class="s1">&#39;error&#39;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Let’s trace the flow of a single question through this state machine:</p>
<p><strong>1. Starting the Interview</strong></p>
<p>When the user clicks “Start Interview”, the <code class="docutils literal notranslate"><span class="pre">/api/start</span></code> route is called, which:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@app</span><span class="o">.</span><span class="n">route</span><span class="p">(</span><span class="s1">&#39;/api/start&#39;</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;POST&#39;</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">start_interview</span><span class="p">():</span>
    <span class="c1"># Create and initialize a new session</span>
    <span class="n">session_state</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">create_session</span><span class="p">()</span>
    <span class="n">session_state</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">start_interview</span><span class="p">(</span><span class="n">session_state</span><span class="p">)</span>

    <span class="c1"># Get the first question</span>
    <span class="n">first_qid</span> <span class="o">=</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;current_question_id&#39;</span><span class="p">)</span>
    <span class="n">first_question_text_core</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">get_question_text_by_id</span><span class="p">(</span><span class="n">first_qid</span><span class="p">)</span>
    <span class="n">first_question_full</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;I see myself as someone who </span><span class="si">{</span><span class="n">first_question_text_core</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="c1"># Create the initial prompt with instructions + first question</span>
    <span class="n">initial_prompt_with_q</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    </span><span class="si">{</span><span class="n">INITIAL_INTERVIEW_PROMPT</span><span class="si">}</span>

<span class="s2">    Let&#39;s begin. Thinking about yourself, how much do you agree with: &#39;</span><span class="si">{</span><span class="n">first_question_full</span><span class="si">}</span><span class="s2">&#39;?</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="c1"># Send this to the model and get the first response</span>
    <span class="n">initial_history_for_ai</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">initial_prompt_with_q</span><span class="p">}]</span>
    <span class="n">ai_response</span> <span class="o">=</span> <span class="n">conversation_handler</span><span class="o">.</span><span class="n">generate_response</span><span class="p">(</span><span class="n">initial_history_for_ai</span><span class="p">,</span> <span class="o">**</span><span class="n">generation_params</span><span class="p">)</span>

    <span class="c1"># Add both the prompt and the AI&#39;s response to the chat history</span>
    <span class="n">session_state</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">add_message_to_history</span><span class="p">(</span><span class="n">session_state</span><span class="p">,</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="n">initial_prompt_with_q</span><span class="p">)</span>
    <span class="n">session_state</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">add_message_to_history</span><span class="p">(</span><span class="n">session_state</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">ai_response</span><span class="p">)</span>

    <span class="c1"># Save the state and return the AI&#39;s greeting + first question</span>
    <span class="n">session</span><span class="p">[</span><span class="s1">&#39;session_state&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">session_state</span>
    <span class="k">return</span> <span class="n">jsonify</span><span class="p">({</span>
        <span class="s1">&#39;success&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;ai_message&#39;</span><span class="p">:</span> <span class="n">ai_response</span><span class="p">,</span>
        <span class="s1">&#39;interview_state&#39;</span><span class="p">:</span> <span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">],</span>
        <span class="s1">&#39;current_question_id&#39;</span><span class="p">:</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;current_question_id&#39;</span><span class="p">)</span>
    <span class="p">})</span>
</pre></div>
</div>
<p>At this point, the session state is <code class="docutils literal notranslate"><span class="pre">IN_PROGRESS</span></code> and the first question is being asked.</p>
<p><strong>2. User Answers the Question</strong></p>
<p>When the user responds to the question, the <code class="docutils literal notranslate"><span class="pre">/api/chat</span></code> route processes the message:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@app</span><span class="o">.</span><span class="n">route</span><span class="p">(</span><span class="s1">&#39;/api/chat&#39;</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;POST&#39;</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">process_chat</span><span class="p">():</span>
    <span class="c1"># Get user message and current session state</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">json</span>
    <span class="n">user_message</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;message&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="n">session_state</span> <span class="o">=</span> <span class="n">session</span><span class="p">[</span><span class="s1">&#39;session_state&#39;</span><span class="p">]</span>

    <span class="c1"># Add user message to chat history</span>
    <span class="n">session_state</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">add_message_to_history</span><span class="p">(</span><span class="n">session_state</span><span class="p">,</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="n">user_message</span><span class="p">)</span>

    <span class="c1"># Get current state information</span>
    <span class="n">current_internal_state</span> <span class="o">=</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;state&#39;</span><span class="p">)</span>
    <span class="n">current_qid</span> <span class="o">=</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;current_question_id&#39;</span><span class="p">)</span>
    <span class="n">last_qid</span> <span class="o">=</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;last_question_id&#39;</span><span class="p">)</span>
    <span class="n">pending_answer</span> <span class="o">=</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;pending_answer&#39;</span><span class="p">)</span>

    <span class="c1"># Clone history for generation</span>
    <span class="n">history_for_generation</span> <span class="o">=</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;chat_history&#39;</span><span class="p">,</span> <span class="p">[])[:]</span>

    <span class="c1"># Process based on current state</span>
    <span class="k">if</span> <span class="n">current_internal_state</span> <span class="o">==</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">STATES</span><span class="p">[</span><span class="s1">&#39;IN_PROGRESS&#39;</span><span class="p">]:</span>
        <span class="c1"># User has answered a question, need to get AI&#39;s interpretation</span>
        <span class="k">if</span> <span class="n">looks_like_direct_answer</span><span class="p">(</span><span class="n">user_message</span><span class="p">):</span>
            <span class="c1"># If it&#39;s a direct answer (like just &quot;4&quot;), add instruction for AI</span>
            <span class="n">instruction</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            Based on the user&#39;s last message (&#39;</span><span class="si">{</span><span class="n">user_message</span><span class="si">}</span><span class="s2">&#39;), please:</span>
<span class="s2">            1. Briefly acknowledge their response naturally.</span>
<span class="s2">            2. Analyze their sentiment regarding the statement.</span>
<span class="s2">            3. Determine the most likely score (1-5) or if they indicated skipping.</span>
<span class="s2">            4. State the score you are proposing clearly.</span>
<span class="s2">            5. Explicitly ask for confirmation: &#39;Does that feel right?&#39;.</span>
<span class="s2">            &quot;&quot;&quot;</span>
            <span class="n">history_for_generation</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">[System instruction: </span><span class="si">{</span><span class="n">instruction</span><span class="si">}</span><span class="s2">]&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Regular conversational response</span>
            <span class="n">instruction</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            The user didn&#39;t provide a direct score rating in their last message (&#39;</span><span class="si">{</span><span class="n">user_message</span><span class="si">}</span><span class="s2">&#39;).</span>
<span class="s2">            Instead, respond conversationally to their message, then re-ask the original question.</span>
<span class="s2">            &quot;&quot;&quot;</span>
            <span class="n">history_for_generation</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">[System instruction: </span><span class="si">{</span><span class="n">instruction</span><span class="si">}</span><span class="s2">]&quot;</span>

        <span class="c1"># Generate AI&#39;s response</span>
        <span class="n">ai_response_text</span> <span class="o">=</span> <span class="n">conversation_handler</span><span class="o">.</span><span class="n">generate_response</span><span class="p">(</span><span class="n">history_for_generation</span><span class="p">)</span>

        <span class="c1"># Check if AI proposed a score in its response</span>
        <span class="n">proposed_answer_details</span> <span class="o">=</span> <span class="n">response_parser</span><span class="o">.</span><span class="n">extract_confirmed_answer</span><span class="p">(</span><span class="n">ai_response_text</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">proposed_answer_details</span><span class="p">:</span>
            <span class="c1"># AI included a clear score proposal</span>
            <span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;pending_answer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">proposed_answer_details</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="n">proposed_answer_details</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]}</span>
            <span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">STATES</span><span class="p">[</span><span class="s1">&#39;AWAITING_CONFIRMATION&#39;</span><span class="p">]</span>
            <span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;last_question_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_qid</span>
            <span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;current_question_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">elif</span> <span class="n">current_internal_state</span> <span class="o">==</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">STATES</span><span class="p">[</span><span class="s1">&#39;AWAITING_CONFIRMATION&#39;</span><span class="p">]:</span>
        <span class="c1"># Checking if user confirmed or rejected the proposed score</span>
        <span class="n">user_confirmation</span> <span class="o">=</span> <span class="n">response_parser</span><span class="o">.</span><span class="n">check_user_confirmation</span><span class="p">(</span><span class="n">user_message</span><span class="p">)</span>
        <span class="n">confirmation_qid</span> <span class="o">=</span> <span class="n">last_qid</span>

        <span class="k">if</span> <span class="n">user_confirmation</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1"># User confirmed the score</span>
            <span class="n">session_state</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">record_answer</span><span class="p">(</span><span class="n">session_state</span><span class="p">,</span> <span class="n">confirmation_qid</span><span class="p">,</span> <span class="n">pending_answer</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">])</span>
            <span class="n">form_update</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;question_id&#39;</span><span class="p">:</span> <span class="n">confirmation_qid</span><span class="p">,</span> <span class="s1">&#39;answer&#39;</span><span class="p">:</span> <span class="n">pending_answer</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]}</span>

            <span class="k">if</span> <span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">STATES</span><span class="p">[</span><span class="s1">&#39;COMPLETED&#39;</span><span class="p">]:</span>
                <span class="c1"># All questions answered</span>
                <span class="n">ai_response_text</span> <span class="o">=</span> <span class="s2">&quot;Great, got it. That completes the questionnaire! Thank you.&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Ask next question</span>
                <span class="n">next_qid</span> <span class="o">=</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;current_question_id&#39;</span><span class="p">)</span>
                <span class="n">next_question_text_core</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">get_question_text_by_id</span><span class="p">(</span><span class="n">next_qid</span><span class="p">)</span>
                <span class="n">next_question_full</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;I see myself as someone who </span><span class="si">{</span><span class="n">next_question_text_core</span><span class="si">}</span><span class="s2">&quot;</span>

                <span class="n">instruction</span><span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Great, thanks for confirming. Now, let&#39;s move to the next one. Thinking about yourself, how much do you agree with: &#39;</span><span class="si">{</span><span class="n">next_question_full</span><span class="si">}</span><span class="s2">&#39;?&quot;</span>
                <span class="n">history_for_generation</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">[System instruction: </span><span class="si">{</span><span class="n">instruction</span><span class="si">}</span><span class="s2">]&quot;</span>
                <span class="n">ai_response_text</span> <span class="o">=</span> <span class="n">conversation_handler</span><span class="o">.</span><span class="n">generate_response</span><span class="p">(</span><span class="n">history_for_generation</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">user_confirmation</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="c1"># User rejected the proposed score</span>
            <span class="n">question_text_core</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">get_question_text_by_id</span><span class="p">(</span><span class="n">confirmation_qid</span><span class="p">)</span>
            <span class="n">instruction</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;My apologies. What score (1-5) should I record for &#39;</span><span class="si">{</span><span class="n">question_text_core</span><span class="si">}</span><span class="s2">&#39; instead? Or &#39;skip&#39;?&quot;</span>
            <span class="n">history_for_generation</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">[System instruction: </span><span class="si">{</span><span class="n">instruction</span><span class="si">}</span><span class="s2">]&quot;</span>
            <span class="n">ai_response_text</span> <span class="o">=</span> <span class="n">conversation_handler</span><span class="o">.</span><span class="n">generate_response</span><span class="p">(</span><span class="n">history_for_generation</span><span class="p">)</span>

            <span class="c1"># Reset to IN_PROGRESS with the same question</span>
            <span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">STATES</span><span class="p">[</span><span class="s1">&#39;IN_PROGRESS&#39;</span><span class="p">]</span>
            <span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;current_question_id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">confirmation_qid</span>
            <span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;pending_answer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Unclear confirmation - acknowledge and guide back</span>
            <span class="n">instruction</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            I understand you might have more to say or questions. However, to keep us on track,</span>
<span class="s2">            I just need to confirm the score for the last statement.</span>
<span class="s2">            The score I suggested was </span><span class="si">{</span><span class="n">pending_answer</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">.</span>
<span class="s2">            Could you please tell me if that score feels right (&#39;yes&#39;) or wrong (&#39;no&#39;)?</span>
<span class="s2">            Or tell me the score (1-5) you&#39;d like me to record, or say &#39;skip&#39;.</span>
<span class="s2">            &quot;&quot;&quot;</span>
            <span class="n">history_for_generation</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">[System instruction: </span><span class="si">{</span><span class="n">instruction</span><span class="si">}</span><span class="s2">]&quot;</span>
            <span class="n">ai_response_text</span> <span class="o">=</span> <span class="n">conversation_handler</span><span class="o">.</span><span class="n">generate_response</span><span class="p">(</span><span class="n">history_for_generation</span><span class="p">)</span>
            <span class="c1"># Stay in AWAITING_CONFIRMATION state</span>

    <span class="c1"># Add AI&#39;s response to history</span>
    <span class="n">session_state</span> <span class="o">=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">add_message_to_history</span><span class="p">(</span><span class="n">session_state</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">ai_response_text</span><span class="p">)</span>

    <span class="c1"># Save updated state</span>
    <span class="n">session</span><span class="p">[</span><span class="s1">&#39;session_state&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">session_state</span>

    <span class="c1"># Return response to frontend</span>
    <span class="k">return</span> <span class="n">jsonify</span><span class="p">({</span>
        <span class="s1">&#39;success&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s1">&#39;ai_message&#39;</span><span class="p">:</span> <span class="n">ai_response_text</span><span class="p">,</span>
        <span class="s1">&#39;form_update&#39;</span><span class="p">:</span> <span class="n">form_update</span><span class="p">,</span>
        <span class="s1">&#39;current_question_id&#39;</span><span class="p">:</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;current_question_id&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;last_question_id&#39;</span><span class="p">),</span>
        <span class="s1">&#39;is_completed&#39;</span><span class="p">:</span> <span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;state&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">STATES</span><span class="p">[</span><span class="s1">&#39;COMPLETED&#39;</span><span class="p">],</span>
        <span class="s1">&#39;progress&#39;</span><span class="p">:</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">get_interview_stats</span><span class="p">(</span><span class="n">session_state</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;progress_percentage&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">})</span>
</pre></div>
</div>
<p>The above code is simplified for clarity, but it demonstrates the core state machine logic. Let’s trace through the key states:</p>
<ol class="arabic simple">
<li><p><strong>IN_PROGRESS</strong>: The AI has asked a question and is waiting for the user’s answer. After the user responds, the AI interprets their answer and proposes a score, then the state changes to AWAITING_CONFIRMATION.</p></li>
<li><p><strong>AWAITING_CONFIRMATION</strong>: The AI has proposed a score and is waiting for the user to confirm. There are three possibilities:
- User confirms (e.g., “yes”) → Record the answer, move to the next question (back to IN_PROGRESS), or complete if done
- User rejects (e.g., “no”) → Ask for the correct score, go back to IN_PROGRESS for the same question
- User gives unclear response → Request clarification, stay in AWAITING_CONFIRMATION</p></li>
<li><p><strong>COMPLETED</strong>: All questions have been answered. The app displays the final personality profile.</p></li>
</ol>
<p><strong>Response Parsing</strong></p>
<p>An important component is the <code class="docutils literal notranslate"><span class="pre">ResponseParser</span></code> class in <code class="docutils literal notranslate"><span class="pre">utils/response_parser.py</span></code>, which analyzes text to:</p>
<ol class="arabic simple">
<li><p>Extract score confirmations from the AI’s responses using regex patterns:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">extract_confirmed_answer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extracts EXPLICIT score or skip confirmation from AI text.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="p">:</span> <span class="k">return</span> <span class="kc">None</span>
    <span class="n">text_lower</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

    <span class="c1"># 1. Check for EXPLICIT scores first</span>
    <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_patterns</span><span class="p">:</span>
        <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text_lower</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">score</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
                <span class="k">if</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">score</span> <span class="o">&lt;=</span> <span class="mi">5</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parser extracted explicit score: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2"> via pattern &#39;</span><span class="si">{</span><span class="n">pattern</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
                    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">score</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;score&#39;</span><span class="p">,</span> <span class="s1">&#39;match_text&#39;</span><span class="p">:</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">0</span><span class="p">)}</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">IndexError</span><span class="p">):</span> <span class="k">continue</span>

    <span class="c1"># 2. Check for EXPLICIT skips</span>
    <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_patterns</span><span class="p">:</span>
        <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text_lower</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parser detected explicit skip via pattern &#39;</span><span class="si">{</span><span class="n">pattern</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="s1">&#39;skipped&#39;</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;skip&#39;</span><span class="p">,</span> <span class="s1">&#39;match_text&#39;</span><span class="p">:</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">0</span><span class="p">)}</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;No explicit score or skip confirmation found in AI text.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">ResponseParser</span></code> looks for specific patterns in the AI’s text that indicate it has assigned a score or marked a question as skipped. The patterns are defined when the parser is initialized:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the parser with relevant patterns.&quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Initializing ResponseParser&quot;</span><span class="p">)</span>
    <span class="c1"># Patterns for detecting explicit score confirmations</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">score_patterns</span> <span class="o">=</span> <span class="p">[</span>
        <span class="c1"># Made &#39;as a&#39; optional, added &#39;recording&#39;</span>
        <span class="sa">r</span><span class="s1">&#39;(?:mark|record|recording|put|rate|score|set)\s+(?:that|it|this|you)\s+(?:as\s+(?:a\s+)?|at\s+)?(\d)(?:\s+(?:out\s+of\s+5|on the scale|points))?&#39;</span><span class="p">,</span>
        <span class="sa">r</span><span class="s1">&#39;sounds\s+like\s+(?:a\s+)?(\d)(?:\s+to\s+me)?&#39;</span><span class="p">,</span>
        <span class="sa">r</span><span class="s1">&#39;put\s+you\s+down\s+as\s+(?:a\s+)?(\d)&#39;</span>
    <span class="p">]</span>
    <span class="c1"># Patterns for detecting skips</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">skip_patterns</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">r</span><span class="s1">&#39;(?:we\s+(?:can|will)|I</span><span class="se">\&#39;</span><span class="s1">ll|I\s+will)\s+skip\s+(?:that|this|the\s+question|it)&#39;</span><span class="p">,</span> <span class="c1"># Added &#39;it&#39;</span>
        <span class="sa">r</span><span class="s1">&#39;(?:let</span><span class="se">\&#39;</span><span class="s1">s|we\s+can)\s+move\s+(?:on|to\s+the\s+next)&#39;</span><span class="p">,</span> <span class="c1"># Made broader</span>
        <span class="sa">r</span><span class="s1">&#39;(?:mark|record)(?:ing|ed)?\s+(?:that|this|it)\s+as\s+skipped&#39;</span><span class="p">,</span>
        <span class="sa">r</span><span class="s1">&#39;no\s+problem\b.{0,25}\b(?:skip|mov(?:e|ing)|next)&#39;</span><span class="p">,</span> <span class="c1"># Slightly longer context window</span>
        <span class="sa">r</span><span class="s1">&#39;we\s+can\s+absolutely\s+(?:skip|move)&#39;</span><span class="p">,</span>
        <span class="sa">r</span><span class="s1">&#39;okay\s+to\s+skip&#39;</span>
    <span class="p">]</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Check if a user message confirms or rejects a proposed score:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">check_user_confirmation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if user text indicates &#39;yes&#39; or &#39;no&#39; confirmation.</span>

<span class="sd">    Args:</span>
<span class="sd">        text: The user&#39;s response text.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Optional[bool]: True for yes, False for no, None for unclear.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">text</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="n">text_lower</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

    <span class="c1"># Define patterns for &#39;yes&#39; and &#39;no&#39;</span>
    <span class="n">yes_patterns</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">r</span><span class="s2">&quot;^\s*yes\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*yeah\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*yep\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*correct\b.*&quot;</span><span class="p">,</span>
        <span class="sa">r</span><span class="s2">&quot;^\s*that&#39;s right\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*sounds right\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*accurate\b.*&quot;</span><span class="p">,</span>
        <span class="sa">r</span><span class="s2">&quot;^\s*confirm\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*ok(ay)?\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*sure\b.*&quot;</span><span class="p">,</span>
        <span class="c1"># Specific positive responses to &quot;Does that feel right?&quot;</span>
         <span class="sa">r</span><span class="s2">&quot;^\s*it does\b.*&quot;</span>
    <span class="p">]</span>
    <span class="n">no_patterns</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">r</span><span class="s2">&quot;^\s*no\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*nope\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*incorrect\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*wrong\b.*&quot;</span><span class="p">,</span>
        <span class="sa">r</span><span class="s2">&quot;^\s*that&#39;s not right\b.*&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;^\s*not really\b.*&quot;</span><span class="p">,</span>
        <span class="c1"># Specific negative responses like &quot;no, make it a 3&quot;</span>
        <span class="sa">r</span><span class="s2">&quot;^\s*no,.*(?:score|rate|mark|value|make it).*\d&quot;</span><span class="p">,</span>
         <span class="sa">r</span><span class="s2">&quot;^\s*actually,.*(?:score|rate|mark|value|make it).*\d&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># Check for &#39;yes&#39;</span>
    <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="n">yes_patterns</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text_lower</span><span class="p">):</span>
            <span class="c1"># Avoid matching things like &quot;no problem&quot; as yes</span>
            <span class="k">if</span> <span class="s2">&quot;no problem&quot;</span> <span class="ow">in</span> <span class="n">text_lower</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_lower</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">15</span><span class="p">:</span>
                 <span class="k">continue</span> <span class="c1"># Treat &quot;no problem&quot; alone as ambiguous or skip-related</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;User confirmation detected: YES (pattern: &#39;</span><span class="si">{</span><span class="n">pattern</span><span class="si">}</span><span class="s2">&#39;)&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">True</span>

    <span class="c1"># Check for &#39;no&#39;</span>
    <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="n">no_patterns</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text_lower</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;User confirmation detected: NO (pattern: &#39;</span><span class="si">{</span><span class="n">pattern</span><span class="si">}</span><span class="s2">&#39;)&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

    <span class="c1"># If neither yes nor no is clearly detected</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;User confirmation unclear.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">None</span>
</pre></div>
</div>
<p>This function analyzes user messages to determine if they’re confirming or rejecting the AI’s proposed score. It returns True for confirmation (e.g., “yes,” “correct”), False for rejection (e.g., “no,” “that’s wrong”), or None if the message is unclear.</p>
<p>The ResponseParser also has a method to detect which BFI question the AI is asking based on patterns in the AI’s text:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">detect_asked_question</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ai_response_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">questions_data</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Tries to identify which BFI question ID was asked in the AI&#39;s response. &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ai_response_text</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">questions_data</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="n">extracted_text</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Try patterns in order of specificity</span>
    <span class="n">match</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bfi_question_pattern_std</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">ai_response_text</span><span class="p">)</span> <span class="ow">or</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">bfi_question_pattern_direct</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">ai_response_text</span><span class="p">)</span> <span class="ow">or</span> \
            <span class="bp">self</span><span class="o">.</span><span class="n">bfi_question_pattern_quoted_core</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">ai_response_text</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">match</span><span class="p">:</span>
        <span class="n">extracted_text</span> <span class="o">=</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="n">extracted_text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;^[.…,;:-]+|[.…,;:-]+$&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">extracted_text</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Parser trying to match extracted question text: &#39;</span><span class="si">{</span><span class="n">extracted_text</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">extracted_text</span><span class="p">:</span>
             <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Extracted text was empty after cleaning.&quot;</span><span class="p">)</span>
             <span class="k">return</span> <span class="kc">None</span>

        <span class="n">best_match_id</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Exact Match First</span>
        <span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">questions_data</span><span class="p">:</span>
            <span class="n">q_text_lower</span> <span class="o">=</span> <span class="n">question</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="c1"># Remove &quot;I see myself as someone who&quot; prefix if present in data for robust matching</span>
            <span class="n">q_text_lower_core</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;^i see myself as someone who\s*&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">q_text_lower</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">q_id</span> <span class="o">=</span> <span class="n">question</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;id&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">q_text_lower_core</span> <span class="ow">and</span> <span class="n">q_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">q_text_lower_core</span> <span class="o">==</span> <span class="n">extracted_text</span><span class="p">:</span>
                     <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Detected QID </span><span class="si">{</span><span class="n">q_id</span><span class="si">}</span><span class="s2"> via EXACT text match.&quot;</span><span class="p">)</span>
                     <span class="k">return</span> <span class="n">q_id</span> <span class="c1"># Return immediately on exact match</span>

        <span class="c1"># Try substring matching if no exact match found</span>
        <span class="n">possible_matches</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">question</span> <span class="ow">in</span> <span class="n">questions_data</span><span class="p">:</span>
            <span class="c1"># ... substring matching logic ...</span>

    <span class="k">return</span> <span class="n">best_match_id</span>
</pre></div>
</div>
<p>Through these parsing functions, the app can understand both the AI’s responses (to extract proposed scores) and the user’s responses (to determine confirmation or rejection). This enables the state machine to properly handle the interview flow.</p>
</section>
<section id="backend-structure-and-data-flow">
<h2>Backend Structure and Data Flow<a class="headerlink" href="#backend-structure-and-data-flow" title="Link to this heading">¶</a></h2>
<p>Now let’s examine how the application components work together to manage the flow of data:</p>
<p><strong>Component Initialization</strong></p>
<p>When the Flask application starts, it initializes all the necessary components:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">initialize_components</span><span class="p">(</span><span class="n">app_instance</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize all global components.&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">model_manager</span><span class="p">,</span> <span class="n">conversation_handler</span><span class="p">,</span> <span class="n">session_manager</span><span class="p">,</span> <span class="n">bfi_scorer</span><span class="p">,</span> <span class="n">bfi_questions</span><span class="p">,</span> <span class="n">response_parser</span>

    <span class="c1"># Load BFI questions from JSON file</span>
    <span class="n">bfi_questions</span> <span class="o">=</span> <span class="n">load_bfi_questions</span><span class="p">()</span>
    <span class="n">app_instance</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;BFI_QUESTIONS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bfi_questions</span>

    <span class="c1"># Initialize session manager</span>
    <span class="n">session_manager</span> <span class="o">=</span> <span class="n">InterviewSessionManager</span><span class="p">(</span><span class="n">questions_data</span><span class="o">=</span><span class="n">bfi_questions</span><span class="p">)</span>

    <span class="c1"># Initialize BFI scorer for processing results</span>
    <span class="n">bfi_scorer</span> <span class="o">=</span> <span class="n">BFIScorer</span><span class="p">(</span><span class="n">bfi_questions</span><span class="p">)</span>

    <span class="c1"># Initialize model</span>
    <span class="n">model_manager</span> <span class="o">=</span> <span class="n">GemmaModelManager</span><span class="p">(</span>
        <span class="n">model_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">MODEL_SIZE</span><span class="p">,</span>
        <span class="n">quantization</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">QUANTIZATION</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">,</span>
        <span class="n">use_flash_attention</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">USE_FLASH_ATTENTION</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">model_manager</span><span class="o">.</span><span class="n">load_model_and_tokenizer</span><span class="p">()</span>

    <span class="c1"># Initialize conversation handler</span>
    <span class="n">generation_params</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get_generation_params</span><span class="p">()</span>
    <span class="n">conversation_handler</span> <span class="o">=</span> <span class="n">GemmaConversationHandler</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">generation_params</span><span class="p">)</span>

    <span class="k">return</span> <span class="kc">True</span>
</pre></div>
</div>
<p>This ensures all components are properly loaded before the application starts accepting requests.</p>
<p><strong>Flask Routes</strong></p>
<p>The application provides several API endpoints:</p>
<ol class="arabic simple">
<li><p><strong>``/``</strong>: The main route that serves the HTML interface</p></li>
<li><p><strong>``/api/start``</strong>: Initializes a new interview session</p></li>
<li><p><strong>``/api/chat``</strong>: Processes user messages and returns AI responses</p></li>
<li><p><strong>``/api/results``</strong>: Generates the final personality profile when the interview is complete</p></li>
<li><p><strong>``/clear_session``</strong>: Resets the current session</p></li>
<li><p><strong>``/model_info``</strong>: Returns information about the loaded model</p></li>
</ol>
<p><strong>Session Management</strong></p>
<p>Flask sessions (via Flask-Session) are used to maintain state between requests. Each user’s browser gets a unique session, which contains the <code class="docutils literal notranslate"><span class="pre">session_state</span></code> dictionary tracking their interview progress.</p>
<p><strong>Results Generation</strong></p>
<p>When all questions are answered, the <code class="docutils literal notranslate"><span class="pre">/api/results</span></code> endpoint uses the <code class="docutils literal notranslate"><span class="pre">BFIScorer</span></code> to calculate personality trait scores:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@app</span><span class="o">.</span><span class="n">route</span><span class="p">(</span><span class="s1">&#39;/api/results&#39;</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;GET&#39;</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_results</span><span class="p">():</span>
    <span class="c1"># Get the completed session state</span>
    <span class="n">session_state</span> <span class="o">=</span> <span class="n">session</span><span class="p">[</span><span class="s1">&#39;session_state&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;state&#39;</span><span class="p">)</span> <span class="o">!=</span> <span class="n">session_manager</span><span class="o">.</span><span class="n">STATES</span><span class="p">[</span><span class="s1">&#39;COMPLETED&#39;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">jsonify</span><span class="p">({</span><span class="s1">&#39;error&#39;</span><span class="p">:</span> <span class="s1">&#39;Interview not completed.&#39;</span><span class="p">}),</span> <span class="mi">400</span>

    <span class="c1"># Extract the answers</span>
    <span class="n">answers</span> <span class="o">=</span> <span class="n">session_state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;answered_questions&#39;</span><span class="p">,</span> <span class="p">{})</span>

    <span class="c1"># Generate a comprehensive report</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">bfi_scorer</span><span class="o">.</span><span class="n">generate_comprehensive_report</span><span class="p">(</span><span class="n">answers</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">jsonify</span><span class="p">({</span><span class="s1">&#39;success&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;report&#39;</span><span class="p">:</span> <span class="n">report</span><span class="p">})</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">BFIScorer</span></code> performs several key functions:</p>
<ol class="arabic simple">
<li><p><strong>Scoring individual traits</strong> based on question answers</p></li>
<li><p><strong>Reversing scores</strong> for reverse-coded items</p></li>
<li><p><strong>Calculating trait levels</strong> (e.g., “High”, “Moderate”, “Low”)</p></li>
<li><p><strong>Generating interpretations</strong> for each trait level</p></li>
</ol>
<p>The final report includes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">report</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;scores&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;traits&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;openness&quot;</span><span class="p">:</span> <span class="p">{</span> <span class="o">/*</span> <span class="n">scores</span> <span class="k">for</span> <span class="n">openness</span> <span class="o">*/</span> <span class="p">},</span>
            <span class="s2">&quot;conscientiousness&quot;</span><span class="p">:</span> <span class="p">{</span> <span class="o">/*</span> <span class="n">scores</span> <span class="k">for</span> <span class="n">conscientiousness</span> <span class="o">*/</span> <span class="p">},</span>
            <span class="s2">&quot;extraversion&quot;</span><span class="p">:</span> <span class="p">{</span> <span class="o">/*</span> <span class="n">scores</span> <span class="k">for</span> <span class="n">extraversion</span> <span class="o">*/</span> <span class="p">},</span>
            <span class="s2">&quot;agreeableness&quot;</span><span class="p">:</span> <span class="p">{</span> <span class="o">/*</span> <span class="n">scores</span> <span class="k">for</span> <span class="n">agreeableness</span> <span class="o">*/</span> <span class="p">},</span>
            <span class="s2">&quot;neuroticism&quot;</span><span class="p">:</span> <span class="p">{</span> <span class="o">/*</span> <span class="n">scores</span> <span class="k">for</span> <span class="n">neuroticism</span> <span class="o">*/</span> <span class="p">}</span>
        <span class="p">},</span>
        <span class="s2">&quot;summary&quot;</span><span class="p">:</span> <span class="p">{</span> <span class="o">/*</span> <span class="n">overall</span> <span class="n">completion</span> <span class="n">stats</span> <span class="o">*/</span> <span class="p">}</span>
    <span class="p">},</span>
    <span class="s2">&quot;interpretations&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;openness&quot;</span><span class="p">:</span> <span class="p">{</span> <span class="o">/*</span> <span class="n">interpretation</span> <span class="n">of</span> <span class="n">openness</span> <span class="n">score</span> <span class="o">*/</span> <span class="p">},</span>
        <span class="s2">&quot;conscientiousness&quot;</span><span class="p">:</span> <span class="p">{</span> <span class="o">/*</span> <span class="n">interpretation</span> <span class="n">of</span> <span class="n">conscientiousness</span> <span class="n">score</span> <span class="o">*/</span> <span class="p">},</span>
        <span class="o">/*</span> <span class="n">etc</span><span class="o">.</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">traits</span> <span class="o">*/</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This report is sent to the frontend, which displays the results in a visual format.</p>
</section>
<section id="psychological-design-considerations">
<h2>Psychological Design Considerations<a class="headerlink" href="#psychological-design-considerations" title="Link to this heading">¶</a></h2>
<p>Beyond the technical workings, it’s important to understand how this app implements psychological survey principles from earlier lectures:</p>
<p><strong>Using a Standardized Instrument</strong>: The Big Five Inventory (BFI) is a well-validated measure of personality. BFITraitTalk_AI adheres to this by asking the exact BFI statements (e.g., “I see myself as someone who is talkative”) in order. Even though Kaya wraps the question in a conversational prompt, the core content is unchanged. This means we can compare the results from this conversational delivery to the known benchmarks of the BFI.</p>
<p><strong>Adaptive Clarification</strong>: In a traditional survey, if a respondent doesn’t understand a question, they might answer incorrectly or not at all, and the researcher might never know. Here, if the user asks “What does ‘tends to find fault with others’ mean?”, Kaya will pause and explain it in plain language before proceeding. This is a big advantage – it enhances respondent understanding, which is likely to improve the quality of the data.</p>
<p><strong>Free-Response Before Forced-Choice</strong>: One notable design choice is letting the user respond in free text before committing to a number. This can provide richer context. From a psychological standpoint, this might reduce priming or framing effects that fixed options can impose. The user articulates their true thoughts or examples (“I’m talkative with friends but shy at work”), which arguably gives a more ecologically valid picture of their personality in context. Only after expressing themselves does the system boil it down to a number.</p>
<p><strong>Confirmation for Accuracy</strong>: Having the user confirm the interpreted score is essentially a check on measurement accuracy. By asking “Does that feel right?”, the system gives control back to the participant to correct any misinterpretation. This is analogous to an interviewer paraphrasing a respondent’s answer and asking if they got it right – a technique sometimes used in qualitative interviewing to validate understanding. It helps ensure the data (the scores recorded) actually reflect what the participant meant.</p>
<p><strong>Reduction of Straightlining and Inattentiveness</strong>: In online surveys, respondents sometimes rush through using the same answer for everything or not truly considering each item. The conversational format might mitigate this by making each question feel more engaging (it’s harder to ignore a question when it’s asked by an interlocutor and when you have to justify your answer in words first). The presence of an interactive agent can encourage respondents to stay present and think about each answer.</p>
<p><strong>Social Presence and Honesty</strong>: Some research suggests people might be more honest to an AI interviewer than a human, especially on sensitive questions, because they don’t feel judged. Personality items aren’t extremely sensitive, but they do probe potentially unflattering traits (e.g., “tends to find fault with others” or “is lazy”). Having a non-judgmental AI that even apologizes for misunderstanding might make participants more comfortable admitting, say, “Yes, I can be lazy sometimes,” which they might soften if just ticking a box on paper due to self-image concerns.</p>
<p><strong>Contextualizing Personality</strong>: One of the goals mentioned was measuring personality “in context”. Traditional BFI scores provide a general measure but lose nuance (you don’t know why someone chose 3 vs 4). With this approach, the context comes out in the conversation. For instance, the transcript may reveal “User finds they are talkative in familiar settings but not in public” – a nuance of Extraversion that a single score can’t capture. While ultimately we still record a single number per item, the conversation can be recorded and later analyzed qualitatively.</p>
<p><strong>Maintaining Validity</strong>: There is a trade-off though. By deviating from the standard questionnaire procedure, are we affecting the instrument’s psychometric properties? For example, the act of explaining one’s answer might cause reflective equilibrium – the person might change their mind as they talk, or feel compelled to be consistent in later answers because they’ve set a narrative. These are things to consider:</p>
<ul class="simple">
<li><p>The app tries to ensure each question is still answered independently and clearly. It doesn’t show previous answers (aside from any allusions the user themselves might make) and the AI treats each question afresh.</p></li>
<li><p>The confirmation process might actually improve reliability: The user double-checks their answer, possibly catching inconsistency or error.</p></li>
<li><p>But the interpersonal aspect (even with an AI) could introduce an interviewer effect – e.g., perhaps people might give more moderate answers to not appear extreme, even though Kaya is not human. The AI’s presence could subconsciously invoke social desirability bias (the user might phrase answers more positively because they feel like someone is listening).</p></li>
</ul>
</section>
<section id="ethical-and-methodological-considerations">
<h2>Ethical and Methodological Considerations<a class="headerlink" href="#ethical-and-methodological-considerations" title="Link to this heading">¶</a></h2>
<p>Whenever we bring AI into data collection with human participants, we must examine ethical and methodological issues:</p>
<p><strong>Data Privacy</strong>: A major advantage of the BFITraitTalk_AI setup is that it runs locally. As noted, no survey responses or personal information are sent to a cloud service. This addresses privacy concerns because personality data can be sensitive. If this were deployed in a research study, participants could be assured that their raw answers and conversation stay on the device or within the researcher’s server, not on Big Tech’s servers.</p>
<p><strong>Informed Consent and Participant Understanding</strong>: If this method were used in a study, participants would need to know they’re interacting with an AI, not a human (to avoid deception unless justified). They should consent to the conversation being recorded for research. One ethical upside is that participants might enjoy the interactive format more than a standard form, but they should also be told that the AI might not be perfect (to not overly trust any feedback it gives).</p>
<p><strong>Bias and Fairness</strong>: We must consider if the LLM could introduce bias. The BFI items themselves are neutral, but how the AI elaborates or interprets could be influenced by biases in training data. For example, if a user says something culturally specific, will Kaya misinterpret it due to not understanding that context? Or could Kaya inadvertently respond differently to users based on dialect or language fluency? We have to test the AI on diverse inputs.</p>
<p><strong>Validity and Reliability</strong>: From a methodological perspective, we should validate that this conversational method yields similar results to the traditional survey. Does a person get roughly the same Big Five scores through BFITraitTalk_AI as they would on a classic pen-and-paper or online form? If not, is the difference due to improved accuracy or due to bias introduced by the method? These questions need empirical testing.</p>
<p><strong>Participant Well-being</strong>: One must ensure the AI remains a beneficent presence. Personality surveys are generally low-risk, but if a participant becomes uncomfortable or starts divulging very personal information (outside the survey scope), the AI should handle it carefully. The current design doesn’t deeply address off-topic sensitive disclosures (e.g., if in the middle of a question about talkativeness the user starts talking about feeling depressed). Kaya might not be equipped to give emotional support beyond polite redirection.</p>
<p><strong>Transparency of AI Decisions</strong>: Another ethical aspect is being transparent about how the AI is scoring responses. In this tutorial scenario, the AI explains why it suggests a certain score (by paraphrasing what the user said). This is good practice – it provides some rationale and invites correction. If the AI just said “Recorded your answer as 4” with no explanation, the user might not know if it understood them. By hearing the AI’s summary, the user can gauge if Kaya got it right.</p>
<p><strong>Handling “Out-of-scope” Situations</strong>: Because the AI is a free-form model, users might test its limits. For example, a user might joke or flirt with Kaya, or try to get it to deviate (“Do you think that’s a good trait to have?”). The initial prompt instructs Kaya to remain on task and maybe politely deflect such queries. This is an important consideration: keeping the AI within its domain (survey interviewing) and not letting it give advice or engage in therapy or other roles.</p>
<p><strong>Future Data Use</strong>: If this were used for research, another ethical point is what happens with the conversation logs. They contain personal reflections. Researchers must treat them as qualitative data with confidentiality. Possibly, identifiable info could emerge in what people say (someone might mention “my job at the bank” in an answer, revealing something). Proper data handling (anonymization if analyzing transcripts, secure storage) is essential.</p>
</section>
<section id="customization-and-extension-ideas">
<h2>Customization and Extension Ideas<a class="headerlink" href="#customization-and-extension-ideas" title="Link to this heading">¶</a></h2>
<p>Finally, let’s consider how students or researchers could extend BFITraitTalk_AI for other purposes or improve it further. This app is a prototype, and its framework can be adapted in many ways:</p>
<p><strong>Using a Different Questionnaire</strong>: You could swap out the BFI with any other survey or set of interview questions. For example, imagine using an organizational culture survey or a clinical screening questionnaire in this format. To do this, you would replace the <code class="docutils literal notranslate"><span class="pre">data/bfi_items.json</span></code> with your own question set (ensuring a similar format). The session manager and logic can largely remain the same. You’d want to update the initial prompt instructions to reflect any differences and adjust the scoring interpretation logic or create a new scorer appropriate for the new instrument.</p>
<p><strong>Scaling Up the Model or Using an API</strong>: If one has access to better hardware or is comfortable with cloud services, one might try using the larger Gemma-3 27B model for even more fluent interactions, or even an API like OpenAI’s GPT-4 for comparison. The modular design of <code class="docutils literal notranslate"><span class="pre">GemmaModelManager</span></code> means you can point it to a different model checkpoint as long as it’s a causal language model. Keep in mind, using an online API would reintroduce privacy issues, so for sensitive data that might be a step backward.</p>
<p><strong>Enhancing the UI/UX</strong>: There are many possibilities:</p>
<ul class="simple">
<li><p><strong>Add voice interaction</strong>: Using text-to-speech for the AI’s questions and speech-to-text for the user’s answers can make it feel like a true interview.</p></li>
<li><p><strong>Add a progress bar or question counter</strong>: The BFI has 44 items; letting users see progress (“Question 10 of 44”) can be motivating and transparent.</p></li>
<li><p><strong>Implement skip logic or branching</strong>: While BFI is linear, other surveys might have skip patterns (e.g., if user answers yes to something, skip the next question).</p></li>
<li><p><strong>Multi-language support</strong>: If Gemma-3 is multilingual or you have models in other languages, you could translate the question set and adjust prompts so that non-English speakers can take the survey in their native language with the AI.</p></li>
<li><p><strong>Visual or multimedia context</strong>: For some types of questions, you might present an image or a video and ask the participant about it.</p></li>
</ul>
<p><strong>Collecting Richer Data (paradata)</strong>: The system could quietly log additional information like response times (how long the user took to respond to each question), which could be an interesting variable in research (e.g., hesitations might indicate uncertainty). It already logs the content of what user says, which is valuable qualitative data.</p>
<p><strong>AI Improvement</strong>: One could try fine-tuning the model on sample interview data to improve its performance. For example, feeding it examples of how to respond to various types of user answers could make Kaya more consistent. Right now, we rely on prompting alone. Fine-tuning or using a reinforcement learning approach could yield an AI that better adheres to the interview protocol out-of-the-box.</p>
<p><strong>Alternate Personas or Styles</strong>: We could experiment with the AI’s persona. The instructions make Kaya empathic and neutral. But what if we deliberately tried a more formal interviewer vs. a more casual friend tone, and see which yields better data or user satisfaction? The prompt could be adjusted to change style.</p>
<p><strong>Beyond Surveys – AI Interviewer for Clinical or Educational settings</strong>: The same framework can drive an AI therapist intake (asking a patient about symptoms systematically) or an AI job interview practice partner (asking common interview questions and giving feedback). The difference is mostly in content and what you do with the data.</p>
<p><strong>Integration with Databases or Research Platforms</strong>: For real studies, one would want to save the data (scores, and possibly transcripts) to a database or at least CSV. Currently, BFITraitTalk_AI likely just keeps data in session memory (and maybe prints results to console). Adding a feature to export results (with consent) would be practical.</p>
<p>In terms of pedagogy, modifying this app is a great exercise. Students can try changing one aspect and observe how it changes the interaction. For instance, “What if we remove the confirmation step? Does the AI sometimes mis-score the answers?” or “What if Kaya didn’t give any intro and just asked the question bluntly?”. Such experiments tie back to understanding both AI behavior and best practices in survey design.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">¶</a></h2>
<p>BFITraitTalk_AI shows how generative AI can be integrated into a survey in a way that complements survey methodology rather than replacing it. By walking through the code, we see a marriage of a deterministic survey structure with the probabilistic nature of AI language generation. The tutorial highlights how earlier conceptual discussions (designing clear surveys, using AI for interviews) materialize in code. As you work with or extend this system, keep asking: Does this preserve the integrity of the data? Does it improve the user experience? Balancing those two is key in any AI-augmented research tool.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<p>Abdurahman, S., Atari, M., Karimi-Malekabadi, F., Xue, M. J., Trager, J. P., Park, P. S., … &amp; Dehghani, M. (2024). Perils and opportunities in using large language models in psychological research. <a href="#id1"><span class="problematic" id="id2">*</span></a>PNAS Nexus, 3*(7), pgae245. <a class="reference external" href="https://doi.org/10.1093/pnasnexus/pgae245">https://doi.org/10.1093/pnasnexus/pgae245</a></p>
<p>Bender, E. M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? In <em>Proceedings of the 2021 ACM FAccT Conference</em>. <a class="reference external" href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a></p>
<p>Groves, R. M., Fowler, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., &amp; Tourangeau, R. (2009). <em>Survey Methodology</em> (2nd ed.). Hoboken, NJ: Wiley.</p>
<p>Jansen, B. J., Jung, S., &amp; Salminen, J. (2023). Employing large language models in survey research. <em>Natural Language Processing, 4</em>, 100020. <a class="reference external" href="https://doi.org/10.1016/j.nlp.2023.100020">https://doi.org/10.1016/j.nlp.2023.100020</a></p>
<p>Krosnick, J. A. (1999). Survey research. <em>Annual Review of Psychology, 50</em>, 537–567.</p>
<p>Wuttke, A., Aßenmacher, M., Klamm, C., Lang, M. M., Würschinger, Q., &amp; Kreuter, F. (2025). AI conversational interviewing: Transforming surveys with LLMs as adaptive interviewers. In <em>Proceedings of the LaTeCH-CLfL 2025 Conference</em>. <a class="reference external" href="https://doi.org/10.48550/arXiv.2410.01824">https://doi.org/10.48550/arXiv.2410.01824</a></p>
<p>Zou, Z., Mubin, O., Alnajjar, F., &amp; Ali, L. (2024). A pilot study of measuring emotional response and perception of LLM-generated and human-generated questionnaires. <em>Scientific Reports, 14</em>, 2781. <a class="reference external" href="https://doi.org/10.1038/s41598-024-53255-1">https://doi.org/10.1038/s41598-024-53255-1</a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_02_2_privacy_considerations.html" class="btn btn-neutral float-left" title="Lecture 02.2 – Privacy Considerations" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Reese.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>