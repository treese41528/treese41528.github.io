

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lecture 01.3 – Synthetic Respondents: Simulation and Supplementation &mdash; Generative AI for Computational Psychology</title>
      <link rel="stylesheet" type="text/css" href="/GenerativeAI_ComputationalPsychology/_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="/GenerativeAI_ComputationalPsychology/_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/GenerativeAI_ComputationalPsychology/lecture_01_3_synthetic_respondents.html" />
      <script src="/GenerativeAI_ComputationalPsychology/_static/jquery.js?v=5d32c60e"></script>
      <script src="/GenerativeAI_ComputationalPsychology/_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="/GenerativeAI_ComputationalPsychology/_static/documentation_options.js?v=f2a433a1"></script>
      <script src="/GenerativeAI_ComputationalPsychology/_static/doctools.js?v=9bcbadda"></script>
      <script src="/GenerativeAI_ComputationalPsychology/_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="/GenerativeAI_ComputationalPsychology/_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 02.1 – Interactive AI Surveys" href="lecture_02_1_interactive_ai_surveys.html" />
    <link rel="prev" title="Lecture 01.2 – Survey Design with Large Language Models" href="lecture_01_2_survey_design_with_llms.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Generative AI in Psychological Research
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="lecture_overview.html">Generative AI in Psychological Research: Course Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_overview.html#module-1-foundations-of-ai-in-psychology">Module 1: Foundations of AI in Psychology</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_overview.html#module-2-ai-driven-data-collection">Module 2: AI-Driven Data Collection</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Module 1: Foundations</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html">Lecture 01.1 – Introduction to Generative AI in Psychology</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#capabilities-of-modern-generative-ai">Capabilities of Modern Generative AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#the-evolution-from-traditional-to-ai-augmented-research">The Evolution from Traditional to AI-Augmented Research</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#the-spectrum-of-llm-applications-in-psychology">The Spectrum of LLM Applications in Psychology</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#key-ethical-and-methodological-considerations">Key Ethical and Methodological Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#case-study-the-emergence-of-silicon-samples">Case Study: The Emergence of “Silicon Samples”</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html">Lecture 01.2 – Survey Design with Large Language Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html#traditional-challenges-in-survey-design">Traditional Challenges in Survey Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html#how-llms-can-enhance-survey-design">How LLMs Can Enhance Survey Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html#case-study-ai-generated-vs-human-generated-questionnaires">Case Study: AI-Generated vs. Human-Generated Questionnaires</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html#practical-implementation-human-ai-collaboration-in-survey-design">Practical Implementation: Human-AI Collaboration in Survey Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html#optimizing-llm-prompts-for-survey-design">Optimizing LLM Prompts for Survey Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html#limitations-and-best-practices">Limitations and Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html#case-example-developing-a-likert-scale-measure-of-climate-anxiety">Case Example: Developing a Likert-Scale Measure of Climate Anxiety</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_2_survey_design_with_llms.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lecture 01.3 – Synthetic Respondents: Simulation and Supplementation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-promise-of-synthetic-respondents">The Promise of Synthetic Respondents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-scientific-reality-empirical-findings-on-synthetic-respondent-fidelity">The Scientific Reality: Empirical Findings on Synthetic Respondent Fidelity</a></li>
<li class="toctree-l2"><a class="reference internal" href="#proper-use-cases-and-limitations">Proper Use Cases and Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#case-study-depression-prediction-from-synthetic-clinical-interviews">Case Study: Depression Prediction from Synthetic Clinical Interviews</a></li>
<li class="toctree-l2"><a class="reference internal" href="#best-practices-for-working-with-synthetic-respondents">Best Practices for Working With Synthetic Respondents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Module 2: Data Collection</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html">Lecture 02.1 – Interactive AI Surveys</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#the-promise-of-conversational-ai-surveys">The Promise of Conversational AI Surveys</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#empirical-evidence-do-ai-interviews-work">Empirical Evidence: Do AI Interviews Work?</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#implementing-ai-conversational-surveys">Implementing AI Conversational Surveys</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#case-study-an-ai-driven-mental-health-assessment">Case Study: An AI-Driven Mental Health Assessment</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#ethical-considerations">Ethical Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#best-practices-for-ai-driven-surveys">Best Practices for AI-Driven Surveys</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#future-directions">Future Directions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_privacy_considerations.html">Lecture 02.2 – Privacy Considerations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#understanding-the-privacy-landscape">Understanding the Privacy Landscape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#closed-api-llms-vs-open-source-models-a-privacy-comparison">Closed API LLMs vs. Open-Source Models: A Privacy Comparison</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#practical-privacy-preservation-strategies">Practical Privacy Preservation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#hybrid-approaches-balancing-privacy-and-capability">Hybrid Approaches: Balancing Privacy and Capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#responsible-documentation-and-transparency">Responsible Documentation and Transparency</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#case-study-privacy-preserving-clinical-assessment">Case Study: Privacy-Preserving Clinical Assessment</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#future-directions-in-privacy-preserving-ai">Future Directions in Privacy-Preserving AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html">Lecture 02.4 - Conversational AI Survey (BFITraitTalk_AI Tutorial)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#setup">Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#installation-steps">Installation Steps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#codebase-walkthrough">Codebase Walkthrough</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#frontend-design-user-interface">Frontend Design (User Interface)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#survey-logic-and-adaptive-interview-flow">Survey Logic and Adaptive Interview Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#backend-structure-and-data-flow">Backend Structure and Data Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#psychological-design-considerations">Psychological Design Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#ethical-and-methodological-considerations">Ethical and Methodological Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#customization-and-extension-ideas">Customization and Extension Ideas</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#references">References</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Generative AI in Psychological Research</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Lecture 01.3 – Synthetic Respondents: Simulation and Supplementation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lecture_01_3_synthetic_respondents.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lecture-01-3-synthetic-respondents-simulation-and-supplementation">
<h1>Lecture 01.3 – Synthetic Respondents: Simulation and Supplementation<a class="headerlink" href="#lecture-01-3-synthetic-respondents-simulation-and-supplementation" title="Link to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Data collection is often the most resource-intensive phase of psychological research. Recruiting participants, especially from specialized populations, can be challenging, time-consuming, and expensive. What if researchers could generate synthetic data that mimics human responses? This possibility has emerged with the development of large language models (LLMs) that can simulate human-like survey responses (Argyle et al., 2023).</p>
<p>This lecture explores the concept of “synthetic respondents” – artificial entities created through LLMs that answer survey questions in place of human participants. We examine the potential of this approach to address data scarcity issues in psychological research, while critically evaluating its limitations and risks (Bisbee et al., 2024). The goal is to develop a nuanced understanding of when and how synthetic data can appropriately supplement traditional data collection methods.</p>
</section>
<section id="the-promise-of-synthetic-respondents">
<h2>The Promise of Synthetic Respondents<a class="headerlink" href="#the-promise-of-synthetic-respondents" title="Link to this heading"></a></h2>
<p>Synthetic respondents offer several potential advantages for psychological research:</p>
<p><strong>1. Rapid Large-Scale Data Generation</strong></p>
<p>Traditional data collection often takes weeks or months, with significant costs per participant. In contrast, an LLM can generate hundreds or thousands of simulated responses in minutes. This capability enables:</p>
<ul class="simple">
<li><p>Quick preliminary analyses to refine research questions</p></li>
<li><p>Exploration of hypothetical scenarios or conditions</p></li>
<li><p>Generation of practice datasets for methodological training</p></li>
<li><p>Augmentation of small human samples to increase statistical power</p></li>
</ul>
<p><strong>2. Access to Hard-to-Reach Populations</strong></p>
<p>Some populations are inherently difficult to recruit in sufficient numbers:</p>
<ul class="simple">
<li><p>Rare clinical conditions</p></li>
<li><p>Highly specialized professional groups</p></li>
<li><p>Geographically remote communities</p></li>
<li><p>Historically marginalized groups</p></li>
</ul>
<p>LLMs, having been trained on text from diverse sources, might simulate responses from these populations based on available information, potentially providing insight into groups that would otherwise be underrepresented in research (Shrestha et al., 2025).</p>
<p><strong>3. Privacy and Ethical Advantages</strong></p>
<p>Working with synthetic data can offer certain ethical benefits:</p>
<ul class="simple">
<li><p>No need to collect sensitive personal information from real individuals</p></li>
<li><p>Reduced burden on vulnerable populations</p></li>
<li><p>Lower risk of confidentiality breaches</p></li>
<li><p>Ability to share “data” that would otherwise be restricted by privacy concerns</p></li>
</ul>
<p><strong>4. Methodological Applications</strong></p>
<p>Synthetic respondents can serve specific methodological purposes:</p>
<ul class="simple">
<li><p>Pilot testing survey instruments before deployment</p></li>
<li><p>Exploring the robustness of analytical techniques</p></li>
<li><p>Testing hypotheses in silico before investing in full-scale studies</p></li>
<li><p>Simulating longitudinal data where real long-term follow-up is impractical</p></li>
</ul>
</section>
<section id="the-scientific-reality-empirical-findings-on-synthetic-respondent-fidelity">
<h2>The Scientific Reality: Empirical Findings on Synthetic Respondent Fidelity<a class="headerlink" href="#the-scientific-reality-empirical-findings-on-synthetic-respondent-fidelity" title="Link to this heading"></a></h2>
<p>How well do LLMs actually mimic human survey responses? Recent research provides mixed evidence:</p>
<p><strong>The “Silicon Samples” Studies: Promising Correlations</strong></p>
<p>Argyle et al. (2023) pioneered the concept of “silicon samples” – LLM-simulated respondents conditioned with specific demographic profiles. They found that when GPT-3 was prompted with rich backstories (e.g., age, gender, ethnicity, political ideology) drawn from real survey participants, the model’s aggregated responses matched the actual response distributions of those human subgroups with surprising accuracy.</p>
<p>The researchers termed this “algorithmic fidelity” – the ability of the AI to be steered in a fine-grained way to reflect specific demographic patterns rather than a generic “one-size” bias. For example, GPT-3 could be prompted to respond as a young liberal individual or an older conservative individual, and its answers to political attitude questions would shift in line with how those groups responded in real surveys.</p>
<p>These findings suggest that LLMs encode substantial information about sociocultural context and attitudes, allowing them to simulate responses that preserve many real-world statistical relationships when properly conditioned.</p>
<p><strong>The Cross-Cultural Challenge: Diminishing Fidelity</strong></p>
<p>While initial results with American samples were promising, subsequent research has shown that LLMs’ ability to mimic responses varies considerably across cultural contexts. Shrestha et al. (2025) tested whether GPT-4 could accurately simulate survey responses from participants in the United States, Saudi Arabia, and the United Arab Emirates.</p>
<p>Their findings indicated that while the model produced reasonably similar responses for American participants, the correlation between AI-generated and human responses was significantly weaker for Saudi and Emirati samples. The researchers concluded that LLMs, predominantly trained on Western data sources, struggle to accurately represent non-Western perspectives and cultural nuances.</p>
<p>This cultural gap has important implications for the use of synthetic respondents in cross-cultural or global research, suggesting that current models may inadvertently reinforce Western-centric biases rather than overcome them (Abdurahman et al., 2024). The phenomenon reflects broader concerns about the representativeness of training data in large language models (Bender et al., 2021).</p>
<p><strong>Technical Artifacts vs. True Understanding</strong></p>
<p>Perhaps most concerning are findings regarding the mechanisms underlying LLM responses to survey questions. Dominguez-Olmedo et al. (2024) discovered that many LLMs exhibit systematic biases when answering multiple-choice questions, such as a preference for options labeled “A” regardless of content.</p>
<p>When these technical artifacts were controlled for, the models often produced nearly random responses—suggesting that their apparent alignment with human answer patterns in some studies might be coincidental rather than reflecting genuine psychological understanding.</p>
<p>This raises fundamental questions about what LLMs are actually doing when they “take” a survey. Are they modeling human psychological processes, or simply reproducing statistical patterns in their training data without the underlying cognitive mechanisms? This concern aligns with critiques about treating LLMs as genuine sources of human-like beliefs or opinions (Bender et al., 2021).</p>
</section>
<section id="proper-use-cases-and-limitations">
<h2>Proper Use Cases and Limitations<a class="headerlink" href="#proper-use-cases-and-limitations" title="Link to this heading"></a></h2>
<p>Given these empirical findings, when and how should researchers consider using synthetic respondents?</p>
<p><strong>Appropriate Applications</strong></p>
<ol class="arabic simple">
<li><p><strong>Preliminary Exploration</strong>: Using synthetic data to generate initial hypotheses or refine research questions before investing in human data collection.</p></li>
<li><p><strong>Methodological Development</strong>: Testing new analytical techniques or survey instruments with simulated responses to identify potential issues.</p></li>
<li><p><strong>Augmenting Limited Samples</strong>: Supplementing small human datasets with synthetic responses to increase statistical power, particularly for rare populations (with appropriate validation).</p></li>
<li><p><strong>Privacy-Preserving Simulation</strong>: Generating synthetic data for teaching, demonstration, or sharing when privacy concerns would otherwise prevent the use of real participant data.</p></li>
</ol>
<p><strong>Critical Limitations</strong></p>
<ol class="arabic simple">
<li><p><strong>Not a Replacement for Human Data</strong>: Synthetic responses should supplement, not replace, real human data for primary analyses and conclusions.</p></li>
<li><p><strong>Cultural and Demographic Boundaries</strong>: LLMs show decreased fidelity when simulating responses from populations underrepresented in training data, limiting generalizability (Shrestha et al., 2025).</p></li>
<li><p><strong>Surface-Level Mimicry</strong>: LLMs may reproduce statistical patterns without capturing underlying psychological processes, potentially leading to misleading inferences about human cognition (Bisbee et al., 2024).</p></li>
<li><p><strong>Reproducibility Challenges</strong>: LLM outputs can vary based on prompting techniques, model versions, and implementation details, creating challenges for scientific reproducibility.</p></li>
</ol>
</section>
<section id="case-study-depression-prediction-from-synthetic-clinical-interviews">
<h2>Case Study: Depression Prediction from Synthetic Clinical Interviews<a class="headerlink" href="#case-study-depression-prediction-from-synthetic-clinical-interviews" title="Link to this heading"></a></h2>
<p>To illustrate a productive application of synthetic respondents, consider the work of Kang et al. (2025) in developing a depression severity prediction model.</p>
<p>The researchers faced a common challenge in clinical machine learning: limited access to real patient data due to privacy concerns and the scarcity of properly labeled examples across the full spectrum of depression severity. They particularly lacked examples of more severe cases, creating a skewed dataset.</p>
<p>Their solution was to use an LLM to generate synthetic clinical interview transcripts that represented different levels of depression. These synthetic interviews were:</p>
<ol class="arabic simple">
<li><p>Based on patterns learned from a smaller set of real de-identified interviews</p></li>
<li><p>Validated by clinical experts for realism and appropriate symptomatic presentation</p></li>
<li><p>Used to augment (not replace) the real data for training purposes</p></li>
</ol>
<p>The results were impressive: the model trained on the combined real and synthetic data outperformed the model trained on real data alone, particularly in correctly identifying severe cases. This suggests that carefully implemented synthetic data can address specific gaps in psychological datasets and improve model performance.</p>
<p>Crucially, the researchers were transparent about their methods, validated the synthetic data against expert judgment, and used it as a supplement rather than a replacement for real clinical data.</p>
</section>
<section id="best-practices-for-working-with-synthetic-respondents">
<h2>Best Practices for Working With Synthetic Respondents<a class="headerlink" href="#best-practices-for-working-with-synthetic-respondents" title="Link to this heading"></a></h2>
<p>For researchers considering the use of synthetic respondents, we recommend the following guidelines:</p>
<p><strong>1. Validation Against Real Human Data</strong></p>
<p>Whenever possible, benchmark synthetic responses against at least a subset of real human data to assess fidelity. Calculate agreement metrics between the distributions of human and AI-generated responses to quantify representativeness (Bisbee et al., 2024).</p>
<p><strong>2. Transparent Reporting</strong></p>
<p>When publishing research that uses synthetic respondents, clearly document:
* The specific LLM used (including version)
* Prompting methods employed
* Validation procedures
* Any limitations or biases observed</p>
<p><strong>3. Consider Fine-tuning for Specialized Domains</strong></p>
<p>For research involving specialized psychological constructs or populations, consider fine-tuning LLMs on domain-specific data before generating synthetic responses. Suh et al. (2024) demonstrated that fine-tuning GPT-style models on actual survey data significantly improved their ability to predict human response distributions.</p>
<p><strong>4. Human Oversight and Interpretation</strong></p>
<p>Always maintain human expertise in the loop when analyzing synthetic data. LLMs lack the experiential understanding and theoretical grounding that human researchers bring. As Abdurahman et al. (2024) warn, avoid “GPTology” – the uncritical acceptance of AI outputs as ground truth about human psychology.</p>
<p><strong>5. Privacy and Ethical Considerations</strong></p>
<p>While synthetic data can enhance privacy, it’s not automatically anonymous. Ensure that generated responses don’t inadvertently reproduce identifiable information from training data. Consider the ethical implications of simulating responses from marginalized groups, particularly if doing so without appropriate cultural context or consultation.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<p>Synthetic respondents represent a fascinating new frontier in psychological research methodology. They offer promising solutions to data scarcity challenges and open up new possibilities for hypothesis exploration. However, current LLMs have significant limitations in their ability to authentically represent human psychological processes, particularly across diverse cultural contexts.</p>
<p>As the technology evolves, the most productive approach is neither uncritical adoption nor categorical rejection, but rather careful integration guided by empirical validation and ethical consideration. By understanding both the capabilities and constraints of LLM-generated responses, researchers can harness this technology where appropriate while maintaining the methodological rigor essential to psychological science.</p>
<p>In the next lecture, we will explore how generative AI can transform not just the data we analyze, but the very way we collect it – through interactive, adaptive survey methods that blur the line between quantitative surveys and qualitative interviews.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Abdurahman, S., Atari, M., Karimi-Malekabadi, F., Xue, M. J., Trager, J. P., Park, P. S., … &amp; Dehghani, M.</strong> (2024). <em>Perils and opportunities in using large language models in psychological research</em>. PNAS Nexus, 3(7), pgae245. <a class="reference external" href="https://doi.org/10.1093/pnasnexus/pgae245">https://doi.org/10.1093/pnasnexus/pgae245</a></p></li>
<li><p><strong>Argyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R., Rytting, C., &amp; Wingate, D.</strong> (2023). <em>Out of one, many: Using language models to simulate human samples</em>. Political Analysis, 31(3), 337–351. <a class="reference external" href="https://doi.org/10.1017/pan.2023.2">https://doi.org/10.1017/pan.2023.2</a></p></li>
<li><p><strong>Bender, E. M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S.</strong> (2021). <em>On the dangers of stochastic parrots: Can language models be too big?</em> In Proceedings of the 2021 ACM FAccT Conference. <a class="reference external" href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a></p></li>
<li><p><strong>Bisbee, J., Clinton, J. D., Dorff, C., Kenkel, B., &amp; Larson, J. M.</strong> (2024). <em>Synthetic replacements for human survey data? The perils of large language models</em>. Political Analysis, 32(3), 401–416. <a class="reference external" href="https://doi.org/10.1017/pan.2024.5">https://doi.org/10.1017/pan.2024.5</a></p></li>
<li><p><strong>Dominguez-Olmedo, R., Hardt, M., &amp; Mendler-Dünner, C.</strong> (2024). <em>Questioning the survey responses of large language models</em>. In Proceedings of NeurIPS 2024.</p></li>
<li><p><strong>Kang, A., Appasani, N., Zaki, M., &amp; Neumann, K.</strong> (2025). <em>Synthetic data generation with LLM for improved depression prediction</em>. Nature Digital Medicine, 3(11), 156-168.</p></li>
<li><p><strong>Shrestha, P., Koaik, F., Schnider, R., &amp; Sayess, D.</strong> (2025). <em>Beyond WEIRD: Can synthetic survey participants substitute for humans in global policy research?</em> Behavioral Science &amp; Policy, 3(X), 1–20. <a class="reference external" href="https://doi.org/10.1177/23794607241311793">https://doi.org/10.1177/23794607241311793</a></p></li>
<li><p><strong>Suh, J., Kim, H., &amp; Park, S.</strong> (2024). <em>Language model fine-tuning on scaled survey data for predicting distributions of public opinions</em>. Proceedings of EMNLP 2024.</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_01_2_survey_design_with_llms.html" class="btn btn-neutral float-left" title="Lecture 01.2 – Survey Design with Large Language Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lecture_02_1_interactive_ai_surveys.html" class="btn btn-neutral float-right" title="Lecture 02.1 – Interactive AI Surveys" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>