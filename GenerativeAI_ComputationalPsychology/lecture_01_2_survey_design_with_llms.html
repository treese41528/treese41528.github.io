

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Lecture 01.2 – Survey Design with Large Language Models &mdash; Generative AI for Computational Psychology</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
    <link rel="canonical" href="https://treese41528.github.io/GenerativeAI_ComputationalPsychology/lecture_01_2_survey_design_with_llms.html" />
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=f2a433a1"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Lecture 01.3 – Synthetic Respondents: Simulation and Supplementation" href="lecture_01_3_synthetic_respondents.html" />
    <link rel="prev" title="Lecture 01.1 – Introduction to Generative AI in Psychology" href="lecture_01_1_intro_to_genai_psychology.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Generative AI in Psychological Research
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="lecture_overview.html">Generative AI in Psychological Research: Course Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_overview.html#module-1-foundations-of-ai-in-psychology">Module 1: Foundations of AI in Psychology</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_overview.html#module-2-ai-driven-data-collection">Module 2: AI-Driven Data Collection</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_overview.html#module-3-practical-applications-and-analysis">Module 3: Practical Applications and Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_overview.html#learning-path">Learning Path</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Module 1: Foundations</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html">Lecture 01.1 – Introduction to Generative AI in Psychology</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#capabilities-of-modern-generative-ai">Capabilities of Modern Generative AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#the-evolution-from-traditional-to-ai-augmented-research">The Evolution from Traditional to AI-Augmented Research</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#the-spectrum-of-llm-applications-in-psychology">The Spectrum of LLM Applications in Psychology</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#key-ethical-and-methodological-considerations">Key Ethical and Methodological Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#case-study-the-emergence-of-silicon-samples">Case Study: The Emergence of “Silicon Samples”</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_1_intro_to_genai_psychology.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Lecture 01.2 – Survey Design with Large Language Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#traditional-challenges-in-survey-design">Traditional Challenges in Survey Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-llms-can-enhance-survey-design">How LLMs Can Enhance Survey Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="#case-study-ai-generated-vs-human-generated-questionnaires">Case Study: AI-Generated vs. Human-Generated Questionnaires</a></li>
<li class="toctree-l2"><a class="reference internal" href="#practical-implementation-human-ai-collaboration-in-survey-design">Practical Implementation: Human-AI Collaboration in Survey Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizing-llm-prompts-for-survey-design">Optimizing LLM Prompts for Survey Design</a></li>
<li class="toctree-l2"><a class="reference internal" href="#limitations-and-best-practices">Limitations and Best Practices</a></li>
<li class="toctree-l2"><a class="reference internal" href="#case-example-developing-a-likert-scale-measure-of-climate-anxiety">Case Example: Developing a Likert-Scale Measure of Climate Anxiety</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_01_3_synthetic_respondents.html">Lecture 01.3 – Synthetic Respondents: Simulation and Supplementation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_synthetic_respondents.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_synthetic_respondents.html#the-promise-of-synthetic-respondents">The Promise of Synthetic Respondents</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_synthetic_respondents.html#the-scientific-reality-empirical-findings-on-synthetic-respondent-fidelity">The Scientific Reality: Empirical Findings on Synthetic Respondent Fidelity</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_synthetic_respondents.html#proper-use-cases-and-limitations">Proper Use Cases and Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_synthetic_respondents.html#case-study-depression-prediction-from-synthetic-clinical-interviews">Case Study: Depression Prediction from Synthetic Clinical Interviews</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_synthetic_respondents.html#best-practices-for-working-with-synthetic-respondents">Best Practices for Working With Synthetic Respondents</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_synthetic_respondents.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_01_3_synthetic_respondents.html#references">References</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Module 2: Data Collection</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html">Lecture 02.1 – Interactive AI Surveys</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#the-promise-of-conversational-ai-surveys">The Promise of Conversational AI Surveys</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#empirical-evidence-do-ai-interviews-work">Empirical Evidence: Do AI Interviews Work?</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#implementing-ai-conversational-surveys">Implementing AI Conversational Surveys</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#case-study-an-ai-driven-mental-health-assessment">Case Study: An AI-Driven Mental Health Assessment</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#ethical-considerations">Ethical Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#best-practices-for-ai-driven-surveys">Best Practices for AI-Driven Surveys</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#future-directions">Future Directions</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_1_interactive_ai_surveys.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_2_privacy_considerations.html">Lecture 02.2 – Privacy Considerations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#understanding-the-privacy-landscape">Understanding the Privacy Landscape</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#closed-api-llms-vs-open-source-models-a-privacy-comparison">Closed API LLMs vs. Open-Source Models: A Privacy Comparison</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#practical-privacy-preservation-strategies">Practical Privacy Preservation Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#hybrid-approaches-balancing-privacy-and-capability">Hybrid Approaches: Balancing Privacy and Capability</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#responsible-documentation-and-transparency">Responsible Documentation and Transparency</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#case-study-privacy-preserving-clinical-assessment">Case Study: Privacy-Preserving Clinical Assessment</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#future-directions-in-privacy-preserving-ai">Future Directions in Privacy-Preserving AI</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_2_privacy_considerations.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html">Lecture 02.4 - Conversational AI Survey (BFITraitTalk_AI Tutorial)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#setup">Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#installation-steps">Installation Steps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#codebase-walkthrough">Codebase Walkthrough</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#frontend-design-user-interface">Frontend Design (User Interface)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#survey-logic-and-adaptive-interview-flow">Survey Logic and Adaptive Interview Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#backend-structure-and-data-flow">Backend Structure and Data Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#psychological-design-considerations">Psychological Design Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#ethical-and-methodological-considerations">Ethical and Methodological Considerations</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#customization-and-extension-ideas">Customization and Extension Ideas</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_02_3_bfitraittalk_ai_tutorial.html#references">References</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Generative AI in Psychological Research</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Lecture 01.2 – Survey Design with Large Language Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lecture_01_2_survey_design_with_llms.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lecture-01-2-survey-design-with-large-language-models">
<h1>Lecture 01.2 – Survey Design with Large Language Models<a class="headerlink" href="#lecture-01-2-survey-design-with-large-language-models" title="Link to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Survey design is both an art and a science. Despite decades of methodological refinement, researchers continue to face challenges in creating surveys that are clear, unbiased, engaging, and psychometrically sound. The introduction of Large Language Models (LLMs) into this process represents a significant advancement in how we approach survey construction.</p>
<p>This lecture explores how LLMs can enhance the survey design process in psychological research, with particular attention to their role in creating more effective Likert-scale instruments. We examine the traditional challenges of survey design, the specific capabilities LLMs bring to address these challenges, and best practices for human-AI collaboration in developing high-quality psychological measures.</p>
</section>
<section id="traditional-challenges-in-survey-design">
<h2>Traditional Challenges in Survey Design<a class="headerlink" href="#traditional-challenges-in-survey-design" title="Link to this heading"></a></h2>
<p>Before exploring AI solutions, it’s important to understand the persistent challenges in survey design:</p>
<ol class="arabic simple">
<li><p><strong>Question Clarity and Ambiguity</strong>: Even experienced researchers sometimes create questions that participants interpret differently than intended. Double-barreled questions, vague language, and ambiguous references can lead to measurement error (Groves et al., 2009).</p></li>
<li><p><strong>Response Option Construction</strong>: Creating balanced, comprehensive response scales (particularly for Likert items) requires careful consideration of the number of points, labeling, and anchoring to ensure they capture the full range of opinions (Likert, 1932; DeVellis, 2017).</p></li>
<li><p><strong>Leading Questions and Bias</strong>: Survey designers may unconsciously introduce their own biases through question framing that subtly pushes respondents toward certain answers (Krosnick, 1999).</p></li>
<li><p><strong>Cultural and Linguistic Inclusivity</strong>: Questions that work well for one population may be confusing or culturally inappropriate for others, limiting generalizability.</p></li>
<li><p><strong>Respondent Engagement</strong>: Long, repetitive, or tedious surveys lead to fatigue, satisficing, and poor data quality (Krosnick, 1999).</p></li>
</ol>
<p>These challenges have traditionally been addressed through expert review, cognitive interviews, and pilot testing—effective but time-consuming and resource-intensive processes.</p>
</section>
<section id="how-llms-can-enhance-survey-design">
<h2>How LLMs Can Enhance Survey Design<a class="headerlink" href="#how-llms-can-enhance-survey-design" title="Link to this heading"></a></h2>
<p>LLMs offer unique capabilities that address many traditional survey design challenges:</p>
<p><strong>1. Question Generation and Refinement</strong></p>
<p>LLMs excel at generating multiple phrasings for the same conceptual question. When prompted, these models can:</p>
<ul class="simple">
<li><p>Produce draft survey items on a specified topic</p></li>
<li><p>Suggest alternative wordings for existing questions</p></li>
<li><p>Rephrase complex items to improve readability</p></li>
<li><p>Identify and reformulate double-barreled questions</p></li>
</ul>
<p>For example, a researcher could prompt an LLM: “Generate five different ways to ask about perceived social support in a Likert-scale format.” The model might produce variants ranging from direct (“I have people I can rely on when I need help”) to more nuanced formulations (“When facing difficulties, support from others is readily available to me”).</p>
<p>This divergent thinking capacity helps researchers explore a broader range of possible items than they might generate alone, potentially identifying more precise or culturally appropriate phrasings.</p>
<p><strong>2. Response Scale Optimization</strong></p>
<p>LLMs can assist in developing appropriate response scales by:</p>
<ul class="simple">
<li><p>Suggesting balanced response options with clear gradations</p></li>
<li><p>Providing anchoring descriptions for scale points</p></li>
<li><p>Analyzing the linguistic distance between proposed scale points</p></li>
<li><p>Customizing scales for different question types (agreement, frequency, etc.)</p></li>
</ul>
<p>A 5-point Likert scale, for instance, might benefit from LLM input on whether the options are perceived as equally spaced psychologically. The model can suggest alternative wording if “Somewhat Agree” and “Agree” don’t seem sufficiently distinct to typical respondents.</p>
<p><strong>3. Bias Detection and Neutrality Enhancement</strong></p>
<p>One of the most valuable applications of LLMs in survey design is identifying potentially biased language:</p>
<ul class="simple">
<li><p>Flagging leading questions or emotionally charged wording</p></li>
<li><p>Suggesting more neutral alternatives</p></li>
<li><p>Identifying questions that make assumptions about respondents</p></li>
<li><p>Detecting subtle linguistic biases that human reviewers might miss</p></li>
</ul>
<p>A researcher might submit a draft survey to an LLM with the prompt: “Identify any questions that contain bias, assumptions, or leading language, and suggest neutral alternatives.” The AI might flag a question like “How beneficial was the therapy?” as subtly leading (presupposing some benefit) and suggest “How would you rate the effect of the therapy?” as a more neutral alternative.</p>
<p><strong>4. Cultural and Linguistic Adaptation</strong></p>
<p>LLMs can help make surveys more inclusive by:</p>
<ul class="simple">
<li><p>Suggesting culturally appropriate phrasings for different populations</p></li>
<li><p>Identifying idioms or references that may not translate well</p></li>
<li><p>Adapting questions for different educational levels or backgrounds</p></li>
<li><p>Providing alternatives to Western-centric concepts</p></li>
</ul>
<p>This capability is particularly valuable for cross-cultural research, where subtle linguistic and cultural differences can significantly impact measurement (Shrestha et al., 2025).</p>
<p><strong>5. Engagement Enhancement</strong></p>
<p>To combat respondent fatigue, LLMs can help:</p>
<ul class="simple">
<li><p>Vary question wording to reduce repetitiveness</p></li>
<li><p>Suggest more conversational or engaging phrasing</p></li>
<li><p>Identify potential areas of tedium in long surveys</p></li>
<li><p>Create natural transitions between topics</p></li>
</ul>
<p>Early evidence suggests that LLM-drafted questions may sometimes be perceived as more engaging by participants. A 2024 pilot study found that respondents reported higher positive affect (increased happiness, decreased sadness) when completing a ChatGPT-generated questionnaire compared to a traditional human-written one (Zou et al., 2024).</p>
</section>
<section id="case-study-ai-generated-vs-human-generated-questionnaires">
<h2>Case Study: AI-Generated vs. Human-Generated Questionnaires<a class="headerlink" href="#case-study-ai-generated-vs-human-generated-questionnaires" title="Link to this heading"></a></h2>
<p>A recent pilot study (Zou et al., 2024) illustrates the potential of LLM-assisted survey design. Researchers compared participant responses to two versions of a questionnaire: one crafted by human experts and another generated by ChatGPT on the same topic. The content covered similar material, but the phrasing and structure differed due to their different origins.</p>
<p>The results were intriguing:</p>
<ul class="simple">
<li><p>Participants reported higher positive affect when responding to the AI-generated questionnaire</p></li>
<li><p>Self-reported happiness increased and sadness decreased during the process</p></li>
<li><p>Participants gave favorable evaluations to the ChatGPT-designed survey, indicating that the questions were engaging and understandable</p></li>
</ul>
<p>This finding is particularly notable given longstanding concerns about participant engagement in surveys (Krosnick, 1999).</p>
<p>While this was a small-scale pilot, it suggests that LLMs, when guided properly, can create survey questions that are not only semantically sound but potentially more engaging to respondents—perhaps due to variety in wording or a conversational tone that differs from the sometimes stilted language of traditional surveys.</p>
</section>
<section id="practical-implementation-human-ai-collaboration-in-survey-design">
<h2>Practical Implementation: Human-AI Collaboration in Survey Design<a class="headerlink" href="#practical-implementation-human-ai-collaboration-in-survey-design" title="Link to this heading"></a></h2>
<p>The most effective approach to LLM-assisted survey design is collaborative rather than fully automated. Here’s a recommended workflow:</p>
<ol class="arabic simple">
<li><p><strong>Initial Item Brainstorming</strong>:
- Provide the LLM with context about the construct you wish to measure
- Request multiple candidate items and response formats
- Review the generated items for relevance and face validity</p></li>
<li><p><strong>Refinement and Critique</strong>:
- Ask the LLM to evaluate its own suggestions and identify potential issues
- Prompt for targeted improvements (e.g., “Make these questions more accessible to adolescents”)
- Request alternatives for any problematic items</p></li>
<li><p><strong>Expert Review</strong>:
- Human experts review LLM-generated content for theoretical alignment and construct validity
- Refine selected items based on domain knowledge
- Identify any overlooked aspects of the construct</p></li>
<li><p><strong>AI-Assisted Testing</strong>:
- Use the LLM to simulate potential respondent interpretations
- Identify items that might be misunderstood by specific populations
- Flag issues that might emerge in different cultural contexts</p></li>
<li><p><strong>Pilot Testing</strong>:
- Collect data from a small human sample to validate the LLM-assisted items
- Compare psychometric properties with traditional measures
- Incorporate feedback for final refinement</p></li>
</ol>
<p>This iterative process leverages both AI capabilities and human expertise, resulting in surveys that benefit from the strengths of each. The LLM serves as a creative partner and critic, while human judgment ensures theoretical coherence and scientific rigor (Abdurahman et al., 2024).</p>
</section>
<section id="optimizing-llm-prompts-for-survey-design">
<h2>Optimizing LLM Prompts for Survey Design<a class="headerlink" href="#optimizing-llm-prompts-for-survey-design" title="Link to this heading"></a></h2>
<p>The quality of LLM outputs depends significantly on the prompting approach. Here are strategies for effective prompting in survey design contexts:</p>
<p><strong>1. Provide Clear Context</strong></p>
<p>Include relevant background information, target population characteristics, and measurement goals:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&quot;I&#39;m designing a survey to measure math anxiety in high school students (ages 14-18).
The target population includes students from diverse socioeconomic backgrounds.
The goal is to assess anxiety specifically related to test-taking in mathematics.&quot;
</pre></div>
</div>
<p><strong>2. Request Multiple Alternatives</strong></p>
<p>Encourage the LLM to generate diverse options to expand your consideration set:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&quot;Generate 7-10 different Likert-scale items that could measure the construct of
&#39;perceived autonomy at work&#39;. Provide variety in wording approaches and perspectives.&quot;
</pre></div>
</div>
<p><strong>3. Specify Format Requirements</strong></p>
<p>Be explicit about the desired question structure and response options:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&quot;Create 5 questions to measure social anxiety using a 5-point Likert scale.
For each question, provide the item text and the specific anchors for the
five points on the scale (e.g., 1=&#39;Not at all characteristic of me&#39;,
5=&#39;Extremely characteristic of me&#39;).&quot;
</pre></div>
</div>
<p><strong>4. Request Self-Critique</strong></p>
<p>Ask the LLM to evaluate its own outputs for quality and potential issues:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&quot;For each of the survey questions you generated, identify any potential
problems with clarity, bias, double-barreled structure, or other issues
that might affect measurement quality.&quot;
</pre></div>
</div>
<p><strong>5. Indicate Target Reading Level</strong></p>
<p>Ensure accessibility for your intended audience:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&quot;Rephrase these questions to be appropriate for a 6th-grade reading level
while preserving the core meaning of each item.&quot;
</pre></div>
</div>
<p><strong>6. Iterative Refinement</strong></p>
<p>Use follow-up prompts to progressively improve items:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&quot;Take the first three items you generated and provide variants that are more
behaviorally focused rather than asking about general attitudes.&quot;
</pre></div>
</div>
<p>By crafting prompts that provide appropriate context and constraints, researchers can guide LLMs to produce high-quality survey content tailored to their specific research needs.</p>
</section>
<section id="limitations-and-best-practices">
<h2>Limitations and Best Practices<a class="headerlink" href="#limitations-and-best-practices" title="Link to this heading"></a></h2>
<p>While LLMs offer significant advantages for survey design, several limitations and considerations warrant attention:</p>
<p><strong>1. Lack of Empirical Grounding</strong></p>
<p>LLMs don’t inherently understand psychometric properties—they generate content based on patterns in their training data, not based on empirical validation (Bisbee et al., 2024):</p>
<ul class="simple">
<li><p>LLM-generated items should always undergo standard psychometric evaluation</p></li>
<li><p>Models may produce plausible-sounding but psychologically invalid items</p></li>
<li><p>Human expertise remains essential for theoretical alignment</p></li>
</ul>
<p><strong>2. Domain-Specific Knowledge Gaps</strong></p>
<p>Despite their broad knowledge, LLMs may lack specialized understanding of particular psychological constructs:</p>
<ul class="simple">
<li><p>Provide models with relevant theoretical frameworks when generating items for specialized domains</p></li>
<li><p>Verify that generated items reflect current understanding in the field</p></li>
<li><p>Be particularly cautious with newly emerging or highly specialized constructs</p></li>
</ul>
<p><strong>3. Algorithmic Biases</strong></p>
<p>LLMs may reproduce or amplify biases present in their training data (Abdurahman et al., 2024):</p>
<ul class="simple">
<li><p>Be alert to potential cultural, gender, or other biases in generated items</p></li>
<li><p>Explicitly request evaluation for bias in prompts</p></li>
<li><p>Use diverse human reviewers to identify bias that the LLM might miss</p></li>
</ul>
<p><strong>4. Technical Limitations</strong></p>
<p>Practical constraints affect how LLMs can be used in survey design:</p>
<ul class="simple">
<li><p>Token limitations may restrict the depth of context provided</p></li>
<li><p>Outputs can vary between models and even between sessions with the same model</p></li>
<li><p>Privacy considerations may limit use of proprietary models for sensitive content</p></li>
</ul>
<p>Based on these limitations, we recommend the following best practices:</p>
<ol class="arabic simple">
<li><p>Use LLMs as a supplement to, not a replacement for, human expertise</p></li>
<li><p>Always validate LLM-generated items with traditional methods</p></li>
<li><p>Document the use of AI assistance in research publications</p></li>
<li><p>Maintain a human-in-the-loop approach for all critical decisions</p></li>
<li><p>Use multiple strategies to mitigate potential biases</p></li>
</ol>
</section>
<section id="case-example-developing-a-likert-scale-measure-of-climate-anxiety">
<h2>Case Example: Developing a Likert-Scale Measure of Climate Anxiety<a class="headerlink" href="#case-example-developing-a-likert-scale-measure-of-climate-anxiety" title="Link to this heading"></a></h2>
<p>Let’s examine how the collaborative process might work in practice. Imagine a researcher developing a new scale to measure climate anxiety in young adults:</p>
<p><strong>Step 1: Initial Prompt to the LLM</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&quot;I&#39;m developing a Likert-scale measure of climate anxiety for young adults
(ages 18-25). Please generate 10 potential items that cover different aspects
of anxiety specifically related to climate change and environmental threats.
Use a 5-point scale from &#39;Strongly Disagree&#39; to &#39;Strongly Agree&#39;.&quot;
</pre></div>
</div>
<p><strong>Step 2: LLM Response</strong></p>
<p>The LLM might generate items such as:</p>
<ol class="arabic simple">
<li><p>“I worry about how climate change will affect my future quality of life.”</p></li>
<li><p>“News about environmental disasters makes me feel anxious.”</p></li>
<li><p>“I find it difficult to focus on daily tasks when thinking about climate threats.”</p></li>
<li><p>“I experience physical symptoms of anxiety (e.g., racing heart, trouble sleeping) when considering the climate crisis.”</p></li>
<li><p>“I feel helpless about humanity’s ability to address climate change.”</p></li>
</ol>
<p>…</p>
<p><strong>Step 3: Researcher Evaluation and Follow-up Prompt</strong></p>
<p>The researcher reviews the items, identifies strengths and weaknesses, and follows up:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&quot;These items are a good start. Could you add more items specifically about
behavioral impacts of climate anxiety? Also, can you suggest some
reverse-coded items to balance the scale and check for response bias?&quot;
</pre></div>
</div>
<p><strong>Step 4: Refinement and Expert Review</strong></p>
<p>After several iterations, the researcher selects a pool of items that appear to have good face validity. These are then reviewed by subject matter experts in climate psychology and psychometrics, who suggest further refinements.</p>
<p><strong>Step 5: Pilot Testing</strong></p>
<p>The refined scale is administered to a small sample, and item analysis is conducted to examine internal consistency, item-total correlations, and factor structure. Based on these results, items are further refined or eliminated.</p>
<p><strong>Step 6: Validation</strong></p>
<p>The final scale is validated against established measures and tested with the target population to establish psychometric properties.</p>
<p>Throughout this process, the LLM serves as a creative assistant and sounding board, but human judgment guides the core scientific decisions.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<p>Large Language Models offer powerful capabilities that can enhance psychological survey design. By generating diverse item phrasings, identifying potential biases, adapting language for different populations, and creating more engaging formulations, these tools address many traditional challenges in survey construction.</p>
<p>The most effective approach to incorporating LLMs in survey design is collaborative—using AI to expand the scope of what researchers can consider and refine, while maintaining human oversight for theoretical coherence and scientific rigor. This human-AI partnership offers the potential to create survey instruments that are not only psychometrically sound but also more accessible, engaging, and inclusive.</p>
<p>As we continue to explore the applications of generative AI in psychological research, it’s important to remember that these tools augment rather than replace expert judgment. The goal is not automation but enhancement—leveraging AI capabilities to extend human creativity and precision in the pursuit of better measurement.</p>
<p>In the next lecture, we will examine another frontier in AI-augmented research: the use of LLMs to generate synthetic respondent data, including both the promising applications and important limitations of this approach.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Abdurahman, S., Atari, M., Karimi-Malekabadi, F., Xue, M. J., Trager, J. P., Park, P. S., … &amp; Dehghani, M.</strong> (2024). <em>Perils and opportunities in using large language models in psychological research</em>. PNAS Nexus, 3(7), pgae245. <a class="reference external" href="https://doi.org/10.1093/pnasnexus/pgae245">https://doi.org/10.1093/pnasnexus/pgae245</a></p></li>
<li><p><strong>Bender, E. M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S.</strong> (2021). <em>On the dangers of stochastic parrots: Can language models be too big?</em> In Proceedings of the 2021 ACM FAccT Conference. <a class="reference external" href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a></p></li>
<li><p><strong>Bisbee, J., Clinton, J. D., Dorff, C., Kenkel, B., &amp; Larson, J. M.</strong> (2024). <em>Synthetic replacements for human survey data? The perils of large language models</em>. Political Analysis, 32(3), 401–416. <a class="reference external" href="https://doi.org/10.1017/pan.2024.5">https://doi.org/10.1017/pan.2024.5</a></p></li>
<li><p><strong>DeVellis, R. F.</strong> (2017). <em>Scale development: Theory and applications</em> (4th ed.). Thousand Oaks, CA: SAGE.</p></li>
<li><p><strong>Groves, R. M., Fowler, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., &amp; Tourangeau, R.</strong> (2009). <em>Survey Methodology</em> (2nd ed.). Hoboken, NJ: Wiley.</p></li>
<li><p><strong>Jansen, B. J., Jung, S., &amp; Salminen, J.</strong> (2023). <em>Employing large language models in survey research</em>. Natural Language Processing, 4, 100020. <a class="reference external" href="https://doi.org/10.1016/j.nlp.2023.100020">https://doi.org/10.1016/j.nlp.2023.100020</a></p></li>
<li><p><strong>Krosnick, J. A.</strong> (1999). <em>Survey research</em>. Annual Review of Psychology, 50, 537–567.</p></li>
<li><p><strong>Likert, R.</strong> (1932). <em>A technique for the measurement of attitudes</em>. Archives of Psychology, 22(140), 1–55.</p></li>
<li><p><strong>Shrestha, P., Koaik, F., Schnider, R., &amp; Sayess, D.</strong> (2025). <em>Beyond WEIRD: Can synthetic survey participants substitute for humans in global policy research?</em> Behavioral Science &amp; Policy, 3(X), 1–20. <a class="reference external" href="https://doi.org/10.1177/23794607241311793">https://doi.org/10.1177/23794607241311793</a></p></li>
<li><p><strong>Wuttke, A., Aßenmacher, M., Klamm, C., Lang, M. M., Würschinger, Q., &amp; Kreuter, F.</strong> (2025). <em>AI conversational interviewing: Transforming surveys with LLMs as adaptive interviewers</em>. In Proceedings of the LaTeCH-CLfL 2025 Conference. <a class="reference external" href="https://doi.org/10.48550/arXiv.2410.01824">https://doi.org/10.48550/arXiv.2410.01824</a></p></li>
<li><p><strong>Zou, Z., Mubin, O., Alnajjar, F., &amp; Ali, L.</strong> (2024). <em>A pilot study of measuring emotional response and perception of LLM-generated and human-generated questionnaires</em>. Scientific Reports, 14, 2781. <a class="reference external" href="https://doi.org/10.1038/s41598-024-53255-1">https://doi.org/10.1038/s41598-024-53255-1</a></p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lecture_01_1_intro_to_genai_psychology.html" class="btn btn-neutral float-left" title="Lecture 01.1 – Introduction to Generative AI in Psychology" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lecture_01_3_synthetic_respondents.html" class="btn btn-neutral float-right" title="Lecture 01.3 – Synthetic Respondents: Simulation and Supplementation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>